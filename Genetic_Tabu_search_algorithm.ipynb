{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkTddWTvTmIpzsxCbq8DgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joimb9064/master_thesis/blob/main/Genetic_Tabu_search_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g4pEi9M5JMM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional, Tuple  # Added Tuple\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich.layout import Layout\n",
        "from rich.live import Live\n",
        "from rich.text import Text\n",
        "import logging\n",
        "import threading\n",
        "from google.colab import drive\n",
        "import os\n",
        "from rich import box  # This is the import we need\n",
        "from datetime import datetime, timedelta\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing\n",
        "import functools\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the log directory in Google Drive\n",
        "log_dir = \"/content/drive/My Drive/EdgeSimPy/logs\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Generate a log filename with current date and time\n",
        "log_filename = datetime.now().strftime(\"simulation_%Y-%m-%d_%H-%M-%S.log\")\n",
        "full_log_path = os.path.join(log_dir, log_filename)\n",
        "\n",
        "# List existing log files in the directory (optional)\n",
        "try:\n",
        "    print(\"\\n📂 Available log files in EdgeSimPy/logs:\")\n",
        "    for filename in os.listdir(log_dir):\n",
        "        print(filename)\n",
        "except Exception as e:\n",
        "    print(f\"Error listing log files: {e}\")\n",
        "\n",
        "# Configure logging to write to both console and file\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Change to DEBUG to capture more detailed logs\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),  # Output to console\n",
        "        logging.FileHandler(full_log_path)  # Save logs to dated file in Google Drive\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a logger instance\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Task:\n",
        "    def __init__(self,\n",
        "                 task_id: int,\n",
        "                 task_type: str,\n",
        "                 input_size: float,    # Input data size in GB\n",
        "                 output_size: float,   # Output data size in GB\n",
        "                 cpu_required: float): # Total MI required\n",
        "        # Basic identification\n",
        "        self.id = task_id\n",
        "        self.type = task_type\n",
        "        # Resource requirements\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.total_cpu_required = cpu_required\n",
        "        self.remaining_cpu = cpu_required\n",
        "\n",
        "        # Timing metrics (CloudSim-style)\n",
        "        self.arrival_time = datetime.now().timestamp()  # Set arrival time immediately\n",
        "        self.start_time = None    # Will be set when actual processing begins\n",
        "        self.completion_time = None\n",
        "        self.exec_start_time = None\n",
        "        self.actual_exec_time = 0.0     # Initialize as float\n",
        "        self.turnaround_time = 0.0      # Initialize as float\n",
        "        self.waiting_time = 0.0         # Initialize as float\n",
        "\n",
        "        # Status tracking\n",
        "        self.status = 'CREATED'\n",
        "        self.completion_percentage = 0.0\n",
        "        self.assigned_resource = None\n",
        "\n",
        "        # Progress tracking\n",
        "        self.transferred_input = 0.0\n",
        "        self.transferred_output = 0.0\n",
        "        self.processed_mi = 0.0\n",
        "\n",
        "        # Tracking flags\n",
        "        self._time_initialized = False\n",
        "        self.last_update_time = None\n",
        "    def calculate_timing_metrics(self):\n",
        "        \"\"\"Calculate final timing metrics when task completes\"\"\"\n",
        "        if self.completion_time and self.arrival_time:\n",
        "            # Total time from arrival to completion\n",
        "            self.turnaround_time = self.completion_time - self.arrival_time\n",
        "\n",
        "            # Time spent in actual processing (excluding transfers)\n",
        "            if self.exec_start_time:\n",
        "                self.actual_exec_time = self.completion_time - self.exec_start_time\n",
        "            else:\n",
        "                self.actual_exec_time = self.completion_time - self.start_time\n",
        "\n",
        "            # Waiting time is turnaround time minus execution time\n",
        "            self.waiting_time = self.turnaround_time - self.actual_exec_time\n",
        "\n",
        "            # Ensure we don't have negative times\n",
        "            self.turnaround_time = max(0, self.turnaround_time)\n",
        "            self.actual_exec_time = max(0, self.actual_exec_time)\n",
        "            self.waiting_time = max(0, self.waiting_time)\n",
        "    def estimate_execution_time(self, resource) -> float:\n",
        "        \"\"\"\n",
        "        Calculate execution time considering separate input and output transfers\n",
        "        \"\"\"\n",
        "        # Input transfer time (for read tasks)\n",
        "        input_mb = self.input_size * 1024\n",
        "        input_transfer_time = input_mb / resource.total_bandwidth if input_mb > 0 else 0\n",
        "\n",
        "        # Processing time\n",
        "        processing_time = self.total_cpu_required / resource.total_cpu_rating\n",
        "\n",
        "        # Output transfer time (for write tasks)\n",
        "        output_mb = self.output_size * 1024\n",
        "        output_transfer_time = output_mb / resource.total_bandwidth if output_mb > 0 else 0\n",
        "\n",
        "        total_time = input_transfer_time + processing_time + output_transfer_time\n",
        "        return total_time\n",
        "\n",
        "    def update_progress(self, resource, current_time: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Update task progress with fixed time step calculations and enhanced timing metrics\n",
        "\n",
        "        This method handles the complete lifecycle of a task, including:\n",
        "        - Input data transfer\n",
        "        - Processing\n",
        "        - Output data transfer\n",
        "        - Timing metrics calculation\n",
        "        \"\"\"\n",
        "        # Log initial state for debugging\n",
        "        logger.info(f\"Task {self.id} UPDATE: Current state={self.status}, Time={current_time:.2f}\")\n",
        "\n",
        "        # SECTION 1: Initialize timing tracking\n",
        "        if not self._time_initialized:\n",
        "            logger.info(f\"Task {self.id} INIT: First update at time {current_time:.2f}\")\n",
        "            self._time_initialized = True\n",
        "            self.last_update_time = current_time\n",
        "\n",
        "        # SECTION 2: Calculate time steps for this update\n",
        "        elapsed = current_time - self.last_update_time  # Time since last update\n",
        "        steps = max(1, int(elapsed / 1.0))  # Divide into 1-second steps\n",
        "        time_per_step = elapsed / steps     # Actual time per step\n",
        "\n",
        "        logger.info(f\"Task {self.id} STEP: Time since last update={elapsed:.3f}s, Steps={steps}\")\n",
        "\n",
        "        # Calculate bandwidth in GB/s (convert from Mb/s)\n",
        "        step_bandwidth = (resource.total_bandwidth / 8.0) / 1024.0\n",
        "\n",
        "        # SECTION 3: State Machine - Handle different task states\n",
        "        old_status = self.status  # Track state transitions\n",
        "\n",
        "        if self.status == 'CREATED':\n",
        "            # Initial state: Set up task for processing\n",
        "            self.status = 'READY'\n",
        "            self.transferred_input = 0.0\n",
        "            self.transferred_output = 0.0\n",
        "            self.processed_mi = 0.0\n",
        "            if not self.start_time:  # Only set start_time if not already set\n",
        "                self.start_time = current_time  # Record when task starts\n",
        "                logger.info(f\"Task {self.id} TRANSITION: CREATED → READY at {current_time:.2f}\")\n",
        "\n",
        "        elif self.status == 'READY':\n",
        "            # Determine next phase based on input requirements\n",
        "            if self.input_size > 0:\n",
        "                self.status = 'TRANSFERRING_INPUT'\n",
        "                logger.info(f\"Task {self.id} TRANSITION: READY → TRANSFERRING_INPUT (input_size={self.input_size}GB)\")\n",
        "            else:\n",
        "                self.status = 'PROCESSING'\n",
        "                self.exec_start_time = current_time  # Record processing start\n",
        "                logger.info(f\"Task {self.id} TRANSITION: READY → PROCESSING (no input transfer needed)\")\n",
        "            self.completion_percentage = 0\n",
        "\n",
        "        elif self.status == 'TRANSFERRING_INPUT':\n",
        "            # Calculate input data transfer for this time step\n",
        "            step_transfer = step_bandwidth * time_per_step * steps\n",
        "            old_transfer = self.transferred_input\n",
        "            self.transferred_input = min(self.input_size, self.transferred_input + step_transfer)\n",
        "\n",
        "            logger.info(f\"Task {self.id} TRANSFER: Input {old_transfer:.3f}GB → {self.transferred_input:.3f}GB of {self.input_size:.3f}GB\")\n",
        "\n",
        "            if self.transferred_input >= self.input_size:\n",
        "                # Input transfer complete, start processing\n",
        "                self.status = 'PROCESSING'\n",
        "                self.exec_start_time = current_time  # Record actual processing start time\n",
        "                self.completion_percentage = 0\n",
        "                logger.info(f\"Task {self.id} TRANSITION: TRANSFERRING_INPUT → PROCESSING at {current_time:.2f}\")\n",
        "            else:\n",
        "                self.completion_percentage = (self.transferred_input / self.input_size) * 100\n",
        "\n",
        "        elif self.status == 'PROCESSING':\n",
        "            # Calculate processing progress\n",
        "            step_processing = resource.total_cpu_rating * time_per_step * steps\n",
        "            old_processed = self.processed_mi\n",
        "            self.processed_mi = min(self.total_cpu_required, self.processed_mi + step_processing)\n",
        "            self.remaining_cpu = self.total_cpu_required - self.processed_mi\n",
        "\n",
        "            logger.info(f\"Task {self.id} PROCESSING: {old_processed:.0f}MI → {self.processed_mi:.0f}MI of {self.total_cpu_required:.0f}MI\")\n",
        "\n",
        "            if self.processed_mi >= self.total_cpu_required:\n",
        "                # Processing complete, check if output transfer needed\n",
        "                if self.output_size > 0:\n",
        "                    self.status = 'TRANSFERRING_OUTPUT'\n",
        "                    self.completion_percentage = 0\n",
        "                    logger.info(f\"Task {self.id} TRANSITION: PROCESSING → TRANSFERRING_OUTPUT (output_size={self.output_size}GB)\")\n",
        "                else:\n",
        "                    # No output transfer needed, task is complete\n",
        "                    self.status = 'COMPLETED'\n",
        "                    self.completion_time = current_time\n",
        "                    self.calculate_timing_metrics()\n",
        "                    self.completion_percentage = 100\n",
        "                    logger.info(f\"Task {self.id} TRANSITION: PROCESSING → COMPLETED at {current_time:.2f}\")\n",
        "            else:\n",
        "                self.completion_percentage = (self.processed_mi / self.total_cpu_required) * 100\n",
        "\n",
        "        elif self.status == 'TRANSFERRING_OUTPUT':\n",
        "            # Calculate output data transfer for this time step\n",
        "            step_transfer = step_bandwidth * time_per_step * steps\n",
        "            old_transfer = self.transferred_output\n",
        "            self.transferred_output = min(self.output_size, self.transferred_output + step_transfer)\n",
        "\n",
        "            logger.info(f\"Task {self.id} TRANSFER: Output {old_transfer:.3f}GB → {self.transferred_output:.3f}GB of {self.output_size:.3f}GB\")\n",
        "\n",
        "            if self.transferred_output >= self.output_size:\n",
        "                # Output transfer complete, task is finished\n",
        "                self.status = 'COMPLETED'\n",
        "                self.completion_time = current_time\n",
        "                self.calculate_timing_metrics()\n",
        "                self.completion_percentage = 100\n",
        "                logger.info(f\"Task {self.id} TRANSITION: TRANSFERRING_OUTPUT → COMPLETED at {current_time:.2f}\")\n",
        "            else:\n",
        "                self.completion_percentage = (self.transferred_output / self.output_size) * 100\n",
        "\n",
        "        # Log state transition if it occurred\n",
        "        if old_status != self.status:\n",
        "            logger.info(f\"Task {self.id} STATE CHANGE: {old_status} → {self.status}\")\n",
        "\n",
        "        # SECTION 4: Update timing and return status\n",
        "        self.last_update_time = current_time\n",
        "\n",
        "        # Calculate current execution time\n",
        "        current_exec_time = current_time - (self.start_time or current_time)\n",
        "\n",
        "        # Final progress report for this update\n",
        "        logger.info(f\"Task {self.id} PROGRESS: Status={self.status}, Completion={self.completion_percentage:.1f}%, ExecTime={current_exec_time:.2f}s\")\n",
        "\n",
        "        return {\n",
        "            'status': self.status,\n",
        "            'progress': self.completion_percentage,\n",
        "            'exec_time': current_exec_time\n",
        "        }\n",
        "    def calculate_final_metrics(self):\n",
        "        \"\"\"\n",
        "        Calculate final timing metrics when task completes\n",
        "        \"\"\"\n",
        "        if self.completion_time and self.arrival_time and self.exec_start_time:\n",
        "            # Turnaround time: time from arrival to completion\n",
        "            self.turnaround_time = self.completion_time - self.arrival_time\n",
        "\n",
        "            # Actual execution time: time spent processing (excluding transfers)\n",
        "            self.actual_exec_time = self.completion_time - self.exec_start_time\n",
        "\n",
        "            # Waiting time: turnaround time minus actual execution time\n",
        "            self.waiting_time = self.turnaround_time - self.actual_exec_time\n",
        "    def update_execution(self, resource, time_step: float) -> bool:\n",
        "            \"\"\"\n",
        "            Process task as a single unit, tracking overall progress.\n",
        "            Returns True if task is completed.\n",
        "            \"\"\"\n",
        "            if self.status == 'CREATED':\n",
        "                self.status = 'RUNNING'\n",
        "                self.start_time = datetime.now().timestamp()\n",
        "\n",
        "            elif self.status == 'RUNNING':\n",
        "                # Calculate progress including both transfer and processing\n",
        "                progress = (time_step * resource.total_cpu_rating)\n",
        "                self.remaining_cpu -= progress\n",
        "\n",
        "                if self.remaining_cpu <= 0:\n",
        "                    self.status = 'COMPLETED'\n",
        "                    self.completion_time = datetime.now().timestamp()\n",
        "                    self.execution_time = self.completion_time - self.start_time\n",
        "                    return True\n",
        "\n",
        "            return False\n",
        "    def process(self, available_cpu: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Process the task with available CPU\n",
        "        Returns processing details\n",
        "        \"\"\"\n",
        "        processed = min(available_cpu, self.remaining_cpu)\n",
        "        self.remaining_cpu -= processed\n",
        "\n",
        "        # Log task processing details\n",
        "        logger.info(f\"Processing task {self.id} on resource. Remaining CPU: {self.remaining_cpu}\")\n",
        "\n",
        "        # Calculate completion percentage\n",
        "        completion_percentage = (self.total_cpu_required - self.remaining_cpu) / self.total_cpu_required * 100\n",
        "\n",
        "        # Update status\n",
        "        if self.remaining_cpu <= 0:\n",
        "            self.status = 'completed'\n",
        "            self.completion_time = datetime.now().timestamp()\n",
        "\n",
        "        return {\n",
        "            'processed': processed,\n",
        "            'remaining': self.remaining_cpu,\n",
        "            'status': self.status,\n",
        "            'completion_percentage': completion_percentage\n",
        "        }\n",
        "class Resource:\n",
        "    def __init__(self,\n",
        "                 resource_id: int,\n",
        "                 resource_type: str,\n",
        "                 cpu_rating: int,    # MIPS\n",
        "                 memory: int,        # GB\n",
        "                 bandwidth: int):    # Mb/s\n",
        "        # Resource identification\n",
        "        self.id = resource_id\n",
        "        self.type = resource_type\n",
        "\n",
        "        # Resource capabilities\n",
        "        self.total_cpu_rating = cpu_rating\n",
        "        self.total_memory = memory\n",
        "        self.total_bandwidth = bandwidth\n",
        "\n",
        "        # Task tracking\n",
        "        self.task_queue = []\n",
        "        self.current_task = None\n",
        "        self.completed_tasks = []\n",
        "        self.failed_tasks = []\n",
        "        self.failed_tasks_count = 0\n",
        "\n",
        "        # Resource utilization tracking\n",
        "        self.used_cpu = 0\n",
        "        self.used_memory = 0\n",
        "        self.current_load = 0.0\n",
        "\n",
        "    def calculate_resource_utilization(self):\n",
        "        \"\"\"\n",
        "        Debug resource utilization\n",
        "        \"\"\"\n",
        "        # First, log the current state\n",
        "        task_info = \"No task\"\n",
        "        if self.current_task:\n",
        "            task_info = f\"Task {self.current_task.id} ({self.current_task.type}) in state {self.current_task.status}\"\n",
        "            # Add extensive task state logging\n",
        "            logger.info(f\"\"\"\n",
        "            CURRENT TASK STATE:\n",
        "            Task ID: {self.current_task.id}\n",
        "            Type: {self.current_task.type}\n",
        "            Status: {self.current_task.status}\n",
        "            Progress: {self.current_task.completion_percentage}%\n",
        "            Input Size: {self.current_task.input_size} GB\n",
        "            Output Size: {self.current_task.output_size} GB\n",
        "            Processed MI: {self.current_task.processed_mi} / {self.current_task.total_cpu_required}\n",
        "            Input Transfer: {self.current_task.transferred_input} / {self.current_task.input_size} GB\n",
        "            Output Transfer: {self.current_task.transferred_output} / {self.current_task.output_size} GB\n",
        "            \"\"\")\n",
        "\n",
        "        logger.info(f\"Resource {self.id} ({self.type}) - {task_info}\")\n",
        "\n",
        "        # Default values\n",
        "        cpu_util = 0\n",
        "        mem_util = 0\n",
        "        bw_util = 0\n",
        "\n",
        "        # Super simple utilization based purely on state\n",
        "        if self.current_task:\n",
        "            status = self.current_task.status\n",
        "\n",
        "            # Debug status transitions\n",
        "            logger.info(f\"Current status: {status}\")\n",
        "\n",
        "            if status == 'PROCESSING':\n",
        "                cpu_util = 100  # Full CPU during processing\n",
        "                logger.info(f\"Setting CPU utilization to 100% for PROCESSING state\")\n",
        "\n",
        "            if status in ['TRANSFERRING_INPUT', 'TRANSFERRING_OUTPUT']:\n",
        "                bw_util = 100  # Full bandwidth during transfers\n",
        "                logger.info(f\"Setting bandwidth utilization to 100% for {status}\")\n",
        "\n",
        "            # Set memory utilization based on data presence\n",
        "            if self.current_task.input_size > 0 or self.current_task.output_size > 0:\n",
        "                mem_util = 50  # Using 50% as a simple indicator\n",
        "                logger.info(f\"Setting memory utilization to 50% due to data presence\")\n",
        "\n",
        "        # Log final values\n",
        "       # logger.info(f\"\"\"\n",
        "       # FINAL UTILIZATION VALUES:\n",
        "       #CPU: {cpu_util}%\n",
        "       # Memory: {mem_util}%\n",
        "       # Bandwidth: {bw_util}%\n",
        "       # \"\"\")\n",
        "\n",
        "        return {\n",
        "            'cpu_utilization': cpu_util,\n",
        "            'memory_utilization': mem_util,\n",
        "            'bandwidth_utilization': bw_util,\n",
        "            'overall_utilization': (cpu_util + mem_util + bw_util) / 3,\n",
        "            'raw_cpu_usage': (cpu_util / 100.0) * self.total_cpu_rating,\n",
        "            'raw_memory_usage': (mem_util / 100.0) * self.total_memory * 1024,\n",
        "            'raw_bandwidth_usage': (bw_util / 100.0) * self.total_bandwidth,\n",
        "            'active_tasks': 1 if self.current_task else 0,\n",
        "            'transfer_phase': self.current_task.status if self.current_task else 'NONE'\n",
        "        }\n",
        "    def _get_zero_utilization(self):\n",
        "        \"\"\"Helper method to return zero utilization state\"\"\"\n",
        "        return {\n",
        "            'cpu_utilization': 0,\n",
        "            'memory_utilization': 0,\n",
        "            'bandwidth_utilization': 0,\n",
        "            'overall_utilization': 0,\n",
        "            'raw_cpu_usage': 0,\n",
        "            'raw_memory_usage': 0,\n",
        "            'raw_bandwidth_usage': 0,\n",
        "            'active_tasks': 0,\n",
        "            'transfer_phase': 'NONE'\n",
        "        }\n",
        "    def can_process_task(self, task: Task) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Check if resource can process a given task based on task type and resource type.\n",
        "        Only RT1 and RT3 tasks are restricted from Smartphone and Raspberry Pi resources.\n",
        "        \"\"\"\n",
        "        # Cloud resources can process all tasks\n",
        "        if self.type.startswith(\"Cloud_\"):\n",
        "            return True, \"\"\n",
        "\n",
        "        # For Smartphone and Raspberry Pi resources\n",
        "        if self.type.startswith((\"Smartphone_\", \"Raspberry_\")):\n",
        "            return True, \"\"\n",
        "\n",
        "        # All other combinations are valid\n",
        "        return True, \"\"\n",
        "\n",
        "    def process_task(self, resource, current_time: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Process task as a single unit, ensuring proper completion.\n",
        "        \"\"\"\n",
        "        status = {\n",
        "            'processed': False,\n",
        "            'completed': False,\n",
        "            'task_id': self.id,\n",
        "            'progress': self.completion_percentage\n",
        "        }\n",
        "\n",
        "        if self.status == 'CREATED':\n",
        "            self.status = 'RUNNING'\n",
        "            self.start_time = current_time\n",
        "            status['processed'] = True\n",
        "\n",
        "        elif self.status == 'RUNNING':\n",
        "            # Calculate progress including both transfer and processing\n",
        "            time_step = current_time - self.start_time\n",
        "            progress = (time_step * resource.total_cpu_rating)\n",
        "            self.remaining_cpu = max(0, self.remaining_cpu - progress)\n",
        "\n",
        "            # Update completion percentage\n",
        "            self.completion_percentage = min(100, ((self.total_cpu_required - self.remaining_cpu) /\n",
        "                                                self.total_cpu_required * 100))\n",
        "\n",
        "            if self.remaining_cpu <= 0:\n",
        "                self.status = 'COMPLETED'\n",
        "                self.completion_time = current_time\n",
        "                self.actual_exec_time = self.completion_time - self.start_time\n",
        "                status['completed'] = True\n",
        "\n",
        "            status['processed'] = True\n",
        "            status['progress'] = self.completion_percentage\n",
        "\n",
        "        return status\n",
        "    def enqueue_task(self, task: Task):\n",
        "        \"\"\"Add task to queue and update resource utilization\"\"\"\n",
        "        can_process, failure_reason = self.can_process_task(task)\n",
        "\n",
        "        if can_process:\n",
        "            task.status = 'READY'\n",
        "            self.task_queue.append(task)\n",
        "            logger.info(f\"Task {task.id} queued on {self.type}. Queue length: {len(self.task_queue)}\")\n",
        "        else:\n",
        "            task.status = 'FAILED'\n",
        "            task.failure_reason = failure_reason\n",
        "            self.failed_tasks.append(task)\n",
        "            self.failed_tasks_count += 1\n",
        "            logger.warning(f\"Task {task.id} ({task.type}) failed on {self.type}: {failure_reason}\")\n",
        "    def process_queue(self, current_time: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Process tasks in queue with fixed progress tracking\n",
        "        \"\"\"\n",
        "        status = {\n",
        "            'completed_tasks': len(self.completed_tasks),\n",
        "            'current_task': None,\n",
        "            'queue_length': len(self.task_queue),\n",
        "            'resource_utilization': self.calculate_resource_utilization()\n",
        "        }\n",
        "\n",
        "        if self.current_task:\n",
        "            # Update task progress\n",
        "            progress = self.current_task.update_progress(self, current_time)\n",
        "\n",
        "            # Update status dictionary\n",
        "            status['current_task'] = {\n",
        "                'id': self.current_task.id,\n",
        "                'type': self.current_task.type,\n",
        "                'phase': self.current_task.status,\n",
        "                'progress': progress['progress'],\n",
        "                'input_size': self.current_task.input_size,\n",
        "                'output_size': self.current_task.output_size\n",
        "            }\n",
        "\n",
        "            # Handle task completion\n",
        "            if self.current_task.status == 'COMPLETED':\n",
        "                self.completed_tasks.append(self.current_task)\n",
        "                logger.info(f\"Task {self.current_task.id} completed on {self.type}\")\n",
        "                self.current_task = None\n",
        "\n",
        "        # Start new task if available\n",
        "        if not self.current_task and self.task_queue:\n",
        "            self.current_task = self.task_queue.pop(0)\n",
        "            # Make sure tasks are in CREATED state before processing\n",
        "            logger.info(f\"Starting Task {self.current_task.id} (status: {self.current_task.status}) - forcing to CREATED\")\n",
        "            self.current_task.status = 'CREATED'\n",
        "\n",
        "            self.current_task.exec_start_time = current_time\n",
        "            self.current_task._time_initialized = False  # Reset this flag to trigger initialization\n",
        "\n",
        "        return status\n",
        "class ResourceFocusedScheduler:\n",
        "    \"\"\"\n",
        "    Scheduler with resource-focused real-time visualization\n",
        "    \"\"\"\n",
        "    def __init__(self, resources: List[Resource]):\n",
        "        self.resources = resources\n",
        "        self.current_time = 0\n",
        "        #self.arrival_rate = 1.0  # Default task arrival rate\n",
        "        self.console = Console()\n",
        "        self.metrics = {\n",
        "            'total_tasks': 0,\n",
        "            'completed_tasks': 0,\n",
        "            'failed_tasks': 0,\n",
        "            'globally_failed_tasks': [],  # Store globally unassigned failed tasks\n",
        "            'makespan': 0,\n",
        "            'throughput': 0\n",
        "        }\n",
        "    def calculate_turnaround_time(self, task, simulation_start_time):\n",
        "        \"\"\"\n",
        "        Calculate turnaround time relative to simulation start time\n",
        "        \"\"\"\n",
        "        if (task.arrival_time is not None and\n",
        "            task.completion_time is not None):\n",
        "            # Adjust times relative to simulation start\n",
        "            arrival_relative = task.arrival_time - simulation_start_time\n",
        "            completion_relative = task.completion_time - simulation_start_time\n",
        "\n",
        "            turnaround_time = max(completion_relative - arrival_relative, 0)\n",
        "            return turnaround_time\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_waiting_time(self, task):\n",
        "        \"\"\"\n",
        "        Calculate waiting time\n",
        "        \"\"\"\n",
        "        if (task.turnaround_time is not None and\n",
        "            task.actual_exec_time is not None):\n",
        "            waiting_time = max(task.turnaround_time - task.actual_exec_time, 0)\n",
        "            return waiting_time\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_timing_metrics(self, completed_tasks: List[Task], simulation_start_time: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate timing metrics with guaranteed dictionary return\n",
        "        \"\"\"\n",
        "        # Initialize metrics with default values\n",
        "        metrics = {\n",
        "            'average_turnaround_time': 0.0,\n",
        "            'average_waiting_time': 0.0,\n",
        "            'average_execution_time': 0.0,\n",
        "            'total_tasks_processed': 0,\n",
        "            'total_turnaround_time': 0.0,\n",
        "            'total_waiting_time': 0.0,\n",
        "            'total_execution_time': 0.0\n",
        "        }\n",
        "\n",
        "        if not completed_tasks:\n",
        "            logger.warning(\"No completed tasks to process\")\n",
        "            return metrics\n",
        "\n",
        "        total_turnaround = 0.0\n",
        "        total_waiting = 0.0\n",
        "        total_execution = 0.0\n",
        "        valid_tasks = 0\n",
        "\n",
        "        logger.info(f\"\\nProcessing timing metrics for {len(completed_tasks)} completed tasks\")\n",
        "\n",
        "        for task in completed_tasks:\n",
        "            if task.status == 'COMPLETED':\n",
        "                # Log timing values for debugging\n",
        "                logger.debug(f\"\"\"\n",
        "                Task {task.id} timing values:\n",
        "                - Arrival: {task.arrival_time}\n",
        "                - Start: {task.start_time}\n",
        "                - Exec Start: {task.exec_start_time}\n",
        "                - Completion: {task.completion_time}\n",
        "                \"\"\")\n",
        "\n",
        "                # Ensure timing values are valid\n",
        "                if all(x is not None for x in [task.arrival_time, task.start_time,\n",
        "                                            task.completion_time, task.exec_start_time]):\n",
        "                    # Recalculate metrics to ensure consistency\n",
        "                    task.turnaround_time = max(0, task.completion_time - task.arrival_time)\n",
        "                    task.actual_exec_time = max(0, task.completion_time - task.exec_start_time)\n",
        "                    task.waiting_time = max(0, task.turnaround_time - task.actual_exec_time)\n",
        "\n",
        "                    if task.turnaround_time > 0 and task.actual_exec_time > 0:\n",
        "                        total_turnaround += task.turnaround_time\n",
        "                        total_waiting += task.waiting_time\n",
        "                        total_execution += task.actual_exec_time\n",
        "                        valid_tasks += 1\n",
        "                        logger.info(f\"Task {task.id} metrics valid - turnaround: {task.turnaround_time:.2f}s\")\n",
        "                    else:\n",
        "                        logger.warning(f\"Task {task.id} has zero or negative timing values\")\n",
        "                else:\n",
        "                    logger.warning(f\"Task {task.id} has missing timing values\")\n",
        "\n",
        "        # Update metrics if we have valid tasks\n",
        "        if valid_tasks > 0:\n",
        "            metrics.update({\n",
        "                'average_turnaround_time': total_turnaround / valid_tasks,\n",
        "                'average_waiting_time': total_waiting / valid_tasks,\n",
        "                'average_execution_time': total_execution / valid_tasks,\n",
        "                'total_tasks_processed': valid_tasks,\n",
        "                'total_turnaround_time': total_turnaround,\n",
        "                'total_waiting_time': total_waiting,\n",
        "                'total_execution_time': total_execution\n",
        "            })\n",
        "\n",
        "        # Log final summary\n",
        "        logger.info(f\"\"\"\n",
        "        Timing Metrics Summary:\n",
        "        - Total Tasks: {len(completed_tasks)}\n",
        "        - Valid Tasks: {valid_tasks}\n",
        "        - Average Turnaround: {metrics['average_turnaround_time']:.2f}s\n",
        "        - Average Waiting: {metrics['average_waiting_time']:.2f}s\n",
        "        - Average Execution: {metrics['average_execution_time']:.2f}s\n",
        "        \"\"\")\n",
        "\n",
        "        return metrics\n",
        "    def calculate_datacenter_utilization(self, start_time: float, end_time: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate utilization for different datacenter types following the paper's integral approach\n",
        "        \"\"\"\n",
        "        # Separate resources by datacenter type\n",
        "        smartphone_resources = [r for r in self.resources if r.type.startswith('Smartphone_')]\n",
        "        raspberry_pi_resources = [r for r in self.resources if r.type.startswith('Raspberry_')]\n",
        "        cloud_resources = [r for r in self.resources if r.type.startswith('Cloud_')]\n",
        "\n",
        "        def calculate_datacenter_ru(resources: List[Resource]) -> Dict:\n",
        "            \"\"\"\n",
        "            Calculate resource utilization for a specific datacenter type\n",
        "            \"\"\"\n",
        "            # Collect individual resource utilizations\n",
        "            resource_utilizations = [r.calculate_resource_utilization() for r in resources]\n",
        "\n",
        "            # Calculate datacenter-wide metrics\n",
        "            return {\n",
        "                'total_resources': len(resources),\n",
        "                'avg_cpu_utilization': np.mean([ru['cpu_utilization'] for ru in resource_utilizations]),\n",
        "                'avg_memory_utilization': np.mean([ru['memory_utilization'] for ru in resource_utilizations]),\n",
        "                'avg_bandwidth_utilization': np.mean([ru['bandwidth_utilization'] for ru in resource_utilizations]),\n",
        "                'overall_utilization': np.mean([ru['overall_utilization'] for ru in resource_utilizations]),\n",
        "                'active_tasks': sum(ru['active_tasks'] for ru in resource_utilizations),\n",
        "                'raw_metrics': resource_utilizations\n",
        "            }\n",
        "\n",
        "        # Calculate utilization for each datacenter type\n",
        "        datacenter_utilization = {\n",
        "            'smartphone_edge': calculate_datacenter_ru(smartphone_resources),\n",
        "            'raspberry_pi_edge': calculate_datacenter_ru(raspberry_pi_resources),\n",
        "            'cloud': calculate_datacenter_ru(cloud_resources)\n",
        "        }\n",
        "\n",
        "        # Calculate overall datacenter utilization\n",
        "        datacenter_utilization['overall'] = {\n",
        "            'total_resources': len(self.resources),\n",
        "            'total_simulation_time': end_time - start_time,\n",
        "            'avg_cpu_utilization': np.mean([\n",
        "                datacenter_utilization['smartphone_edge']['avg_cpu_utilization'],\n",
        "                datacenter_utilization['raspberry_pi_edge']['avg_cpu_utilization'],\n",
        "                datacenter_utilization['cloud']['avg_cpu_utilization']\n",
        "            ]),\n",
        "            'avg_memory_utilization': np.mean([\n",
        "                datacenter_utilization['smartphone_edge']['avg_memory_utilization'],\n",
        "                datacenter_utilization['raspberry_pi_edge']['avg_memory_utilization'],\n",
        "                datacenter_utilization['cloud']['avg_memory_utilization']\n",
        "            ]),\n",
        "            'avg_bandwidth_utilization': np.mean([\n",
        "                datacenter_utilization['smartphone_edge']['avg_bandwidth_utilization'],\n",
        "                datacenter_utilization['raspberry_pi_edge']['avg_bandwidth_utilization'],\n",
        "                datacenter_utilization['cloud']['avg_bandwidth_utilization']\n",
        "            ]),\n",
        "            'total_active_tasks': sum(\n",
        "                datacenter_utilization[dc_type]['active_tasks']\n",
        "                for dc_type in ['smartphone_edge', 'raspberry_pi_edge', 'cloud']\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return datacenter_utilization\n",
        "\n",
        "    def generate_tasks(self, total_tasks: int) -> Tuple[List[Task], List[float], List[float]]:\n",
        "            \"\"\"\n",
        "            Generate tasks with Poisson-distributed arrival times and return raw inter-arrival times.\n",
        "\n",
        "            Args:\n",
        "                total_tasks (int): The total number of tasks to generate.\n",
        "\n",
        "            Returns:\n",
        "                Tuple[List[Task], List[float], List[float]]: A tuple containing:\n",
        "                    - List of generated Task objects\n",
        "                    - List of cumulative arrival times\n",
        "                    - List of raw inter-arrival times\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            simulation_duration = 100\n",
        "            lambda_rate = total_tasks / simulation_duration\n",
        "            inter_arrival_times = []\n",
        "            cumulative_times = [0]  # Start with 0 as the first arrival time\n",
        "\n",
        "            while len(inter_arrival_times) < total_tasks - 1:  # We need one less inter-arrival time than total tasks\n",
        "                interval = np.random.exponential(1.0 / lambda_rate)\n",
        "                inter_arrival_times.append(interval)\n",
        "                cumulative_times.append(cumulative_times[-1] + interval)\n",
        "            # Use a base time to ensure consistent and positive arrival times\n",
        "            # In the generate_tasks method\n",
        "            base_time = datetime.now().timestamp()\n",
        "\n",
        "            tasks = []\n",
        "            for i in range(total_tasks):\n",
        "                task = Task(\n",
        "                    task_id=i + 1,\n",
        "                    task_type=\"placeholder\",\n",
        "                    input_size=0.0,    # Changed from data_size to input_size\n",
        "                    output_size=0.0,   # Added output_size\n",
        "                    cpu_required=0.0\n",
        "                )\n",
        "                task.arrival_time = base_time + cumulative_times[i]\n",
        "                tasks.append(task)\n",
        "\n",
        "            # Write inter-arrival times to a CSV file\n",
        "            with open('inter_arrival_times.csv', 'w', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['Inter-arrival Time'])\n",
        "                for time in inter_arrival_times:\n",
        "                    writer.writerow([time])\n",
        "\n",
        "            return tasks, cumulative_times, inter_arrival_times\n",
        "\n",
        "    def _create_resource_panel(self, resource: Resource, status: Dict) -> Panel:\n",
        "        \"\"\"\n",
        "        Create a detailed panel for a specific resource with comprehensive task information.\n",
        "        \"\"\"\n",
        "        table = Table(show_header=False, show_lines=True)\n",
        "\n",
        "        # Resource basic information\n",
        "        table.add_row(\"[bold]Resource Details[/bold]\")\n",
        "        table.add_row(f\"[cyan]Type:[/cyan] {resource.type}\")\n",
        "        table.add_row(f\"[green]CPU Rating:[/green] {resource.total_cpu_rating} MI/s\")\n",
        "        table.add_row(f\"[blue]Memory:[/blue] {resource.total_memory} GB\")\n",
        "        table.add_row(f\"[yellow]Bandwidth:[/yellow] {resource.total_bandwidth} MB/s\")\n",
        "\n",
        "        # Utilization information\n",
        "        table.add_row(\"\\n[bold]Utilization Metrics[/bold]\")\n",
        "        table.add_row(\n",
        "            f\"[green]CPU Usage:[/green] {status['resource_utilization']['cpu_utilization']:.2f}% \"\n",
        "            f\"({status['resource_utilization']['raw_cpu_usage']:.2f}/{resource.total_cpu_rating} MI/s)\"\n",
        "        )\n",
        "        table.add_row(\n",
        "            f\"[blue]Memory Usage:[/blue] {status['resource_utilization']['memory_utilization']:.2f}% \"\n",
        "            f\"({status['resource_utilization']['raw_memory_usage']:.2f}/{resource.total_memory} GB)\"\n",
        "        )\n",
        "        table.add_row(\n",
        "            f\"[yellow]Bandwidth Usage:[/yellow] {status['resource_utilization']['bandwidth_utilization']:.2f}% \"\n",
        "            f\"({status['resource_utilization']['raw_bandwidth_usage']:.2f}/{resource.total_bandwidth} MB/s)\"\n",
        "        )\n",
        "\n",
        "        # Task Queue Information\n",
        "        table.add_row(\"\\n[bold]Task Queue[/bold]\")\n",
        "        table.add_row(f\"[yellow]Queued Tasks:[/yellow] {status['queue_length']}\")\n",
        "\n",
        "        # Current Tasks\n",
        "        table.add_row(\"\\n[bold]Current Tasks[/bold]\")\n",
        "        if status['current_task']:\n",
        "            current_task = status['current_task']\n",
        "            table.add_row(\n",
        "                f\"[blue]Task {current_task['id']} ({current_task['type']}):[/blue] \"\n",
        "                f\"Phase: {current_task['phase']} Progress: {current_task['progress']}\"\n",
        "            )\n",
        "        else:\n",
        "            table.add_row(\"[dim]No tasks currently processing[/dim]\")\n",
        "\n",
        "        # Failed Tasks\n",
        "        table.add_row(\"\\n[bold]Failed Tasks[/bold]\")\n",
        "        if resource.failed_tasks:\n",
        "            for task in resource.failed_tasks:\n",
        "                table.add_row(\n",
        "                    f\"[red]Task {task.id} ({task.type}):[/red] \"\n",
        "                    f\"Reason: {task.failure_reason}\"\n",
        "                )\n",
        "        else:\n",
        "            table.add_row(\"[dim]No failed tasks[/dim]\")\n",
        "\n",
        "        return Panel(\n",
        "            table,\n",
        "            title=f\"Resource {resource.id}: {resource.type}\",\n",
        "            border_style=\"green\"\n",
        "        )\n",
        "    def verify_and_fix_task_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Emergency fix to directly ensure tasks are properly queued to resources\n",
        "        regardless of distribution algorithm issues.\n",
        "        \"\"\"\n",
        "        logger.info(\"🚨 EMERGENCY FIX: Verifying task distribution...\")\n",
        "\n",
        "        # Check current queue status\n",
        "        total_queued = sum(len(resource.task_queue) for resource in self.resources)\n",
        "        logger.info(f\"Current queued tasks: {total_queued} of {total_tasks}\")\n",
        "\n",
        "        # If we have tasks queued, no action needed\n",
        "        if total_queued >= total_tasks:\n",
        "            logger.info(\"✅ Task distribution verified: All tasks are properly queued\")\n",
        "\n",
        "            # Collect all tasks to return\n",
        "            all_tasks = []\n",
        "            for resource in self.resources:\n",
        "                all_tasks.extend(resource.task_queue)\n",
        "                all_tasks.extend(resource.failed_tasks)\n",
        "\n",
        "            return all_tasks\n",
        "\n",
        "        # If we have zero or insufficient tasks queued, perform emergency distribution\n",
        "        logger.warning(f\"⚠️ Task distribution problem detected: Only {total_queued} of {total_tasks} tasks queued\")\n",
        "\n",
        "        # Clear all existing resource queues\n",
        "        for resource in self.resources:\n",
        "            resource.task_queue = []\n",
        "            resource.failed_tasks = []\n",
        "\n",
        "        # Generate fresh tasks\n",
        "        logger.info(\"Generating emergency replacement tasks...\")\n",
        "        tasks, _, _ = self.generate_tasks(total_tasks)\n",
        "\n",
        "        # Define task types with fixed requirements\n",
        "        task_types = [\n",
        "            # Read Tasks\n",
        "            {\"type\": \"RT1\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"RT2\", \"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "            {\"type\": \"RT3\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "            {\"type\": \"RT4\", \"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "            # Write Tasks\n",
        "            {\"type\": \"WT1\", \"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"WT2\", \"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "            {\"type\": \"WT3\", \"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "            {\"type\": \"WT4\", \"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "        ]\n",
        "\n",
        "        # Distribute tasks using simple round-robin\n",
        "        distributed_tasks = []\n",
        "        resource_index = 0\n",
        "\n",
        "        for i, task_record in enumerate(tasks):\n",
        "            # Select task type based on index\n",
        "            task_type = task_types[i % len(task_types)]\n",
        "\n",
        "            # Create task with proper properties\n",
        "            task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "            task.arrival_time = task_record.arrival_time\n",
        "            task.status = 'CREATED'  # Ensure correct initial status\n",
        "\n",
        "            # Assign to resource using round-robin\n",
        "            resource = self.resources[resource_index]\n",
        "            resource.task_queue.append(task)\n",
        "\n",
        "            # Log the assignment\n",
        "            logger.info(f\"Emergency assigned Task {task.id} ({task.type}) to {resource.type}\")\n",
        "\n",
        "            # Advance resource index for round-robin\n",
        "            resource_index = (resource_index + 1) % len(self.resources)\n",
        "\n",
        "            # Add to distributed tasks list\n",
        "            distributed_tasks.append(task)\n",
        "\n",
        "        # Verify fix worked\n",
        "        total_queued = sum(len(resource.task_queue) for resource in self.resources)\n",
        "        logger.info(f\"Emergency fix completed: {total_queued} of {total_tasks} tasks now queued\")\n",
        "\n",
        "        # Double check each resource has tasks\n",
        "        for resource in self.resources:\n",
        "            logger.info(f\"Resource {resource.type} now has {len(resource.task_queue)} tasks queued\")\n",
        "\n",
        "        return distributed_tasks\n",
        "\n",
        "    def calculate_load_balancing_metrics(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate load balancing metrics based on resource utilization formula:\n",
        "        1. U(Ri) = L(Ri) / Makespan - Utilization of each resource\n",
        "        2. Uav = (Σ U(Ri)) / n - Average utilization across all resources\n",
        "\n",
        "        Returns:\n",
        "            Dict: Load balancing metrics\n",
        "        \"\"\"\n",
        "        # Start debug logging\n",
        "        logger.info(\"===== LOAD BALANCING METRICS CALCULATION STARTED =====\")\n",
        "\n",
        "        # Get makespan from metrics (correct order - define before using)\n",
        "        # Calculate makespan dynamically\n",
        "        makespan = max(\n",
        "            max(\n",
        "                (task.completion_time or 0)\n",
        "                for resource in self.resources\n",
        "                for task in resource.completed_tasks\n",
        "            ),\n",
        "            0.001  # Ensure a minimum makespan to avoid division by zero\n",
        "        )\n",
        "\n",
        "        # Debug output\n",
        "        logger.info(f\"DEBUG - Makespan value: {makespan}\")\n",
        "        logger.info(f\"DEBUG - self.metrics contents: {self.metrics}\")\n",
        "\n",
        "        # Check for valid makespan\n",
        "        if makespan <= 0:\n",
        "            logger.warning(\"Makespan is zero or negative, returning empty metrics\")\n",
        "            return {\n",
        "                'resource_utilizations': {},\n",
        "                'average_utilization': 0,\n",
        "                'load_balance_score': 0,\n",
        "                'utilization_std_dev': 0,\n",
        "                'resource_type_metrics': {}\n",
        "            }\n",
        "\n",
        "        # Calculate load on each resource\n",
        "        # L(Ri) = total processing time of all tasks assigned to Ri\n",
        "        resource_loads = {}\n",
        "        resource_utilizations = {}\n",
        "\n",
        "        # Group resources by type\n",
        "        smartphone_resources = []\n",
        "        raspberry_pi_resources = []\n",
        "        cloud_resources = []\n",
        "\n",
        "        # Check and fix task timing issues\n",
        "        task_timing_issues = 0\n",
        "        fixed_task_count = 0\n",
        "\n",
        "        # Process each resource\n",
        "        logger.info(\"Processing individual resource metrics:\")\n",
        "\n",
        "        for resource in self.resources:\n",
        "            # Group resource by type\n",
        "            if resource.type.startswith('Smartphone_'):\n",
        "                smartphone_resources.append(resource)\n",
        "            elif resource.type.startswith('Raspberry_'):\n",
        "                raspberry_pi_resources.append(resource)\n",
        "            elif resource.type.startswith('Cloud_'):\n",
        "                cloud_resources.append(resource)\n",
        "\n",
        "            # Fix any task timing issues\n",
        "            for task in resource.completed_tasks:\n",
        "                if not hasattr(task, 'actual_exec_time') or task.actual_exec_time is None:\n",
        "                    task_timing_issues += 1\n",
        "                    # Try to calculate from raw timestamps\n",
        "                    if hasattr(task, 'completion_time') and hasattr(task, 'exec_start_time'):\n",
        "                        if task.completion_time and task.exec_start_time:\n",
        "                            task.actual_exec_time = max(0.001, task.completion_time - task.exec_start_time)\n",
        "                            fixed_task_count += 1\n",
        "                            logger.info(f\"Fixed task {task.id} exec_time to {task.actual_exec_time:.4f}s\")\n",
        "                elif task.actual_exec_time <= 0:\n",
        "                    task_timing_issues += 1\n",
        "                    # Force a minimum value\n",
        "                    task.actual_exec_time = 0.001  # 1 millisecond minimum\n",
        "                    fixed_task_count += 1\n",
        "                    logger.info(f\"Corrected zero/negative exec_time for task {task.id}\")\n",
        "\n",
        "            # Debug task information\n",
        "            completed_task_count = len(resource.completed_tasks)\n",
        "            valid_task_count = sum(\n",
        "                1 for task in resource.completed_tasks\n",
        "                if hasattr(task, 'actual_exec_time') and task.actual_exec_time is not None and task.actual_exec_time > 0\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Resource {resource.id} ({resource.type}):\")\n",
        "            logger.info(f\"  - Completed tasks: {completed_task_count}\")\n",
        "            logger.info(f\"  - Tasks with valid execution time: {valid_task_count}\")\n",
        "\n",
        "            # Log individual task execution times\n",
        "            task_times_summary = []\n",
        "            for i, task in enumerate(resource.completed_tasks[:5]):  # Log first 5 tasks for brevity\n",
        "                task_exec_time = getattr(task, 'actual_exec_time', None)\n",
        "                task_times_summary.append(f\"Task {task.id}: {task_exec_time:.4f}s\" if task_exec_time else f\"Task {task.id}: None\")\n",
        "\n",
        "            logger.info(f\"  - Task times: {', '.join(task_times_summary)}\")\n",
        "\n",
        "            if completed_task_count > 5:\n",
        "                logger.info(f\"  - ... and {completed_task_count - 5} more tasks\")\n",
        "\n",
        "            # Calculate total processing time for all completed tasks\n",
        "            total_processing_time = sum(\n",
        "                task.actual_exec_time\n",
        "                for task in resource.completed_tasks\n",
        "                if hasattr(task, 'actual_exec_time') and task.actual_exec_time is not None and task.actual_exec_time > 0\n",
        "            )\n",
        "\n",
        "            logger.info(f\"  - Total processing time: {total_processing_time:.4f} seconds\")\n",
        "\n",
        "            # Calculate utilization: U(Ri) = L(Ri) / Makespan\n",
        "            utilization = total_processing_time / makespan if makespan > 0 else 0\n",
        "            logger.info(f\"  - Utilization (U(Ri) = L(Ri) / Makespan): {utilization:.4f}\")\n",
        "\n",
        "            resource_loads[resource.id] = total_processing_time\n",
        "            resource_utilizations[resource.id] = utilization\n",
        "\n",
        "        # Log task timing fixes\n",
        "        logger.info(f\"Found {task_timing_issues} tasks with timing issues, fixed {fixed_task_count}\")\n",
        "\n",
        "        # Check if we have any valid utilization data\n",
        "        if not resource_utilizations or all(util == 0 for util in resource_utilizations.values()):\n",
        "            logger.warning(\"All resources have zero utilization, metrics will be zero\")\n",
        "\n",
        "        # Calculate average utilization overall: Uav = (Σ U(Ri)) / n\n",
        "        average_utilization = sum(resource_utilizations.values()) / len(self.resources) if self.resources else 0\n",
        "        logger.info(f\"Average utilization across all resources (Uav): {average_utilization:.4f}\")\n",
        "\n",
        "        # Calculate standard deviation of utilizations (measure of balance)\n",
        "        utilization_values = list(resource_utilizations.values())\n",
        "        logger.info(f\"All utilization values: {utilization_values}\")\n",
        "\n",
        "        utilization_std_dev = np.std(utilization_values) if utilization_values else 0\n",
        "        logger.info(f\"Standard deviation of utilizations: {utilization_std_dev:.4f}\")\n",
        "\n",
        "        # Load balance score (lower is better - 0 is perfect balance)\n",
        "        # Using coefficient of variation to normalize\n",
        "        if average_utilization <= 0.0001:  # Very small threshold\n",
        "            logger.warning(\"Average utilization near zero, setting load balance score to 0\")\n",
        "            load_balance_score = 0\n",
        "        else:\n",
        "            load_balance_score = utilization_std_dev / average_utilization\n",
        "\n",
        "        logger.info(f\"Load balance score (CV = std_dev / avg): {load_balance_score:.4f}\")\n",
        "\n",
        "        # Calculate metrics per resource type\n",
        "        def calculate_resource_type_metrics(resources, type_name):\n",
        "            logger.info(f\"\\nCalculating metrics for {type_name} resources:\")\n",
        "\n",
        "            if not resources:\n",
        "                logger.info(f\"  No {type_name} resources found\")\n",
        "                return {\n",
        "                    'average_utilization': 0,\n",
        "                    'utilization_std_dev': 0,\n",
        "                    'load_balance_score': 0,\n",
        "                    'resource_count': 0\n",
        "                }\n",
        "\n",
        "            logger.info(f\"  Found {len(resources)} {type_name} resources\")\n",
        "            type_utilizations = [resource_utilizations[r.id] for r in resources]\n",
        "            logger.info(f\"  Utilization values: {type_utilizations}\")\n",
        "\n",
        "            type_avg_util = sum(type_utilizations) / len(resources)\n",
        "            logger.info(f\"  Average utilization: {type_avg_util:.4f}\")\n",
        "\n",
        "            type_std_dev = np.std(type_utilizations) if len(type_utilizations) > 1 else 0\n",
        "            logger.info(f\"  Standard deviation: {type_std_dev:.4f}\")\n",
        "\n",
        "            type_balance_score = type_std_dev / type_avg_util if type_avg_util > 0 else 0\n",
        "            logger.info(f\"  Balance score: {type_balance_score:.4f}\")\n",
        "\n",
        "            return {\n",
        "                'average_utilization': type_avg_util,\n",
        "                'utilization_std_dev': type_std_dev,\n",
        "                'load_balance_score': type_balance_score,\n",
        "                'resource_count': len(resources)\n",
        "            }\n",
        "\n",
        "        # Get metrics for each resource type\n",
        "        logger.info(\"\\nCalculating metrics by resource type:\")\n",
        "        resource_type_metrics = {\n",
        "            'smartphone': calculate_resource_type_metrics(smartphone_resources, \"smartphone\"),\n",
        "            'raspberry_pi': calculate_resource_type_metrics(raspberry_pi_resources, \"raspberry_pi\"),\n",
        "            'cloud': calculate_resource_type_metrics(cloud_resources, \"cloud\")\n",
        "        }\n",
        "\n",
        "        # Calculate inter-type balance\n",
        "        logger.info(\"\\nCalculating inter-type balance metrics:\")\n",
        "        type_avg_utils = [\n",
        "            resource_type_metrics['smartphone']['average_utilization'],\n",
        "            resource_type_metrics['raspberry_pi']['average_utilization'],\n",
        "            resource_type_metrics['cloud']['average_utilization']\n",
        "        ]\n",
        "        logger.info(f\"  Resource type average utilizations: {type_avg_utils}\")\n",
        "\n",
        "        inter_type_std_dev = np.std(type_avg_utils)\n",
        "        logger.info(f\"  Inter-type standard deviation: {inter_type_std_dev:.4f}\")\n",
        "\n",
        "        inter_type_avg = sum(type_avg_utils) / 3 if sum(type_avg_utils) > 0 else 0\n",
        "        logger.info(f\"  Inter-type average: {inter_type_avg:.4f}\")\n",
        "\n",
        "        inter_type_balance = inter_type_std_dev / inter_type_avg if inter_type_avg > 0 else 0\n",
        "        logger.info(f\"  Inter-type balance score: {inter_type_balance:.4f}\")\n",
        "\n",
        "        resource_type_metrics['inter_type_balance'] = {\n",
        "            'standard_deviation': inter_type_std_dev,\n",
        "            'balance_score': inter_type_balance\n",
        "        }\n",
        "\n",
        "        logger.info(\"===== LOAD BALANCING METRICS CALCULATION COMPLETED =====\")\n",
        "\n",
        "        # Force log flush\n",
        "        for handler in logger.handlers:\n",
        "            if isinstance(handler, logging.FileHandler):\n",
        "                handler.flush()\n",
        "\n",
        "        return {\n",
        "            'resource_utilizations': resource_utilizations,\n",
        "            'average_utilization': average_utilization,\n",
        "            'load_balance_score': load_balance_score,\n",
        "            'utilization_std_dev': utilization_std_dev,\n",
        "            'resource_type_metrics': resource_type_metrics\n",
        "        }\n",
        "\n",
        "    def _get_edge_cloud_distribution(self, solution):\n",
        "        \"\"\"Get distribution of tasks between edge and cloud resources\"\"\"\n",
        "        cloud_tasks = 0\n",
        "        edge_tasks = 0\n",
        "\n",
        "        for task, resource in solution.items():\n",
        "            if resource:  # Skip unassigned tasks\n",
        "                if resource.type.startswith(\"Cloud_\"):\n",
        "                    cloud_tasks += 1\n",
        "                else:  # Edge resources (Raspberry Pi, Smartphone, etc.)\n",
        "                    edge_tasks += 1\n",
        "\n",
        "        total_assigned = cloud_tasks + edge_tasks\n",
        "\n",
        "        if total_assigned > 0:\n",
        "            cloud_percentage = (cloud_tasks / total_assigned) * 100\n",
        "            edge_percentage = (edge_tasks / total_assigned) * 100\n",
        "        else:\n",
        "            cloud_percentage = 0\n",
        "            edge_percentage = 0\n",
        "\n",
        "        return {\n",
        "            \"edge_tasks\": edge_tasks,\n",
        "            \"cloud_tasks\": cloud_tasks,\n",
        "            \"edge_percentage\": edge_percentage,\n",
        "            \"cloud_percentage\": cloud_percentage\n",
        "        }\n",
        "\n",
        "    def genetic_modified(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Pure Genetic Algorithm for task scheduling (extracted from the hybrid approach).\n",
        "        This version focuses solely on genetic algorithm optimization without Tabu Search.\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Total number of tasks to distribute\n",
        "\n",
        "        Returns:\n",
        "            List[Task]: Distributed tasks with optimized resource assignments\n",
        "        \"\"\"\n",
        "        # Log start time and show progress indicator\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"\\n⏳ Starting Genetic Algorithm for {total_tasks} tasks...\")\n",
        "        logger.info(\"Phase 1/3: Task generation and initialization...\")\n",
        "\n",
        "        # Initialize CSV tracking for algorithm progress\n",
        "        csv_folder = \"/content/drive/My Drive/EdgeSimPy/algorithm_tracking\"\n",
        "        os.makedirs(csv_folder, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        ga_csv_path = os.path.join(csv_folder, f'ga_evolution_{timestamp}.csv')\n",
        "        solution_csv_path = os.path.join(csv_folder, f'solution_comparison_{timestamp}.csv')\n",
        "\n",
        "        # Initialize CSV files with headers\n",
        "        with open(ga_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Generation', 'Best_Fitness', 'Average_Fitness', 'Population_Diversity',\n",
        "                            'Mutation_Rate', 'Crossover_Rate', 'Tasks_Per_Resource',\n",
        "                            'Load_Balance_Score', 'Solution_Hash'])\n",
        "\n",
        "        with open(solution_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Algorithm_Phase', 'Timestamp', 'Solution_Cost', 'Load_Balance_Score',\n",
        "                            'Resource_Distribution', 'Task_Type_Distribution', 'Average_Execution_Time',\n",
        "                            'Worst_Task_Time', 'Best_Task_Time'])\n",
        "\n",
        "        try:\n",
        "            # === STEP 1: Generate tasks with batch processing for efficiency ===\n",
        "            tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "            # Pre-compute resource capabilities for faster lookup\n",
        "            RESOURCE_CAPABILITIES = {\n",
        "                resource.id: {\n",
        "                    'type': resource.type,\n",
        "                    'cpu_rating': resource.total_cpu_rating,\n",
        "                    'bandwidth': resource.total_bandwidth,\n",
        "                    'is_cloud': resource.type.startswith(\"Cloud_\"),\n",
        "                    'memory': resource.total_memory,\n",
        "                } for resource in self.resources\n",
        "            }\n",
        "\n",
        "            # Helper methods for tracking metrics\n",
        "            def _calculate_population_diversity(population):\n",
        "                \"\"\"Calculate population diversity as percentage of different assignments\"\"\"\n",
        "                if len(population) <= 1:\n",
        "                    return 0\n",
        "\n",
        "                # Take the first solution as reference\n",
        "                reference = population[0]\n",
        "                total_assignments = len(reference) * (len(population) - 1)\n",
        "                different_assignments = 0\n",
        "\n",
        "                # Compare each solution with the reference\n",
        "                for solution in population[1:]:\n",
        "                    for task in reference:\n",
        "                        if task in solution and reference[task] != solution[task]:\n",
        "                            different_assignments += 1\n",
        "\n",
        "                return (different_assignments / total_assignments) * 100 if total_assignments > 0 else 0\n",
        "\n",
        "            def _get_resource_distribution(solution):\n",
        "                \"\"\"Get distribution of tasks per resource\"\"\"\n",
        "                distribution = {resource.type: 0 for resource in self.resources}\n",
        "\n",
        "                for task, resource in solution.items():\n",
        "                    if resource:\n",
        "                        distribution[resource.type] += 1\n",
        "\n",
        "                return distribution\n",
        "\n",
        "            def _get_task_type_distribution(solution):\n",
        "                \"\"\"Get distribution of task types per resource type\"\"\"\n",
        "                distribution = {}\n",
        "\n",
        "                for task, resource in solution.items():\n",
        "                    if resource:\n",
        "                        resource_type = resource.type.split('_')[0]  # Cloud, Smartphone, or Raspberry\n",
        "                        task_type = task.type\n",
        "\n",
        "                        if resource_type not in distribution:\n",
        "                            distribution[resource_type] = {}\n",
        "\n",
        "                        if task_type not in distribution[resource_type]:\n",
        "                            distribution[resource_type][task_type] = 0\n",
        "\n",
        "                        distribution[resource_type][task_type] += 1\n",
        "\n",
        "                return distribution\n",
        "\n",
        "            def fast_exec_time_estimate(task, resource_id):\n",
        "                \"\"\"Simplified execution time estimation for speed\"\"\"\n",
        "                resource = RESOURCE_CAPABILITIES[resource_id]\n",
        "\n",
        "                # Basic estimate combining transfer and processing time\n",
        "                transfer_time = ((task.input_size + task.output_size) * 1024) / resource['bandwidth']\n",
        "                process_time = task.total_cpu_required / resource['cpu_rating']\n",
        "\n",
        "\n",
        "                # Simplified total time calculation\n",
        "                return transfer_time + process_time\n",
        "\n",
        "            def simplified_fitness(solution):\n",
        "                \"\"\"\n",
        "                Fitness function based solely on equation (1) from page 4716 of the paper:\n",
        "                minimize (1/m) * Σ(j=1 to m) Cmax(j)\n",
        "\n",
        "                Where:\n",
        "                - m: Number of resources/virtual machines\n",
        "                - Cmax(j): Maximum completion time (makespan) for resource j\n",
        "\n",
        "                This minimizes the mean makespan across all resources to achieve\n",
        "                load balancing and overall system performance.\n",
        "\n",
        "                Args:\n",
        "                    solution: Dict mapping tasks to resources\n",
        "\n",
        "                Returns:\n",
        "                    float: Fitness value (lower is better)\n",
        "                \"\"\"\n",
        "                # Return high penalty for empty solutions\n",
        "                if not solution:\n",
        "                    return float('inf')\n",
        "\n",
        "                # Get total number of resources/VMs (m)\n",
        "                m = len(self.resources)\n",
        "\n",
        "                # Track completion time for each resource\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                # Sort tasks by arrival time to respect task order\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                # Assign tasks and calculate completion times for each resource\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:  # Skip unassigned tasks\n",
        "                        continue\n",
        "\n",
        "                    # Calculate task execution time\n",
        "                    transfer_time = ((task.input_size + task.output_size) * 1024) / resource.total_bandwidth\n",
        "                    process_time = task.total_cpu_required / resource.total_cpu_rating\n",
        "                    total_task_time = transfer_time + process_time\n",
        "\n",
        "                    # Task starts at either its arrival time or when resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + total_task_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                # Calculate individual resource makespans (Cmax(j) for each j)\n",
        "                resource_makespans = list(resource_completion_times.values())\n",
        "\n",
        "                # Calculate mean makespan as per equation (1): (1/m) * Σ(j=1 to m) Cmax(j)\n",
        "                mean_makespan = sum(resource_makespans) / m if m > 0 else float('inf')\n",
        "\n",
        "                # Add penalty only for unassigned tasks\n",
        "                unassigned_count = len([1 for _, resource in solution.items() if not resource])\n",
        "                if unassigned_count > 0:\n",
        "                    mean_makespan += unassigned_count * 10000\n",
        "\n",
        "                # The fitness is purely the mean makespan without any additional penalties\n",
        "                return mean_makespan\n",
        "\n",
        "            # Faster task creation with larger batch size\n",
        "            batch_size = 500\n",
        "            initial_tasks = []\n",
        "            resource_loads = {r.id: 0 for r in self.resources}\n",
        "\n",
        "            # Show progress for task creation\n",
        "            logger.info(f\"Creating {total_tasks} tasks in batches of {batch_size}...\")\n",
        "\n",
        "            # Define the TASK_TYPES dictionary\n",
        "            TASK_TYPES = {\n",
        "                \"RT1\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "                \"RT2\": {\"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "                \"RT3\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "                \"RT4\": {\"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "                \"WT1\": {\"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "                \"WT2\": {\"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "                \"WT3\": {\"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "                \"WT4\": {\"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "            }\n",
        "\n",
        "            for i in range(0, total_tasks, batch_size):\n",
        "                batch_end = min(i + batch_size, total_tasks)\n",
        "                batch_tasks = []\n",
        "\n",
        "                for j in range(i, batch_end):\n",
        "                    task_record = tasks[j]\n",
        "                    # Use random selection for task type\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    task = Task(\n",
        "                        task_id=task_record.id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "                    task.arrival_time = task_record.arrival_time\n",
        "                    batch_tasks.append(task)\n",
        "\n",
        "                initial_tasks.extend(batch_tasks)\n",
        "                logger.info(f\"Created {min(batch_end, total_tasks)}/{total_tasks} tasks ({(min(batch_end, total_tasks)/total_tasks)*100:.1f}%)\")\n",
        "\n",
        "            # Special handling for very small task counts\n",
        "            if total_tasks <= 10:\n",
        "                logger.info(f\"Small task count ({total_tasks}) detected - using specialized optimization parameters\")\n",
        "                # Smaller population, fewer generations for tiny problems\n",
        "                population_size = max(5, min(10, total_tasks))\n",
        "                generations = 3\n",
        "                max_stagnation = 2\n",
        "            else:\n",
        "                # === STEP 2: Adjusted optimization parameters for larger task counts ===\n",
        "                population_size = min(50, int(total_tasks * 0.1))  # Proportional to total tasks\n",
        "                generations = min(20, int(total_tasks * 0.05))     # More generations for larger problems\n",
        "                max_stagnation = 10    # Increased early stopping patience\n",
        "\n",
        "            # Initialize GA parameters\n",
        "            # === Initialize epsilon decay parameters ===\n",
        "            epsilon_start = 1.0  # Start with full exploration\n",
        "            epsilon_end = 0.1    # End with minimal exploration\n",
        "            ga_epsilon_decay_rate = (epsilon_start - epsilon_end) / generations\n",
        "            current_ga_epsilon = epsilon_start\n",
        "\n",
        "            # With these adaptive mutation parameters\n",
        "            base_mutation_rate = 0.15\n",
        "            min_mutation_rate = 0.1\n",
        "            max_mutation_rate = 0.4\n",
        "            current_mutation_rate = base_mutation_rate\n",
        "            diversity_threshold_low = 15  # Percentage below which to increase mutation\n",
        "            diversity_threshold_high = 40  # Percentage above which to decrease mutation\n",
        "            crossover_rate = 0.8\n",
        "            elite_count = 2\n",
        "\n",
        "            # === STEP 3: Create random initial solution ===\n",
        "            logger.info(\"Phase 2/3: Creating initial population...\")\n",
        "\n",
        "            # Create initial population\n",
        "            population = []\n",
        "            for i in range(population_size):\n",
        "                # Create completely random solution\n",
        "                solution = {}\n",
        "                for task in initial_tasks:\n",
        "                    solution[task] = random.choice(self.resources)\n",
        "\n",
        "                population.append(solution)\n",
        "\n",
        "            # === STEP 4: Run Optimized Genetic Algorithm ===\n",
        "            logger.info(\"Phase 3/3: Running genetic algorithm optimization...\")\n",
        "\n",
        "            # Evaluate initial population\n",
        "            fitnesses = [simplified_fitness(sol) for sol in population]\n",
        "\n",
        "            # Track best solution\n",
        "            best_idx = fitnesses.index(min(fitnesses))\n",
        "            best_solution = copy.deepcopy(population[best_idx])\n",
        "            best_fitness = fitnesses[best_idx]\n",
        "            best_cost = simplified_fitness(best_solution)\n",
        "\n",
        "            # Record initial state for the solution comparison CSV\n",
        "            initial_resource_distribution = json.dumps(_get_resource_distribution(best_solution))\n",
        "            initial_task_type_distribution = json.dumps(_get_task_type_distribution(best_solution))\n",
        "            initial_task_times = [fast_exec_time_estimate(task, resource.id) for task, resource in best_solution.items()]\n",
        "            avg_time = sum(initial_task_times) / len(initial_task_times) if initial_task_times else 0\n",
        "            worst_time = max(initial_task_times) if initial_task_times else 0\n",
        "            best_time = min(initial_task_times) if initial_task_times else 0\n",
        "\n",
        "            # Calculate initial load balance\n",
        "            load_balance = np.std([len([t for t, r in best_solution.items() if r.id == res.id]) for res in self.resources])\n",
        "\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Initial_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    best_fitness,  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    initial_resource_distribution,  # Resource distribution\n",
        "                    initial_task_type_distribution,  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "            # Add edge-cloud distribution logging here\n",
        "            ga_edge_cloud_dist = self._get_edge_cloud_distribution(best_solution)\n",
        "            logger.info(f\"GA Solution - Edge: {ga_edge_cloud_dist['edge_percentage']:.2f}%, Cloud: {ga_edge_cloud_dist['cloud_percentage']:.2f}%\")\n",
        "\n",
        "            # Run genetic algorithm with progress updates\n",
        "            stagnation_counter = 0\n",
        "\n",
        "            for gen in range(generations):\n",
        "                # Progress indicator\n",
        "                current_cost = simplified_fitness(best_solution)\n",
        "                logger.info(f\"Generation {gen+1}/{generations} - Current best cost: {current_cost}\")\n",
        "\n",
        "                # Update epsilon for this generation\n",
        "                current_ga_epsilon = max(epsilon_end, epsilon_start - gen * ga_epsilon_decay_rate)\n",
        "                logger.info(f\"Current GA epsilon: {current_ga_epsilon:.3f}\")\n",
        "\n",
        "                # Sort by fitness and retain elites\n",
        "                sorted_indices = sorted(range(len(fitnesses)), key=lambda i: fitnesses[i])\n",
        "                new_population = [copy.deepcopy(population[i]) for i in sorted_indices[:elite_count]]\n",
        "\n",
        "                # Fill remaining population with crossover and mutation\n",
        "                while len(new_population) < population_size:\n",
        "                    # Use epsilon-greedy selection for parent selection\n",
        "                    if random.random() < current_ga_epsilon:\n",
        "                        # Exploration: Random selection\n",
        "                        parent1_idx = random.randrange(len(population))\n",
        "                        parent2_idx = random.randrange(len(population))\n",
        "                    else:\n",
        "                        # Exploitation: Tournament selection\n",
        "                        tournament_size = min(3, len(population))\n",
        "                        parent1_idx = min(random.sample(range(len(population)), tournament_size),\n",
        "                                        key=lambda i: fitnesses[i])\n",
        "                        parent2_idx = min(random.sample(range(len(population)), tournament_size),\n",
        "                                        key=lambda i: fitnesses[i])\n",
        "\n",
        "                    parent1 = population[parent1_idx]\n",
        "                    parent2 = population[parent2_idx]\n",
        "\n",
        "                    # Adaptive crossover rate based on epsilon\n",
        "                    effective_crossover_rate = crossover_rate * (1 - current_ga_epsilon * 0.3)\n",
        "\n",
        "                    # Simplified crossover\n",
        "                    if random.random() < effective_crossover_rate:\n",
        "                        child = {}\n",
        "\n",
        "                        # Create a random crossover point\n",
        "                        tasks_ordered = sorted(parent1.keys(), key=lambda t: t.arrival_time)\n",
        "                        crossover_point = random.randint(1, len(tasks_ordered) - 1)\n",
        "\n",
        "                        # Take first part from parent1, second from parent2\n",
        "                        for i, task in enumerate(tasks_ordered):\n",
        "                            if i < crossover_point:\n",
        "                                child[task] = parent1[task]\n",
        "                            else:\n",
        "                                # Make sure task exists in parent2\n",
        "                                if task in parent2:\n",
        "                                    child[task] = parent2[task]\n",
        "                                else:\n",
        "                                    child[task] = parent1[task]\n",
        "                    else:\n",
        "                        # No crossover, just clone parent1\n",
        "                        child = copy.deepcopy(parent1)\n",
        "\n",
        "                    # Epsilon-influenced mutation\n",
        "                    # Higher mutation when epsilon is high (exploration), lower when epsilon is low (exploitation)\n",
        "                    epsilon_adjusted_mutation_rate = current_mutation_rate * current_ga_epsilon + min_mutation_rate\n",
        "\n",
        "                    # Simple mutation\n",
        "                    if random.random() < epsilon_adjusted_mutation_rate:\n",
        "                        # Adaptive mutation count based on epsilon\n",
        "                        # More aggressive mutation when epsilon is high\n",
        "                        mutation_ratio = 0.05 + current_ga_epsilon * 0.15  # 5-20% mutation\n",
        "                        mutation_count = max(1, int(len(child) * mutation_ratio))\n",
        "                        tasks_to_mutate = random.sample(list(child.keys()), k=mutation_count)\n",
        "\n",
        "                        for task in tasks_to_mutate:\n",
        "                            child[task] = random.choice(self.resources)\n",
        "\n",
        "                    new_population.append(child)\n",
        "\n",
        "                # Update population\n",
        "                population = new_population\n",
        "\n",
        "                # Calculate fitness - evaluate each solution\n",
        "                fitnesses = [simplified_fitness(sol) for sol in population]\n",
        "\n",
        "                # Check for improvement\n",
        "                current_best_idx = fitnesses.index(min(fitnesses))\n",
        "                current_best_fitness = fitnesses[current_best_idx]\n",
        "\n",
        "                if current_best_fitness < best_fitness:\n",
        "                    best_solution = copy.deepcopy(population[current_best_idx])\n",
        "                    best_fitness = current_best_fitness\n",
        "                    best_cost = simplified_fitness(best_solution)\n",
        "                    logger.info(f\"✓ New best solution found! Fitness: {best_fitness}, Cost: {best_cost}\")\n",
        "                    stagnation_counter = 0\n",
        "                else:\n",
        "                    stagnation_counter += 1\n",
        "\n",
        "                # Calculate diversity (percentage of different assignments between solutions)\n",
        "                diversity = _calculate_population_diversity(population)\n",
        "\n",
        "                if diversity < diversity_threshold_low:\n",
        "                    # Low diversity - increase mutation rate to encourage exploration\n",
        "                    current_mutation_rate = min(max_mutation_rate, current_mutation_rate * 1.5)\n",
        "                    logger.info(f\"Low diversity ({diversity:.1f}%) - increasing mutation rate to {current_mutation_rate:.3f}\")\n",
        "                elif diversity > diversity_threshold_high:\n",
        "                    # High diversity - decrease mutation rate to encourage exploitation\n",
        "                    current_mutation_rate = max(min_mutation_rate, current_mutation_rate * 0.8)\n",
        "                    logger.info(f\"High diversity ({diversity:.1f}%) - decreasing mutation rate to {current_mutation_rate:.3f}\")\n",
        "\n",
        "                # Calculate resource distribution\n",
        "                resource_distribution = _get_resource_distribution(best_solution)\n",
        "\n",
        "                # Calculate task type distribution\n",
        "                task_type_distribution = _get_task_type_distribution(best_solution)\n",
        "\n",
        "                # Calculate task execution times\n",
        "                task_times = [fast_exec_time_estimate(task, resource.id)\n",
        "                            for task, resource in best_solution.items()]\n",
        "                avg_time = sum(task_times) / len(task_times) if task_times else 0\n",
        "                worst_time = max(task_times) if task_times else 0\n",
        "                best_time = min(task_times) if task_times else 0\n",
        "\n",
        "                # Create a hash to track solution differences\n",
        "                solution_hash = hash(frozenset((task.id, resource.id)\n",
        "                                            for task, resource in best_solution.items()))\n",
        "\n",
        "                # Calculate load balance score\n",
        "                load_balance = np.std([len([t for t, r in best_solution.items() if r.id == res.id])\n",
        "                                    for res in self.resources])\n",
        "\n",
        "                # Write GA progress to CSV\n",
        "                with open(ga_csv_path, 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\n",
        "                        gen + 1,  # Generation number\n",
        "                        best_fitness,  # Best fitness\n",
        "                        sum(fitnesses) / len(fitnesses),  # Average fitness\n",
        "                        diversity,  # Population diversity\n",
        "                        current_mutation_rate,  # Mutation rate\n",
        "                        crossover_rate,  # Crossover rate\n",
        "                        json.dumps(resource_distribution),  # Tasks per resource\n",
        "                        load_balance,  # Load balance score\n",
        "                        solution_hash  # Solution hash\n",
        "                    ])\n",
        "\n",
        "                # Record overall solution at end of each generation\n",
        "                if gen % 5 == 0 or gen == generations - 1:  # Every 5 generations and the last one\n",
        "                    with open(solution_csv_path, 'a', newline='') as f:\n",
        "                        writer = csv.writer(f)\n",
        "                        writer.writerow([\n",
        "                            f\"GA_Gen_{gen+1}\",  # Algorithm phase\n",
        "                            datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                            best_fitness,  # Solution cost\n",
        "                            load_balance,  # Load balance score\n",
        "                            json.dumps(resource_distribution),  # Resource distribution\n",
        "                            json.dumps(task_type_distribution),  # Task type distribution\n",
        "                            avg_time,  # Average execution time\n",
        "                            worst_time,  # Worst task time\n",
        "                            best_time  # Best task time\n",
        "                        ])\n",
        "\n",
        "                # Early stopping\n",
        "                if stagnation_counter >= max_stagnation:\n",
        "                    logger.info(f\"Early stopping at generation {gen+1} (no improvement for {max_stagnation} generations)\")\n",
        "                    break\n",
        "\n",
        "            # Record final GA solution\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Final_GA_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    best_fitness,  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    json.dumps(_get_resource_distribution(best_solution)),  # Resource distribution\n",
        "                    json.dumps(_get_task_type_distribution(best_solution)),  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "\n",
        "            # === STEP 5: Finalize solution and apply to resources ===\n",
        "            logger.info(\"Phase 3/3: Finalizing solution and distributing tasks...\")\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            # Calculate and log final edge-cloud distribution\n",
        "            final_edge_cloud_dist = self._get_edge_cloud_distribution(best_solution)\n",
        "            logger.info(f\"Final Solution - Edge: {final_edge_cloud_dist['edge_percentage']:.2f}%, Cloud: {final_edge_cloud_dist['cloud_percentage']:.2f}%\")\n",
        "\n",
        "            # Print summary\n",
        "            logger.info(\"\\n=== Optimization Complete ===\")\n",
        "            logger.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
        "            # Include edge-cloud distribution in summary\n",
        "            logger.info(f\"Edge-Cloud Distribution: {final_edge_cloud_dist['edge_percentage']:.2f}% Edge, {final_edge_cloud_dist['cloud_percentage']:.2f}% Cloud\")\n",
        "            # Reset resources - explicitly clear queues\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = []\n",
        "                resource.failed_tasks = []\n",
        "                logger.info(f\"Reset {resource.type} queue to 0 tasks\")\n",
        "\n",
        "            # Apply solution to resources\n",
        "            distributed_tasks = []\n",
        "            assignment_data = []\n",
        "\n",
        "            # Task distribution counts\n",
        "            resource_assignment_counts = {r.type: 0 for r in self.resources}\n",
        "            # Temporary task assignment tracking\n",
        "            temp_resource_task_queues = {resource.id: [] for resource in self.resources}\n",
        "\n",
        "            # Process and apply all assignments, logging each step\n",
        "            logger.info(f\"Processing {len(best_solution)} task assignments...\")\n",
        "            assignment_count = 0\n",
        "\n",
        "            # Set reference to task's assigned resource\n",
        "            for task, resource in best_solution.items():\n",
        "                # Verify task is valid\n",
        "                if task is None:\n",
        "                    logger.error(\"Found None task in solution!\")\n",
        "                    continue\n",
        "\n",
        "                # Handle case where resource is None by assigning to least loaded resource\n",
        "                if resource is None:\n",
        "                    logger.warning(f\"Task {task.id} has no resource assignment, assigning to least loaded resource\")\n",
        "                    continue\n",
        "\n",
        "                # Set task status and add to queue\n",
        "                task.status = 'CREATED'  # Ensure correct initial status\n",
        "                task.assigned_resource = resource  # Maintain reference to resource\n",
        "                temp_resource_task_queues[resource.id].append(task)\n",
        "                resource_assignment_counts[resource.type] += 1\n",
        "                assignment_count += 1\n",
        "\n",
        "                # Record assignment\n",
        "                record = {\n",
        "                    'Task ID': task.id,\n",
        "                    'Type': task.type,\n",
        "                    'Input Size (GB)': task.input_size,\n",
        "                    'Output Size (GB)': task.output_size,\n",
        "                    'Time of Arrival': datetime.fromtimestamp(task.arrival_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'Status': task.status,\n",
        "                    'Assigned Node': resource.type if resource else 'None',\n",
        "                    'Estimated Time': fast_exec_time_estimate(task, resource.id) if resource else 'N/A'\n",
        "                }\n",
        "                assignment_data.append(record)\n",
        "                distributed_tasks.append(task)\n",
        "\n",
        "            logger.info(f\"Completed {assignment_count} task assignments\")\n",
        "\n",
        "            # Write to CSV\n",
        "            csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "            os.makedirs(csv_folder, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_filepath = os.path.join(csv_folder, f'genetic_algorithm_{timestamp}.csv')\n",
        "\n",
        "            # Ensure the CSV is properly created and written\n",
        "            try:\n",
        "                with open(csv_filepath, mode='w', newline='') as f:\n",
        "                    fieldnames = [\n",
        "                        'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                        'Time of Arrival', 'Status', 'Assigned Node',\n",
        "                        'Estimated Time'\n",
        "                    ]\n",
        "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(assignment_data)\n",
        "\n",
        "                logger.info(f\"Successfully wrote assignment data to: {csv_filepath}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error writing to CSV file: {e}\")\n",
        "                # Try alternate location if Google Drive isn't accessible\n",
        "                alt_csv_filepath = f'genetic_algorithm_{timestamp}.csv'\n",
        "                with open(alt_csv_filepath, mode='w', newline='') as f:\n",
        "                    fieldnames = [\n",
        "                        'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                        'Time of Arrival', 'Status', 'Assigned Node',\n",
        "                        'Estimated Time'\n",
        "                    ]\n",
        "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(assignment_data)\n",
        "                logger.info(f\"Wrote assignment data to alternate location: {alt_csv_filepath}\")\n",
        "                csv_filepath = alt_csv_filepath\n",
        "\n",
        "            # Print summary\n",
        "            logger.info(\"\\n=== Optimization Complete ===\")\n",
        "            logger.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
        "            logger.info(f\"Final solution cost: {simplified_fitness(best_solution)}\")\n",
        "            logger.info(f\"Task distribution:\")\n",
        "            for resource_type, count in resource_assignment_counts.items():\n",
        "                if count > 0:\n",
        "                    logger.info(f\"  {resource_type}: {count} tasks\")\n",
        "            logger.info(f\"Results saved to: {csv_filepath}\")\n",
        "\n",
        "            # Add detailed debug information to verify task distribution\n",
        "            total_queued = 0\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = temp_resource_task_queues[resource.id]\n",
        "                logger.info(f\"Copied {len(resource.task_queue)} tasks to {resource.type} queue\")\n",
        "                queued = len(resource.task_queue)\n",
        "                total_queued += queued\n",
        "                logger.info(f\"After distribution: {resource.type} has {queued} tasks in queue\")\n",
        "\n",
        "            logger.info(f\"Total tasks queued: {total_queued} of {total_tasks} requested\")\n",
        "\n",
        "            # Return CSV paths alongside the distributed tasks\n",
        "            return distributed_tasks, {\n",
        "                'ga_csv': ga_csv_path,\n",
        "                'solution_csv': solution_csv_path,\n",
        "                'final_assignments': csv_filepath\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Genetic algorithm: {e}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "\n",
        "            # Just raise the exception instead of falling back to round-robin\n",
        "            logger.error(\"Genetic algorithm failed without fallback mechanism enabled\")\n",
        "            raise\n",
        "    def _standard_gwo_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Grey Wolf Optimizer for task scheduling in edge-cloud environments.\n",
        "\n",
        "        This algorithm uses the standard GWO position update equations,\n",
        "        adapted for discrete task scheduling.\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Total number of tasks to distribute\n",
        "\n",
        "        Returns:\n",
        "            List[Task]: Distributed tasks with optimized resource assignments\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"\\n⏳ Starting Standard Grey Wolf Optimization Algorithm for {total_tasks} tasks...\")\n",
        "\n",
        "        try:\n",
        "            # === STEP 1: Generate tasks and initialize task types ===\n",
        "            logger.info(\"Phase 1/4: Task generation and initialization...\")\n",
        "            tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "            # Define task types with requirements\n",
        "            TASK_TYPES = {\n",
        "                \"RT1\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "                \"RT2\": {\"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "                \"RT3\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "                \"RT4\": {\"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "                \"WT1\": {\"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "                \"WT2\": {\"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "                \"WT3\": {\"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "                \"WT4\": {\"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "            }\n",
        "\n",
        "            # Pre-compute resource capabilities for faster lookup\n",
        "            RESOURCE_CAPABILITIES = {\n",
        "                resource.id: {\n",
        "                    'type': resource.type,\n",
        "                    'cpu_rating': resource.total_cpu_rating,\n",
        "                    'bandwidth': resource.total_bandwidth,\n",
        "                    'is_cloud': resource.type.startswith(\"Cloud_\"),\n",
        "                    'memory': resource.total_memory,\n",
        "                } for resource in self.resources\n",
        "            }\n",
        "\n",
        "            # Create tasks with batch processing\n",
        "            batch_size = 500\n",
        "            initial_tasks = []\n",
        "\n",
        "            for i in range(0, total_tasks, batch_size):\n",
        "                batch_end = min(i + batch_size, total_tasks)\n",
        "                batch_tasks = []\n",
        "\n",
        "                for j in range(i, batch_end):\n",
        "                    task_record = tasks[j]\n",
        "                    # Use random selection:\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    task = Task(\n",
        "                        task_id=task_record.id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "                    task.arrival_time = task_record.arrival_time\n",
        "                    batch_tasks.append(task)\n",
        "\n",
        "                initial_tasks.extend(batch_tasks)\n",
        "                logger.info(f\"Created {min(batch_end, total_tasks)}/{total_tasks} tasks ({(min(batch_end, total_tasks)/total_tasks)*100:.1f}%)\")\n",
        "\n",
        "            # === STEP 2: Define optimization utility functions ===\n",
        "            logger.info(\"Phase 2/4: Setting up optimization parameters...\")\n",
        "\n",
        "            def calculate_execution_time(task, resource_id):\n",
        "                \"\"\"Calculate task execution time on a given resource\"\"\"\n",
        "                resource = RESOURCE_CAPABILITIES[resource_id]\n",
        "\n",
        "                # Calculate transfer time (input + output)\n",
        "                transfer_time = ((task.input_size + task.output_size) * 1024) / resource['bandwidth']\n",
        "\n",
        "                # Calculate processing time\n",
        "                process_time = task.total_cpu_required / resource['cpu_rating']\n",
        "\n",
        "\n",
        "                # Total execution time\n",
        "                return transfer_time + process_time\n",
        "\n",
        "            def calculate_objective_function(solution):\n",
        "                \"\"\"\n",
        "                Multi-objective function based on the formula:\n",
        "                F = [0.7 × Makespan + 0.3 × Utilization]\n",
        "\n",
        "                Where:\n",
        "                - Makespan: Maximum completion time across all resources\n",
        "                - Utilization: Uses the U(Ri) = L(Ri) / Makespan formula\n",
        "                \"\"\"\n",
        "                # Return high penalty for empty or invalid solutions\n",
        "                if not solution:\n",
        "                    return float('inf')\n",
        "\n",
        "                # Handle unassigned tasks with high penalty\n",
        "                unassigned_tasks = sum(1 for _, resource in solution.items() if not resource)\n",
        "                if unassigned_tasks > 0:\n",
        "                    return float('inf')\n",
        "\n",
        "                # 1. Calculate makespan and resource processing times\n",
        "                # Track completion time for each resource\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "                resource_processing_times = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                # Sort tasks by arrival time to respect task order\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                # Calculate completion times by processing tasks in order\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate task execution time\n",
        "                    exec_time = calculate_execution_time(task, resource.id)\n",
        "\n",
        "                    # Task starts at either its arrival time or when resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + exec_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                    # Track total processing time for each resource (for utilization)\n",
        "                    resource_processing_times[resource.id] += exec_time\n",
        "\n",
        "                # 2. Calculate makespan (maximum completion time across all resources)\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else float('inf')\n",
        "\n",
        "                # 3. Calculate utilization for each resource using U(Ri) = L(Ri) / Makespan\n",
        "                resource_utilizations = {}\n",
        "                for resource_id, processing_time in resource_processing_times.items():\n",
        "                    resource_utilizations[resource_id] = processing_time / makespan if makespan > 0 else 0\n",
        "\n",
        "                # 4. Calculate average utilization\n",
        "                avg_utilization = sum(resource_utilizations.values()) / len(self.resources) if self.resources else 0\n",
        "\n",
        "                # 5. Apply the F = [0.7 × Makespan + 0.3 × Utilization] formula\n",
        "                # Note: Since we want to maximize utilization but minimize makespan,\n",
        "                # we use (1 - avg_utilization) for the utilization component\n",
        "                objective_value = 0.7 * makespan + 0.3 * (1 - avg_utilization) * makespan\n",
        "\n",
        "                return objective_value\n",
        "\n",
        "            # === STEP 3: Initialize Grey Wolf Optimizer parameters ===\n",
        "            logger.info(\"Phase 3/4: Initializing Grey Wolf Optimizer...\")\n",
        "\n",
        "            # GWO parameters\n",
        "            num_wolves = min(30, int(total_tasks * 0.05))  # Number of search agents\n",
        "            max_iterations = min(25, int(total_tasks * 0.02))  # Maximum iterations\n",
        "\n",
        "            # Function to create a random initial solution\n",
        "            def create_initial_solution():\n",
        "                \"\"\"Create a random initial solution for wolves\"\"\"\n",
        "                solution = {}\n",
        "\n",
        "                # Simple random assignment for each task\n",
        "                for task in initial_tasks:\n",
        "                    # Randomly select any resource\n",
        "                    resource = random.choice(self.resources)\n",
        "                    solution[task] = resource\n",
        "\n",
        "                return solution\n",
        "\n",
        "            # Map resources to indices for vector operations\n",
        "            resource_idx_map = {resource: idx for idx, resource in enumerate(self.resources)}\n",
        "            idx_resource_map = {idx: resource for idx, resource in enumerate(self.resources)}\n",
        "            num_resources = len(self.resources)\n",
        "\n",
        "            # Initialize the wolf pack with purely random solutions\n",
        "            wolf_pack = []\n",
        "            for i in range(num_wolves):\n",
        "                solution = create_initial_solution()\n",
        "                # Each wolf gets a completely random initial solution\n",
        "\n",
        "                # Evaluate the solution\n",
        "                fitness = calculate_objective_function(solution)\n",
        "                wolf_pack.append({\"solution\": solution, \"fitness\": fitness})\n",
        "\n",
        "            # Sort wolves by fitness (ascending - lower is better)\n",
        "            wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "            # Initialize alpha, beta, and delta wolves (best three solutions)\n",
        "            alpha = wolf_pack[0]\n",
        "            beta = wolf_pack[1] if len(wolf_pack) > 1 else alpha\n",
        "            delta = wolf_pack[2] if len(wolf_pack) > 2 else beta\n",
        "\n",
        "            logger.info(f\"Initial alpha wolf fitness: {alpha['fitness']:.2f}\")\n",
        "\n",
        "            # === STEP 4: Run Grey Wolf Optimizer with standard equations ===\n",
        "            logger.info(\"Phase 4/4: Running Grey Wolf Optimization with standard equations...\")\n",
        "\n",
        "            for iteration in range(max_iterations):\n",
        "                # Update parameter a (linearly decreases from 2 to 0)\n",
        "                a = 2 - iteration * (2 / max_iterations)\n",
        "\n",
        "                # Update each wolf's position (solution)\n",
        "                for i in range(num_wolves):\n",
        "                    # Get current wolf\n",
        "                    wolf = wolf_pack[i]\n",
        "                    current_solution = wolf[\"solution\"]\n",
        "                    new_solution = {}\n",
        "\n",
        "                    # For each task, update its assignment using standard GWO equations\n",
        "                    for task in initial_tasks:\n",
        "                        # Get current resource assignment\n",
        "                        current_resource = current_solution.get(task, random.choice(self.resources))\n",
        "                        current_idx = resource_idx_map[current_resource]\n",
        "\n",
        "                        # Get alpha, beta, and delta positions for this task\n",
        "                        alpha_resource = alpha[\"solution\"].get(task, random.choice(self.resources))\n",
        "                        beta_resource = beta[\"solution\"].get(task, random.choice(self.resources))\n",
        "                        delta_resource = delta[\"solution\"].get(task, random.choice(self.resources))\n",
        "\n",
        "                        alpha_idx = resource_idx_map[alpha_resource]\n",
        "                        beta_idx = resource_idx_map[beta_resource]\n",
        "                        delta_idx = resource_idx_map[delta_resource]\n",
        "\n",
        "                        # Calculate A and C vectors for alpha, beta, delta\n",
        "                        A1, A2, A3 = 2 * a * random.random() - a, 2 * a * random.random() - a, 2 * a * random.random() - a\n",
        "                        C1, C2, C3 = 2 * random.random(), 2 * random.random(), 2 * random.random()\n",
        "\n",
        "                        # Calculate distance vectors (adapted for discrete indices)\n",
        "                        D_alpha = abs(C1 * alpha_idx - current_idx)\n",
        "                        D_beta = abs(C2 * beta_idx - current_idx)\n",
        "                        D_delta = abs(C3 * delta_idx - current_idx)\n",
        "\n",
        "                        # Calculate new position influences\n",
        "                        X1 = alpha_idx - A1 * D_alpha\n",
        "                        X2 = beta_idx - A2 * D_beta\n",
        "                        X3 = delta_idx - A3 * D_delta\n",
        "\n",
        "                        # Average position in continuous space\n",
        "                        avg_position = (X1 + X2 + X3) / 3\n",
        "\n",
        "                        # Convert to discrete resource index (nearest resource)\n",
        "                        # Use modulo to ensure it's within valid range\n",
        "                        nearest_idx = int(round(avg_position)) % num_resources\n",
        "                        new_resource = idx_resource_map[nearest_idx]\n",
        "\n",
        "                        # Apply the new assignment\n",
        "                        new_solution[task] = new_resource\n",
        "\n",
        "                    # Evaluate the new solution\n",
        "                    new_fitness = calculate_objective_function(new_solution)\n",
        "\n",
        "                    # Update the wolf if the new solution is better\n",
        "                    if new_fitness < wolf[\"fitness\"]:\n",
        "                        wolf_pack[i] = {\"solution\": new_solution, \"fitness\": new_fitness}\n",
        "\n",
        "                # Re-sort the wolf pack\n",
        "                wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "                # Update alpha, beta, delta wolves\n",
        "                new_alpha = wolf_pack[0]\n",
        "                new_beta = wolf_pack[1] if len(wolf_pack) > 1 else new_alpha\n",
        "                new_delta = wolf_pack[2] if len(wolf_pack) > 2 else new_beta\n",
        "\n",
        "                # Log progress if alpha improved\n",
        "                if new_alpha[\"fitness\"] < alpha[\"fitness\"]:\n",
        "                    logger.info(f\"Iteration {iteration+1}/{max_iterations}: New alpha fitness: {new_alpha['fitness']:.2f}\")\n",
        "\n",
        "                alpha, beta, delta = new_alpha, new_beta, new_delta\n",
        "\n",
        "            # Get the best solution from GWO (alpha wolf)\n",
        "            best_solution = alpha[\"solution\"]\n",
        "            best_fitness = alpha[\"fitness\"]\n",
        "\n",
        "            logger.info(f\"GWO completed. Best fitness: {best_fitness:.2f}\")\n",
        "\n",
        "            # Finalize solution and apply to resources\n",
        "            logger.info(\"Finalizing and distributing tasks...\")\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            logger.info(f\"Total algorithm execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "            # Reset resources\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = []\n",
        "                resource.failed_tasks = []\n",
        "\n",
        "            # Apply the final solution\n",
        "            distributed_tasks = []\n",
        "            assignment_data = []\n",
        "            resource_assignment_counts = {r.type: 0 for r in self.resources}\n",
        "            temp_resource_task_queues = {resource.id: [] for resource in self.resources}\n",
        "\n",
        "            # Process all assignments\n",
        "            for task, resource in best_solution.items():\n",
        "                if task is None or resource is None:\n",
        "                    continue\n",
        "\n",
        "                # Set task status and maintain reference\n",
        "                task.status = 'CREATED'\n",
        "                task.assigned_resource = resource\n",
        "                temp_resource_task_queues[resource.id].append(task)\n",
        "                resource_assignment_counts[resource.type] += 1\n",
        "\n",
        "                # Record the assignment\n",
        "                arrival_time = datetime.fromtimestamp(task.arrival_time)\n",
        "                record = {\n",
        "                    'Task ID': task.id,\n",
        "                    'Type': task.type,\n",
        "                    'Input Size (GB)': task.input_size,\n",
        "                    'Output Size (GB)': task.output_size,\n",
        "                    'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'Status': task.status,\n",
        "                    'Assigned Node': resource.type,\n",
        "                    'Estimated Time': calculate_execution_time(task, resource.id)\n",
        "                }\n",
        "                assignment_data.append(record)\n",
        "                distributed_tasks.append(task)\n",
        "\n",
        "            # Update resource queues\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = temp_resource_task_queues[resource.id]\n",
        "\n",
        "            # Save assignments to CSV\n",
        "            csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "            os.makedirs(csv_folder, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_filepath = os.path.join(csv_folder, f'standard_gwo_{timestamp}.csv')\n",
        "\n",
        "            with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "                fieldnames = [\n",
        "                    'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                    'Time of Arrival', 'Status', 'Assigned Node', 'Estimated Time'\n",
        "                ]\n",
        "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(assignment_data)\n",
        "\n",
        "            # Log distribution summary\n",
        "            total_tasks_assigned = sum(resource_assignment_counts.values())\n",
        "            logger.info(\"\\n=== Task Distribution Summary ===\")\n",
        "            logger.info(f\"Total tasks assigned: {total_tasks_assigned}/{total_tasks}\")\n",
        "\n",
        "            for resource_type, count in sorted(resource_assignment_counts.items()):\n",
        "                percent = (count / total_tasks_assigned) * 100 if total_tasks_assigned > 0 else 0\n",
        "                logger.info(f\"  {resource_type}: {count} tasks ({percent:.1f}%)\")\n",
        "\n",
        "            logger.info(f\"Final solution fitness: {best_fitness:.2f}\")\n",
        "            logger.info(f\"Results saved to: {csv_filepath}\")\n",
        "\n",
        "            return distributed_tasks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Standard Grey Wolf Optimization algorithm: {e}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "            raise\n",
        "    def _hybrid_gwo_tabu_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Hybrid Grey Wolf Optimizer and Tabu Search for task scheduling in edge-cloud environments.\n",
        "\n",
        "        This algorithm combines the global exploration capabilities of Grey Wolf Optimization with\n",
        "        the local exploitation strengths of Tabu Search to find an efficient task distribution.\n",
        "\n",
        "        Modified objective function focuses solely on minimizing makespan: f(Z) = min MK(Z)\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Total number of tasks to distribute\n",
        "\n",
        "        Returns:\n",
        "            List[Task]: Distributed tasks with optimized resource assignments\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"\\n⏳ Starting Hybrid GWO-Tabu Search Algorithm for {total_tasks} tasks...\")\n",
        "\n",
        "        try:\n",
        "            # === STEP 1: Generate tasks and initialize task types ===\n",
        "            logger.info(\"Phase 1/6: Task generation and initialization...\")\n",
        "            tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "            # Define task types with requirements\n",
        "            TASK_TYPES = {\n",
        "                \"RT1\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "                \"RT2\": {\"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "                \"RT3\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "                \"RT4\": {\"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "                \"WT1\": {\"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "                \"WT2\": {\"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "                \"WT3\": {\"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "                \"WT4\": {\"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "            }\n",
        "\n",
        "            # Pre-compute resource capabilities for faster lookup\n",
        "            RESOURCE_CAPABILITIES = {\n",
        "                resource.id: {\n",
        "                    'type': resource.type,\n",
        "                    'cpu_rating': resource.total_cpu_rating,\n",
        "                    'bandwidth': resource.total_bandwidth,\n",
        "                    'is_cloud': resource.type.startswith(\"Cloud_\"),\n",
        "                    'memory': resource.total_memory,\n",
        "                } for resource in self.resources\n",
        "            }\n",
        "\n",
        "            # Create tasks with batch processing\n",
        "            batch_size = 500\n",
        "            initial_tasks = []\n",
        "\n",
        "            for i in range(0, total_tasks, batch_size):\n",
        "                batch_end = min(i + batch_size, total_tasks)\n",
        "                batch_tasks = []\n",
        "\n",
        "                for j in range(i, batch_end):\n",
        "                    # Ensure we're using randomized task type selection\n",
        "                    task_record = tasks[j]\n",
        "                    # Always use random selection for task type without any patterns\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    task = Task(\n",
        "                        task_id=task_record.id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "                    task.arrival_time = task_record.arrival_time\n",
        "                    batch_tasks.append(task)\n",
        "\n",
        "                initial_tasks.extend(batch_tasks)\n",
        "                logger.info(f\"Created {min(batch_end, total_tasks)}/{total_tasks} tasks ({(min(batch_end, total_tasks)/total_tasks)*100:.1f}%)\")\n",
        "\n",
        "            # === STEP 2: Define optimization utility functions ===\n",
        "            logger.info(\"Phase 2/6: Setting up optimization parameters...\")\n",
        "\n",
        "            def calculate_execution_time(task, resource_id):\n",
        "                \"\"\"Calculate task execution time on a given resource\"\"\"\n",
        "                resource = RESOURCE_CAPABILITIES[resource_id]\n",
        "\n",
        "                # Calculate transfer time (input + output)\n",
        "                transfer_time = ((task.input_size + task.output_size) * 1024) / resource['bandwidth']\n",
        "\n",
        "                # Calculate processing time\n",
        "                process_time = task.total_cpu_required / resource['cpu_rating']\n",
        "\n",
        "\n",
        "                # Total execution time\n",
        "                return transfer_time + process_time\n",
        "\n",
        "            def calculate_makespan(solution):\n",
        "                \"\"\"\n",
        "                Calculate makespan (completion time of the last task to finish) for a solution.\n",
        "\n",
        "                Makespan is the maximum completion time across all resources, considering\n",
        "                task dependencies and resource constraints.\n",
        "\n",
        "                Args:\n",
        "                    solution: Dict mapping tasks to resources\n",
        "\n",
        "                Returns:\n",
        "                    float: Makespan value (lower is better)\n",
        "                \"\"\"\n",
        "                # Return high penalty for invalid solutions\n",
        "                if not solution:\n",
        "                    return float('inf')\n",
        "\n",
        "                # Track completion time for each resource\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                # Sort tasks by arrival time to respect task order\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                # Process each task in order\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:\n",
        "                        # Penalize unassigned tasks heavily\n",
        "                        return float('inf')\n",
        "\n",
        "                    # Calculate task execution time on this resource\n",
        "                    exec_time = calculate_execution_time(task, resource.id)\n",
        "\n",
        "                    # Task starts at either its arrival time or when resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + exec_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                # Makespan is the maximum completion time across all resources\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else float('inf')\n",
        "\n",
        "                return makespan\n",
        "\n",
        "            # === STEP 3: Initialize Grey Wolf Optimizer parameters ===\n",
        "            logger.info(\"Phase 3/6: Initializing Grey Wolf Optimizer...\")\n",
        "\n",
        "            # GWO parameters\n",
        "            num_wolves = min(30, int(total_tasks * 0.05))  # Number of search agents\n",
        "            max_iterations = min(25, int(total_tasks * 0.02))  # Maximum iterations\n",
        "\n",
        "            # Function to create a random initial solution\n",
        "            def create_initial_solution():\n",
        "                \"\"\"Create a completely random initial solution for wolves\"\"\"\n",
        "                solution = {}\n",
        "\n",
        "                # Simple random assignment for each task\n",
        "                for task in initial_tasks:\n",
        "                    # Randomly select any resource without any preferences\n",
        "                    resource = random.choice(self.resources)\n",
        "                    solution[task] = resource\n",
        "\n",
        "                return solution\n",
        "\n",
        "            # Initialize the wolf pack with purely random solutions\n",
        "            wolf_pack = []\n",
        "            for i in range(num_wolves):\n",
        "                solution = create_initial_solution()\n",
        "                # Each wolf gets a completely random solution\n",
        "                # No special treatment for any wolf\n",
        "\n",
        "                # Evaluate the solution using makespan\n",
        "                fitness = calculate_makespan(solution)\n",
        "                wolf_pack.append({\"solution\": solution, \"fitness\": fitness})\n",
        "\n",
        "            # Sort wolves by fitness (ascending - lower is better)\n",
        "            wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "            # Initialize alpha, beta, and delta wolves (best three solutions)\n",
        "            alpha = wolf_pack[0]\n",
        "            beta = wolf_pack[1] if len(wolf_pack) > 1 else alpha\n",
        "            delta = wolf_pack[2] if len(wolf_pack) > 2 else beta\n",
        "\n",
        "            logger.info(f\"Initial alpha wolf fitness (makespan): {alpha['fitness']:.2f}\")\n",
        "\n",
        "            # === STEP 4: Run Grey Wolf Optimizer ===\n",
        "            logger.info(\"Phase 4/6: Running Grey Wolf Optimization...\")\n",
        "\n",
        "            for iteration in range(max_iterations):\n",
        "                # Update parameter a (linearly decreases from 2 to 0)\n",
        "                a = 2 - iteration * (2 / max_iterations)\n",
        "\n",
        "                # Update each wolf's position (solution)\n",
        "                for i in range(num_wolves):\n",
        "                    # Get current wolf\n",
        "                    wolf = wolf_pack[i]\n",
        "                    new_solution = {}\n",
        "\n",
        "                    # For each task, update its assignment based on alpha, beta, delta\n",
        "                    for task in initial_tasks:\n",
        "                        # Calculate the influence of each leader wolf\n",
        "\n",
        "                        # Randomly select some positions from alpha, beta, delta solutions\n",
        "                        # to create diversification and influence the current wolf's solution\n",
        "                        r1, r2, r3 = random.random(), random.random(), random.random()\n",
        "\n",
        "                        # Determine which leader wolves to follow for this task\n",
        "                        if r1 < 0.6:  # 60% chance to follow alpha\n",
        "                            new_solution[task] = alpha[\"solution\"].get(task, random.choice(self.resources))\n",
        "                        elif r2 < 0.5:  # 30% chance to follow beta (0.6 + 0.4*0.5 = 0.8)\n",
        "                            new_solution[task] = beta[\"solution\"].get(task, random.choice(self.resources))\n",
        "                        elif r3 < 0.5:  # 10% chance to follow delta (0.8 + 0.2*0.5 = 0.9)\n",
        "                            new_solution[task] = delta[\"solution\"].get(task, random.choice(self.resources))\n",
        "                        else:  # 10% chance to explore randomly\n",
        "                            new_solution[task] = random.choice(self.resources)\n",
        "\n",
        "                    # Evaluate the new solution using makespan\n",
        "                    new_fitness = calculate_makespan(new_solution)\n",
        "\n",
        "                    # Update the wolf if the new solution is better\n",
        "                    if new_fitness < wolf[\"fitness\"]:\n",
        "                        wolf_pack[i] = {\"solution\": new_solution, \"fitness\": new_fitness}\n",
        "\n",
        "                # Re-sort the wolf pack\n",
        "                wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "                # Update alpha, beta, delta wolves\n",
        "                new_alpha = wolf_pack[0]\n",
        "                new_beta = wolf_pack[1] if len(wolf_pack) > 1 else new_alpha\n",
        "                new_delta = wolf_pack[2] if len(wolf_pack) > 2 else new_beta\n",
        "\n",
        "                # Log progress if alpha improved\n",
        "                if new_alpha[\"fitness\"] < alpha[\"fitness\"]:\n",
        "                    logger.info(f\"Iteration {iteration+1}/{max_iterations}: New alpha fitness (makespan): {new_alpha['fitness']:.2f}\")\n",
        "\n",
        "                alpha, beta, delta = new_alpha, new_beta, new_delta\n",
        "\n",
        "            # Get the best solution from GWO (alpha wolf)\n",
        "            best_gwo_solution = alpha[\"solution\"]\n",
        "            best_gwo_fitness = alpha[\"fitness\"]\n",
        "\n",
        "            logger.info(f\"GWO completed. Best makespan: {best_gwo_fitness:.2f}\")\n",
        "\n",
        "            # === STEP 5: Enhance with Tabu Search ===\n",
        "            logger.info(\"Phase 5/6: Enhancing solution with Tabu Search...\")\n",
        "\n",
        "            # Tabu Search parameters\n",
        "            tabu_tenure = min(50, int(total_tasks * 0.03))\n",
        "            tabu_iterations = min(40, int(total_tasks * 0.05))\n",
        "            tabu_list = collections.deque(maxlen=tabu_tenure)\n",
        "\n",
        "            # Start with the best GWO solution\n",
        "            current_solution = copy.deepcopy(best_gwo_solution)\n",
        "            current_fitness = best_gwo_fitness\n",
        "\n",
        "            # Initialize best Tabu solution\n",
        "            tabu_best_solution = copy.deepcopy(current_solution)\n",
        "            tabu_best_fitness = current_fitness\n",
        "\n",
        "            # Function to identify critical path tasks\n",
        "            def identify_critical_path_tasks(solution, limit=20):\n",
        "                \"\"\"\n",
        "                Identify tasks on the critical path - tasks that directly affect makespan.\n",
        "                These are tasks that finish at or close to the overall makespan.\n",
        "                \"\"\"\n",
        "                # Calculate resource completion times\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "                task_end_times = {}\n",
        "\n",
        "                # Sort tasks by arrival time\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                # Process each task and track end times\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate execution time\n",
        "                    exec_time = calculate_execution_time(task, resource.id)\n",
        "\n",
        "                    # Calculate start and end times\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    end_time = start_time + exec_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = end_time\n",
        "\n",
        "                    # Store task end time\n",
        "                    task_end_times[task] = end_time\n",
        "\n",
        "                # Calculate overall makespan\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else 0\n",
        "\n",
        "                # Find tasks close to makespan\n",
        "                threshold = makespan * 0.9  # Consider tasks within 10% of makespan\n",
        "                critical_tasks = [task for task, end_time in task_end_times.items()\n",
        "                                if end_time >= threshold]\n",
        "\n",
        "                # If we have too few tasks, just return the tasks with highest end times\n",
        "                if len(critical_tasks) < limit:\n",
        "                    critical_tasks = sorted(task_end_times.keys(),\n",
        "                                        key=lambda task: task_end_times[task],\n",
        "                                        reverse=True)[:limit]\n",
        "\n",
        "                return critical_tasks[:limit]  # Return at most 'limit' tasks\n",
        "\n",
        "            # Run Tabu Search\n",
        "            stagnation_counter = 0\n",
        "            max_stagnation = 10\n",
        "\n",
        "            for iteration in range(tabu_iterations):\n",
        "                # Focus on critical path tasks\n",
        "                task_limit = max(20, int(len(current_solution) * 0.15))\n",
        "                critical_tasks = identify_critical_path_tasks(current_solution, limit=task_limit)\n",
        "\n",
        "                # Generate and evaluate moves\n",
        "                best_move = None\n",
        "                best_move_fitness = float('inf')\n",
        "\n",
        "                # For each critical task, try moving to different resources\n",
        "                for task in critical_tasks:\n",
        "                    current_resource = current_solution[task]\n",
        "\n",
        "                    # Try moving to each resource\n",
        "                    for resource in self.resources:\n",
        "                        # Skip current assignment and tabu moves\n",
        "                        if resource == current_resource or (task.id, resource.id) in tabu_list:\n",
        "                            continue\n",
        "\n",
        "                        # Make the move\n",
        "                        current_solution[task] = resource\n",
        "\n",
        "                        # Evaluate the move using makespan\n",
        "                        move_fitness = calculate_makespan(current_solution)\n",
        "\n",
        "                        # Update best move if better\n",
        "                        if move_fitness < best_move_fitness:\n",
        "                            best_move = (task, resource)\n",
        "                            best_move_fitness = move_fitness\n",
        "\n",
        "                        # Restore the solution\n",
        "                        current_solution[task] = current_resource\n",
        "\n",
        "                # Apply best move if found\n",
        "                if best_move:\n",
        "                    task, resource = best_move\n",
        "                    current_solution[task] = resource\n",
        "                    current_fitness = best_move_fitness\n",
        "\n",
        "                    # Add move to tabu list\n",
        "                    tabu_list.append((task.id, resource.id))\n",
        "\n",
        "                    # Update best tabu solution if improved\n",
        "                    if current_fitness < tabu_best_fitness:\n",
        "                        tabu_best_solution = copy.deepcopy(current_solution)\n",
        "                        tabu_best_fitness = current_fitness\n",
        "                        stagnation_counter = 0\n",
        "                        logger.info(f\"Tabu iteration {iteration+1}/{tabu_iterations}: New best makespan: {tabu_best_fitness:.2f}\")\n",
        "                    else:\n",
        "                        stagnation_counter += 1\n",
        "                else:\n",
        "                    stagnation_counter += 1\n",
        "\n",
        "                # Early stopping\n",
        "                if stagnation_counter >= max_stagnation:\n",
        "                    logger.info(f\"Early stopping Tabu Search at iteration {iteration+1} (no improvement for {max_stagnation} iterations)\")\n",
        "                    break\n",
        "\n",
        "            # === STEP 6: Finalize solution and apply to resources ===\n",
        "            logger.info(\"Phase 6/6: Finalizing and distributing tasks...\")\n",
        "\n",
        "            # Compare GWO and Tabu solutions\n",
        "            logger.info(f\"GWO solution makespan: {best_gwo_fitness:.2f}\")\n",
        "            logger.info(f\"Tabu enhanced solution makespan: {tabu_best_fitness:.2f}\")\n",
        "\n",
        "            # Select the better solution\n",
        "            final_solution = tabu_best_solution if tabu_best_fitness < best_gwo_fitness else best_gwo_solution\n",
        "            final_fitness = min(tabu_best_fitness, best_gwo_fitness)\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            logger.info(f\"Total algorithm execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "            # Reset resources\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = []\n",
        "                resource.failed_tasks = []\n",
        "\n",
        "            # Apply the final solution\n",
        "            distributed_tasks = []\n",
        "            assignment_data = []\n",
        "            resource_assignment_counts = {r.type: 0 for r in self.resources}\n",
        "            temp_resource_task_queues = {resource.id: [] for resource in self.resources}\n",
        "\n",
        "            # Process all assignments\n",
        "            for task, resource in final_solution.items():\n",
        "                if task is None or resource is None:\n",
        "                    continue\n",
        "\n",
        "                # Set task status and maintain reference\n",
        "                task.status = 'CREATED'\n",
        "                task.assigned_resource = resource\n",
        "                temp_resource_task_queues[resource.id].append(task)\n",
        "                resource_assignment_counts[resource.type] += 1\n",
        "\n",
        "                # Record the assignment\n",
        "                arrival_time = datetime.fromtimestamp(task.arrival_time)\n",
        "                record = {\n",
        "                    'Task ID': task.id,\n",
        "                    'Type': task.type,\n",
        "                    'Input Size (GB)': task.input_size,\n",
        "                    'Output Size (GB)': task.output_size,\n",
        "                    'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'Status': task.status,\n",
        "                    'Assigned Node': resource.type,\n",
        "                    'Estimated Time': calculate_execution_time(task, resource.id)\n",
        "                }\n",
        "                assignment_data.append(record)\n",
        "                distributed_tasks.append(task)\n",
        "\n",
        "            # Update resource queues\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = temp_resource_task_queues[resource.id]\n",
        "\n",
        "            # Save assignments to CSV\n",
        "            csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "            os.makedirs(csv_folder, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_filepath = os.path.join(csv_folder, f'hybrid_gwo_tabu_makespan_{timestamp}.csv')\n",
        "\n",
        "            with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "                fieldnames = [\n",
        "                    'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                    'Time of Arrival', 'Status', 'Assigned Node', 'Estimated Time'\n",
        "                ]\n",
        "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(assignment_data)\n",
        "\n",
        "            # Log distribution summary\n",
        "            total_tasks_assigned = sum(resource_assignment_counts.values())\n",
        "            logger.info(\"\\n=== Task Distribution Summary ===\")\n",
        "            logger.info(f\"Total tasks assigned: {total_tasks_assigned}/{total_tasks}\")\n",
        "\n",
        "            for resource_type, count in sorted(resource_assignment_counts.items()):\n",
        "                percent = (count / total_tasks_assigned) * 100 if total_tasks_assigned > 0 else 0\n",
        "                logger.info(f\"  {resource_type}: {count} tasks ({percent:.1f}%)\")\n",
        "\n",
        "            logger.info(f\"Final solution makespan: {final_fitness:.2f}\")\n",
        "            logger.info(f\"Results saved to: {csv_filepath}\")\n",
        "\n",
        "            return distributed_tasks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Hybrid GWO-Tabu Search algorithm: {e}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "            raise\n",
        "\n",
        "\n",
        "    def _tabu_standard_gwo_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Tabu-GWO algorithm with standard GWO equations for task scheduling in edge-cloud environments.\n",
        "\n",
        "        This implementation combines standard GWO position update equations for global exploration\n",
        "        with Tabu Search for local exploitation to improve load balancing.\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Total number of tasks to distribute\n",
        "\n",
        "        Returns:\n",
        "            List[Task]: Distributed tasks with optimized resource assignments\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"\\n⏳ Starting Tabu-Standard-GWO Algorithm for {total_tasks} tasks...\")\n",
        "\n",
        "        try:\n",
        "            # === STEP 1: Initialize parameters and variables ===\n",
        "            logger.info(\"Phase 1/5: Initialization of parameters and variables...\")\n",
        "\n",
        "            # Generate tasks with random arrival times\n",
        "            tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "            # Define task types with requirements\n",
        "            TASK_TYPES = {\n",
        "                \"RT1\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "                \"RT2\": {\"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "                \"RT3\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "                \"RT4\": {\"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "                \"WT1\": {\"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "                \"WT2\": {\"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "                \"WT3\": {\"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "                \"WT4\": {\"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "            }\n",
        "\n",
        "            # Create tasks with random task types\n",
        "            initial_tasks = []\n",
        "            batch_size = 500\n",
        "\n",
        "            for i in range(0, total_tasks, batch_size):\n",
        "                batch_end = min(i + batch_size, total_tasks)\n",
        "                batch_tasks = []\n",
        "\n",
        "                for j in range(i, batch_end):\n",
        "                    task_record = tasks[j]\n",
        "                    # Always use random selection for task type\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    task = Task(\n",
        "                        task_id=task_record.id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "                    task.arrival_time = task_record.arrival_time\n",
        "                    batch_tasks.append(task)\n",
        "\n",
        "                initial_tasks.extend(batch_tasks)\n",
        "                logger.info(f\"Created {min(batch_end, total_tasks)}/{total_tasks} tasks ({(min(batch_end, total_tasks)/total_tasks)*100:.1f}%)\")\n",
        "\n",
        "            # Pre-compute resource capabilities for faster lookup\n",
        "            RESOURCE_CAPABILITIES = {\n",
        "                resource.id: {\n",
        "                    'type': resource.type,\n",
        "                    'cpu_rating': resource.total_cpu_rating,\n",
        "                    'bandwidth': resource.total_bandwidth,\n",
        "                    'is_cloud': resource.type.startswith(\"Cloud_\"),\n",
        "                    'memory': resource.total_memory,\n",
        "                } for resource in self.resources\n",
        "            }\n",
        "\n",
        "            # === STEP 2: Define utility functions ===\n",
        "            logger.info(\"Phase 2/5: Defining utility functions...\")\n",
        "\n",
        "            def calculate_execution_time(task, resource_id):\n",
        "                \"\"\"Calculate task execution time on a given resource\"\"\"\n",
        "                resource = RESOURCE_CAPABILITIES[resource_id]\n",
        "\n",
        "                # Calculate transfer time (input + output)\n",
        "                transfer_time = ((task.input_size + task.output_size) * 1024) / resource['bandwidth']\n",
        "\n",
        "                # Calculate processing time\n",
        "                process_time = task.total_cpu_required / resource['cpu_rating']\n",
        "\n",
        "\n",
        "                # Total execution time\n",
        "                return transfer_time + process_time\n",
        "\n",
        "            def calculate_fitness(solution):\n",
        "                \"\"\"\n",
        "                Calculate fitness based solely on makespan minimization\n",
        "                f(Z) = min MK(Z)\n",
        "                \"\"\"\n",
        "                # Track completion time for each resource\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                # Sort tasks by arrival time\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                # Process each task in order\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:\n",
        "                        return float('inf')\n",
        "\n",
        "                    # Calculate task execution time\n",
        "                    exec_time = calculate_execution_time(task, resource.id)\n",
        "\n",
        "                    # Task starts at either its arrival time or when resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + exec_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                # Makespan is the maximum completion time across all resources\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else float('inf')\n",
        "\n",
        "                return makespan\n",
        "\n",
        "            # === STEP 3: Initialize GWO parameters ===\n",
        "            logger.info(\"Phase 3/5: Initializing Grey Wolf Optimizer with standard equations...\")\n",
        "\n",
        "            # Map resources to indices for vector operations\n",
        "            resource_idx_map = {resource: idx for idx, resource in enumerate(self.resources)}\n",
        "            idx_resource_map = {idx: resource for idx, resource in enumerate(self.resources)}\n",
        "            num_resources = len(self.resources)\n",
        "\n",
        "            # GWO parameters\n",
        "            num_wolves = min(30, int(total_tasks * 0.05))  # Number of search agents\n",
        "            max_iterations = min(20, int(total_tasks * 0.02))  # Maximum iterations\n",
        "\n",
        "            # Initialize wolf population with random solutions\n",
        "            wolf_pack = []\n",
        "\n",
        "            for _ in range(num_wolves):\n",
        "                # Create a completely random solution (position)\n",
        "                solution = {}\n",
        "                for task in initial_tasks:\n",
        "                    solution[task] = random.choice(self.resources)\n",
        "\n",
        "                # Calculate fitness\n",
        "                fitness = calculate_fitness(solution)\n",
        "\n",
        "                wolf_pack.append({\"position\": solution, \"fitness\": fitness})\n",
        "\n",
        "            # Sort wolves by fitness (ascending - lower is better)\n",
        "            wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "            # Initialize alpha, beta, delta wolves (best three positions)\n",
        "            alpha = wolf_pack[0]\n",
        "            beta = wolf_pack[1] if len(wolf_pack) > 1 else alpha\n",
        "            delta = wolf_pack[2] if len(wolf_pack) > 2 else beta\n",
        "\n",
        "            logger.info(f\"Initial alpha wolf fitness: {alpha['fitness']:.2f}\")\n",
        "\n",
        "            # === STEP 4: Run GWO with standard position update equations ===\n",
        "            logger.info(\"Phase 4/5: Running Grey Wolf Optimization with standard equations...\")\n",
        "\n",
        "            for iteration in range(max_iterations):\n",
        "                # Update parameter a (linearly decreases from 2 to 0)\n",
        "                a = 2 - iteration * (2 / max_iterations)\n",
        "\n",
        "                # Update each wolf's position using standard GWO equations\n",
        "                for i in range(num_wolves):\n",
        "                    wolf = wolf_pack[i]\n",
        "                    current_position = wolf[\"position\"]\n",
        "                    new_position = {}\n",
        "\n",
        "                    # For each task, update assignment using standard GWO equations\n",
        "                    for task in initial_tasks:\n",
        "                        # Get current resource assignment\n",
        "                        current_resource = current_position.get(task, random.choice(self.resources))\n",
        "                        current_idx = resource_idx_map[current_resource]\n",
        "\n",
        "                        # Get alpha, beta, and delta positions for this task\n",
        "                        alpha_resource = alpha[\"position\"].get(task, random.choice(self.resources))\n",
        "                        beta_resource = beta[\"position\"].get(task, random.choice(self.resources))\n",
        "                        delta_resource = delta[\"position\"].get(task, random.choice(self.resources))\n",
        "\n",
        "                        alpha_idx = resource_idx_map[alpha_resource]\n",
        "                        beta_idx = resource_idx_map[beta_resource]\n",
        "                        delta_idx = resource_idx_map[delta_resource]\n",
        "\n",
        "                        # Calculate A and C vectors for alpha, beta, delta\n",
        "                        A1, A2, A3 = 2 * a * random.random() - a, 2 * a * random.random() - a, 2 * a * random.random() - a\n",
        "                        C1, C2, C3 = 2 * random.random(), 2 * random.random(), 2 * random.random()\n",
        "\n",
        "                        # Calculate distance vectors (adapted for discrete indices)\n",
        "                        D_alpha = abs(C1 * alpha_idx - current_idx)\n",
        "                        D_beta = abs(C2 * beta_idx - current_idx)\n",
        "                        D_delta = abs(C3 * delta_idx - current_idx)\n",
        "\n",
        "                        # Calculate new position influences\n",
        "                        X1 = alpha_idx - A1 * D_alpha\n",
        "                        X2 = beta_idx - A2 * D_beta\n",
        "                        X3 = delta_idx - A3 * D_delta\n",
        "\n",
        "                        # Average position in continuous space\n",
        "                        avg_position = (X1 + X2 + X3) / 3\n",
        "\n",
        "                        # Convert to discrete resource index (nearest resource)\n",
        "                        # Use modulo to ensure it's within valid range\n",
        "                        nearest_idx = int(round(avg_position)) % num_resources\n",
        "                        new_resource = idx_resource_map[nearest_idx]\n",
        "\n",
        "                        # Apply the new assignment\n",
        "                        new_position[task] = new_resource\n",
        "\n",
        "                    # Evaluate new position\n",
        "                    new_fitness = calculate_fitness(new_position)\n",
        "\n",
        "                    # Update the wolf if the new position is better\n",
        "                    if new_fitness < wolf[\"fitness\"]:\n",
        "                        wolf_pack[i] = {\"position\": new_position, \"fitness\": new_fitness}\n",
        "\n",
        "                # Re-sort the wolf pack\n",
        "                wolf_pack.sort(key=lambda w: w[\"fitness\"])\n",
        "\n",
        "                # Update alpha, beta, delta wolves\n",
        "                new_alpha = wolf_pack[0]\n",
        "                new_beta = wolf_pack[1] if len(wolf_pack) > 1 else new_alpha\n",
        "                new_delta = wolf_pack[2] if len(wolf_pack) > 2 else new_beta\n",
        "\n",
        "                # Log progress if alpha improved\n",
        "                if new_alpha[\"fitness\"] < alpha[\"fitness\"]:\n",
        "                    logger.info(f\"Iteration {iteration+1}/{max_iterations}: New alpha fitness: {new_alpha['fitness']:.2f}\")\n",
        "\n",
        "                alpha, beta, delta = new_alpha, new_beta, new_delta\n",
        "\n",
        "            # === STEP 5: Enhance with Tabu Search ===\n",
        "            logger.info(\"Phase 5/5: Enhancing solution with Tabu Search...\")\n",
        "\n",
        "            # Tabu Search parameters\n",
        "            tabu_tenure = min(30, int(total_tasks * 0.02))  # Maximum size of Tabu List\n",
        "            max_tabu_iterations = min(30, int(total_tasks * 0.03))  # Maximum iterations for Tabu Search\n",
        "            tabu_list = collections.deque(maxlen=tabu_tenure)  # Use collections.deque for efficient Tabu List\n",
        "\n",
        "            # Start with the best solution from GWO (alpha wolf's position)\n",
        "            X_alpha = copy.deepcopy(alpha[\"position\"])\n",
        "            best_solution = X_alpha\n",
        "            best_fitness = alpha[\"fitness\"]\n",
        "\n",
        "            # Global best solution for aspiration\n",
        "            global_best_fitness = best_fitness\n",
        "\n",
        "            # Tabu Search Loop with Simple Aspiration Criterion\n",
        "            for m in range(max_tabu_iterations):\n",
        "                # Create a temporary solution by modifying best solution\n",
        "                X_temp = copy.deepcopy(best_solution)\n",
        "\n",
        "                # Randomly select a resource to modify\n",
        "                resource = random.choice(self.resources)\n",
        "\n",
        "                # Find tasks currently not on this resource\n",
        "                movable_tasks = [task for task, current_resource in X_temp.items() if current_resource != resource]\n",
        "\n",
        "                # If no movable tasks, continue to next iteration\n",
        "                if not movable_tasks:\n",
        "                    continue\n",
        "\n",
        "                # Randomly select tasks to move\n",
        "                num_tasks_to_move = random.randint(1, min(5, len(movable_tasks)))\n",
        "                tasks_to_move = random.sample(movable_tasks, num_tasks_to_move)\n",
        "\n",
        "                # Move selected tasks to the new resource\n",
        "                for task in tasks_to_move:\n",
        "                    X_temp[task] = resource\n",
        "\n",
        "                # Calculate fitness of the new solution\n",
        "                temp_fitness = calculate_fitness(X_temp)\n",
        "\n",
        "                # Simple Aspiration Criterion:\n",
        "                # 1. If the move is not in Tabu List\n",
        "                # 2. OR if the new solution is better than the global best solution\n",
        "                is_tabu_move = any((task.id, resource.id) in tabu_list for task, resource in X_temp.items())\n",
        "\n",
        "                if not is_tabu_move or temp_fitness < global_best_fitness:\n",
        "                    # Update solutions if improved\n",
        "                    if temp_fitness < best_fitness:\n",
        "                        best_solution = X_temp\n",
        "                        best_fitness = temp_fitness\n",
        "\n",
        "                        # Update global best if significantly improved\n",
        "                        if temp_fitness < global_best_fitness:\n",
        "                            global_best_fitness = temp_fitness\n",
        "                            logger.info(f\"New global best found: {global_best_fitness:.2f}\")\n",
        "\n",
        "                    # Add move to Tabu List\n",
        "                    for task in tasks_to_move:\n",
        "                        tabu_list.append((task.id, resource.id))\n",
        "\n",
        "                    logger.info(f\"Tabu iteration {m+1}/{max_tabu_iterations}: Improved solution. Fitness: {best_fitness:.2f}\")\n",
        "\n",
        "            # Final solution is the best found\n",
        "            final_solution = best_solution\n",
        "            final_fitness = best_fitness\n",
        "\n",
        "            ## Stop here for tabu standard implementation ###\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            logger.info(f\"Total algorithm execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "            # Apply the final solution to resources\n",
        "            # Rest of the code remains the same as in the original function\n",
        "            # (resource resetting, task distribution, CSV saving, etc.)\n",
        "\n",
        "            # Reset resources\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = []\n",
        "                resource.failed_tasks = []\n",
        "\n",
        "            # Apply the final solution\n",
        "            distributed_tasks = []\n",
        "            assignment_data = []\n",
        "            resource_assignment_counts = {r.type: 0 for r in self.resources}\n",
        "            temp_resource_task_queues = {resource.id: [] for resource in self.resources}\n",
        "\n",
        "            # Process all assignments\n",
        "            for task, resource in final_solution.items():\n",
        "                if task is None or resource is None:\n",
        "                    continue\n",
        "\n",
        "                # Set task status and maintain reference\n",
        "                task.status = 'CREATED'\n",
        "                task.assigned_resource = resource\n",
        "                temp_resource_task_queues[resource.id].append(task)\n",
        "                resource_assignment_counts[resource.type] += 1\n",
        "\n",
        "                # Record the assignment\n",
        "                arrival_time = datetime.fromtimestamp(task.arrival_time)\n",
        "                record = {\n",
        "                    'Task ID': task.id,\n",
        "                    'Type': task.type,\n",
        "                    'Input Size (GB)': task.input_size,\n",
        "                    'Output Size (GB)': task.output_size,\n",
        "                    'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'Status': task.status,\n",
        "                    'Assigned Node': resource.type,\n",
        "                    'Estimated Time': calculate_execution_time(task, resource.id)\n",
        "                }\n",
        "                assignment_data.append(record)\n",
        "                distributed_tasks.append(task)\n",
        "\n",
        "            # Update resource queues\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = temp_resource_task_queues[resource.id]\n",
        "\n",
        "            # Save assignments to CSV\n",
        "            csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "            os.makedirs(csv_folder, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_filepath = os.path.join(csv_folder, f'tabu_standard_gwo_{timestamp}.csv')\n",
        "\n",
        "            with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "                fieldnames = [\n",
        "                    'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                    'Time of Arrival', 'Status', 'Assigned Node', 'Estimated Time'\n",
        "                ]\n",
        "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(assignment_data)\n",
        "\n",
        "            # Log distribution summary\n",
        "            total_tasks_assigned = sum(resource_assignment_counts.values())\n",
        "            logger.info(\"\\n=== Task Distribution Summary ===\")\n",
        "            logger.info(f\"Total tasks assigned: {total_tasks_assigned}/{total_tasks}\")\n",
        "\n",
        "            for resource_type, count in sorted(resource_assignment_counts.items()):\n",
        "                percent = (count / total_tasks_assigned) * 100 if total_tasks_assigned > 0 else 0\n",
        "                logger.info(f\"  {resource_type}: {count} tasks ({percent:.1f}%)\")\n",
        "\n",
        "            logger.info(f\"Final solution fitness: {final_fitness:.2f}\")\n",
        "            logger.info(f\"Results saved to: {csv_filepath}\")\n",
        "\n",
        "            return distributed_tasks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Tabu-Standard-GWO algorithm: {e}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "            raise\n",
        "    def _optimized_hybrid_algorithm(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Optimized hybrid Tabu-Genetic algorithm for smaller task sets (2000-5000 tasks).\n",
        "        This version is significantly faster while maintaining solution quality.\n",
        "\n",
        "        Key optimizations:\n",
        "        1. Faster initialization with smaller population\n",
        "        2. Reduced generations with early convergence detection\n",
        "        3. Streamlined Tabu Search focused on high-impact tasks\n",
        "        4. Simplified fitness function with multi-objective cost calculation\n",
        "        5. Better progress tracking throughout execution\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Total number of tasks to distribute\n",
        "\n",
        "        Returns:\n",
        "            List[Task]: Distributed tasks with optimized resource assignments\n",
        "        \"\"\"\n",
        "        # Log start time and show progress indicator\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"\\n⏳ Starting Optimized Hybrid Algorithm for {total_tasks} tasks...\")\n",
        "        logger.info(\"Phase 1/5: Task generation and initialization...\")\n",
        "\n",
        "        # Initialize CSV tracking for algorithm progress\n",
        "        csv_folder = \"/content/drive/My Drive/EdgeSimPy/algorithm_tracking\"\n",
        "        os.makedirs(csv_folder, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        ga_csv_path = os.path.join(csv_folder, f'ga_evolution_{timestamp}.csv')\n",
        "        ts_csv_path = os.path.join(csv_folder, f'tabu_search_{timestamp}.csv')\n",
        "        solution_csv_path = os.path.join(csv_folder, f'solution_comparison_{timestamp}.csv')\n",
        "\n",
        "        # Initialize CSV files with headers\n",
        "        with open(ga_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Generation', 'Best_Fitness', 'Average_Fitness', 'Population_Diversity',\n",
        "                            'Mutation_Rate', 'Crossover_Rate', 'Tasks_Per_Resource',\n",
        "                            'Load_Balance_Score', 'Solution_Hash'])\n",
        "\n",
        "        with open(ts_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Iteration', 'Current_Fitness', 'Best_Fitness', 'Move_Type',\n",
        "                            'Task_ID', 'Old_Resource', 'New_Resource', 'Tabu_List_Size',\n",
        "                            'Improvement_Percentage', 'Solution_Hash'])\n",
        "\n",
        "        with open(solution_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Algorithm_Phase', 'Timestamp', 'Solution_Cost', 'Load_Balance_Score',\n",
        "                            'Resource_Distribution', 'Task_Type_Distribution', 'Average_Execution_Time',\n",
        "                            'Worst_Task_Time', 'Best_Task_Time'])\n",
        "\n",
        "        try:\n",
        "            # === STEP 1: Generate tasks with batch processing for efficiency ===\n",
        "            tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "            # Define task types with cached requirements\n",
        "            TASK_TYPES = {\n",
        "                \"RT1\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "                \"RT2\": {\"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "                \"RT3\": {\"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "                \"RT4\": {\"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "                \"WT1\": {\"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "                \"WT2\": {\"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "                \"WT3\": {\"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "                \"WT4\": {\"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "            }\n",
        "\n",
        "            # Pre-compute resource capabilities for faster lookup\n",
        "            RESOURCE_CAPABILITIES = {\n",
        "                resource.id: {\n",
        "                    'type': resource.type,\n",
        "                    'cpu_rating': resource.total_cpu_rating,\n",
        "                    'bandwidth': resource.total_bandwidth,\n",
        "                    'is_cloud': resource.type.startswith(\"Cloud_\"),\n",
        "                    'memory': resource.total_memory,\n",
        "                } for resource in self.resources\n",
        "            }\n",
        "\n",
        "            # Helper methods for tracking metrics\n",
        "            def _calculate_population_diversity(population):\n",
        "                \"\"\"Calculate population diversity as percentage of different assignments\"\"\"\n",
        "                if len(population) <= 1:\n",
        "                    return 0\n",
        "\n",
        "                # Take the first solution as reference\n",
        "                reference = population[0]\n",
        "                total_assignments = len(reference) * (len(population) - 1)\n",
        "                different_assignments = 0\n",
        "\n",
        "                # Compare each solution with the reference\n",
        "                for solution in population[1:]:\n",
        "                    for task in reference:\n",
        "                        if task in solution and reference[task] != solution[task]:\n",
        "                            different_assignments += 1\n",
        "\n",
        "                return (different_assignments / total_assignments) * 100 if total_assignments > 0 else 0\n",
        "\n",
        "            def _get_resource_distribution(solution):\n",
        "                \"\"\"Get distribution of tasks per resource\"\"\"\n",
        "                distribution = {resource.type: 0 for resource in self.resources}\n",
        "\n",
        "                for task, resource in solution.items():\n",
        "                    if resource:\n",
        "                        distribution[resource.type] += 1\n",
        "\n",
        "                return distribution\n",
        "\n",
        "            def _get_task_type_distribution(solution):\n",
        "                \"\"\"Get distribution of task types per resource type\"\"\"\n",
        "                distribution = {}\n",
        "\n",
        "                for task, resource in solution.items():\n",
        "                    if resource:\n",
        "                        resource_type = resource.type.split('_')[0]  # Cloud, Smartphone, or Raspberry\n",
        "                        task_type = task.type\n",
        "\n",
        "                        if resource_type not in distribution:\n",
        "                            distribution[resource_type] = {}\n",
        "\n",
        "                        if task_type not in distribution[resource_type]:\n",
        "                            distribution[resource_type][task_type] = 0\n",
        "\n",
        "                        distribution[resource_type][task_type] += 1\n",
        "\n",
        "                return distribution\n",
        "           # === NEW MAKESPAN WEIGHTED SUM FUNCTION ===\n",
        "            def makespan_weighted_sum_fitness(solution, weights=None):\n",
        "                \"\"\"\n",
        "                Multi-objective weighted sum fitness function focusing on:\n",
        "                1. Makespan (total completion time)\n",
        "                2. Average resource utilization\n",
        "\n",
        "                Args:\n",
        "                    solution: Task-to-resource mapping\n",
        "                    weights: Optional weights for different objectives\n",
        "\n",
        "                Returns:\n",
        "                    float: Weighted sum fitness value (lower is better)\n",
        "                \"\"\"\n",
        "                if weights is None:\n",
        "                    # Default weights - fixed for consistent optimization\n",
        "                    weights = {\n",
        "                        'makespan': 0.65,\n",
        "                        'resource_utilization': 0.35\n",
        "                    }\n",
        "\n",
        "                # 1. Makespan calculation\n",
        "                # Track earliest completion time for each resource\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                # Sort tasks by arrival time to respect task arrival order\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate task execution time\n",
        "                    transfer_time = ((task.input_size + task.output_size) * 1024) / resource.total_bandwidth\n",
        "                    process_time = task.total_cpu_required / resource.total_cpu_rating\n",
        "                    total_task_time = transfer_time + process_time\n",
        "\n",
        "                    # Task starts at either its arrival time or when the resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + total_task_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                # Makespan is the maximum completion time across all resources\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else float('inf')\n",
        "\n",
        "                # 2. Average resource utilization calculation\n",
        "                # Track total processing time for each resource\n",
        "                resource_total_processing = {resource.id: 0 for resource in self.resources}\n",
        "\n",
        "                for task, resource in solution.items():\n",
        "                    if not resource:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate processing time for this task\n",
        "                    transfer_time = ((task.input_size + task.output_size) * 1024) / resource.total_bandwidth\n",
        "                    process_time = task.total_cpu_required / resource.total_cpu_rating\n",
        "                    total_task_time = transfer_time + process_time\n",
        "\n",
        "                    # Add to resource's total processing time\n",
        "                    resource_total_processing[resource.id] += total_task_time\n",
        "\n",
        "                # Calculate average utilization across all resources\n",
        "                if makespan > 0:\n",
        "                    resource_utilizations = [processing_time / makespan for processing_time in resource_total_processing.values()]\n",
        "                    avg_resource_utilization = sum(resource_utilizations) / len(self.resources)\n",
        "\n",
        "                    # Convert to cost: higher utilization should have LOWER cost\n",
        "                    utilization_cost = 1.0 / (avg_resource_utilization + 0.001)  # Add small constant to avoid division by zero\n",
        "                else:\n",
        "                    utilization_cost = 100000  # High penalty for zero makespan (shouldn't happen)\n",
        "\n",
        "                # Calculate weighted sum of all objectives\n",
        "                total_cost = (\n",
        "                    weights['makespan'] * makespan +\n",
        "                    weights['resource_utilization'] * utilization_cost\n",
        "                )\n",
        "\n",
        "                return total_cost\n",
        "\n",
        "            # Faster task creation with larger batch size\n",
        "            batch_size = 500\n",
        "            initial_tasks = []\n",
        "            resource_loads = {r.id: 0 for r in self.resources}\n",
        "\n",
        "            # Show progress for task creation\n",
        "            logger.info(f\"Creating {total_tasks} tasks in batches of {batch_size}...\")\n",
        "\n",
        "            for i in range(0, total_tasks, batch_size):\n",
        "                batch_end = min(i + batch_size, total_tasks)\n",
        "                batch_tasks = []\n",
        "\n",
        "                for j in range(i, batch_end):\n",
        "                    task_record = tasks[j]\n",
        "                    # Instead of cycling through task types:\n",
        "                    # task_type = list(TASK_TYPES.keys())[j % len(TASK_TYPES)]\n",
        "\n",
        "                    # Use random selection:\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    task = Task(\n",
        "                        task_id=task_record.id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "                    task.arrival_time = task_record.arrival_time\n",
        "                    batch_tasks.append(task)\n",
        "\n",
        "                initial_tasks.extend(batch_tasks)\n",
        "                logger.info(f\"Created {min(batch_end, total_tasks)}/{total_tasks} tasks ({(min(batch_end, total_tasks)/total_tasks)*100:.1f}%)\")\n",
        "\n",
        "            # Special handling for very small task counts (like 5)\n",
        "            if total_tasks <= 10:\n",
        "                logger.info(f\"Small task count ({total_tasks}) detected - using specialized optimization parameters\")\n",
        "                # Smaller population, fewer generations for tiny problems\n",
        "                population_size = max(5, min(10, total_tasks))\n",
        "                generations = 3\n",
        "                tabu_iterations = max(5, min(10, total_tasks))\n",
        "                tabu_tenure = max(3, min(5, total_tasks))\n",
        "                max_stagnation = 2\n",
        "            else:\n",
        "            # === STEP 2: Adjusted optimization parameters for larger task counts ===\n",
        "                population_size = min(50, int(total_tasks * 0.1))  # Proportional to total tasks\n",
        "                generations = min(20, int(total_tasks * 0.05))     # More generations for larger problems\n",
        "                tabu_iterations = min(100, int(total_tasks * 0.05))  # Increased iterations\n",
        "                tabu_tenure = min(50, int(total_tasks * 0.02))     # Slightly larger tabu tenure\n",
        "                max_stagnation = 10    # Increased early stopping patience\n",
        "\n",
        "            # === Initialize epsilon decay parameters ===\n",
        "                epsilon_start = 1.0  # Start with full exploration\n",
        "                epsilon_end = 0.1    # End with minimal exploration\n",
        "                ga_epsilon_decay_rate = (epsilon_start - epsilon_end) / generations\n",
        "                ts_epsilon_decay_rate = (epsilon_start - epsilon_end) / tabu_iterations\n",
        "                current_ga_epsilon = epsilon_start\n",
        "                current_ts_epsilon = epsilon_start\n",
        "\n",
        "            # With these adaptive mutation parameters:\n",
        "            base_mutation_rate = 0.15\n",
        "            min_mutation_rate = 0.1\n",
        "            max_mutation_rate = 0.4\n",
        "            current_mutation_rate = base_mutation_rate\n",
        "            diversity_threshold_low = 15  # Percentage below which to increase mutation\n",
        "            diversity_threshold_high = 40  # Percentage above which to decrease mutation\n",
        "            crossover_rate = 0.8\n",
        "            elite_count = 2\n",
        "\n",
        "            tabu_list = collections.deque(maxlen=tabu_tenure)\n",
        "\n",
        "            # === STEP 3: Define optimized utility functions ===\n",
        "            def fast_exec_time_estimate(task, resource_id):\n",
        "                \"\"\"Simplified execution time estimation for speed\"\"\"\n",
        "                resource = RESOURCE_CAPABILITIES[resource_id]\n",
        "\n",
        "                # Basic estimate combining transfer and processing time\n",
        "                transfer_time = ((task.input_size + task.output_size) * 1024) / resource['bandwidth']\n",
        "                process_time = task.total_cpu_required / resource['cpu_rating']\n",
        "\n",
        "\n",
        "                # Simplified total time calculation\n",
        "                return transfer_time + process_time\n",
        "\n",
        "            def simplified_fitness(solution):\n",
        "                \"\"\"\n",
        "                Multi-objective fitness function with core components only:\n",
        "                1. Makespan (minimize)\n",
        "                2. Throughput (maximize)\n",
        "                3. Utilization (maximize)\n",
        "\n",
        "                Fitness Formula:\n",
        "                Fitness = (w_makespan * Makespan)\n",
        "                        - (w_throughput * Throughput)\n",
        "                        - (w_utilization * Utilization)\n",
        "\n",
        "                Weight Control Guidelines:\n",
        "                - w_makespan: Controls importance of completion time\n",
        "                - Higher values (1.5-3.0): Prioritize minimizing overall completion time\n",
        "                - Lower values (0.1-0.5): Reduce emphasis on completion time\n",
        "                - For time-critical applications, use higher values\n",
        "\n",
        "                - w_throughput: Controls importance of tasks processed per time unit\n",
        "                - Higher values (1.5-3.0): Prioritize processing more tasks quickly\n",
        "                - Lower values (0.1-0.5): Focus less on throughput, more on other objectives\n",
        "                - For batch processing systems, use higher values\n",
        "\n",
        "                - w_utilization: Controls importance of balanced resource usage\n",
        "                - Higher values (1.5-3.0): Prioritize keeping resources busy\n",
        "                - Lower values (0.1-0.5): Allow some resources to remain idle if it helps other objectives\n",
        "                - For resource-constrained environments, use higher values\n",
        "\n",
        "                To Control Edge-Cloud Distribution Without Explicit Penalties:\n",
        "                - For more edge processing: Increase w_throughput slightly (favors lower transfer times)\n",
        "                - For more cloud processing: Increase w_makespan slightly (favors faster processing)\n",
        "                - A balanced approach with equal weights typically produces a natural distribution\n",
        "                \"\"\"\n",
        "                # Weights - These can be adjusted to change optimization priorities\n",
        "                w_makespan = 0.7    # Range: 0.1-3.0, Higher = prioritize minimizing completion time\n",
        "                w_throughput = 1.8  # Range: 0.1-3.0, Higher = prioritize maximizing tasks per time unit\n",
        "                w_utilization = 1.5 # Range: 0.1-3.0, Higher = prioritize keeping resources busy\n",
        "\n",
        "                # Return high penalty for empty solutions\n",
        "                if not solution:\n",
        "                    return float('inf')\n",
        "\n",
        "                # Calculate makespan\n",
        "                resource_completion_times = {resource.id: 0 for resource in self.resources}\n",
        "                sorted_tasks = sorted(solution.keys(), key=lambda task: task.arrival_time)\n",
        "\n",
        "                for task in sorted_tasks:\n",
        "                    resource = solution[task]\n",
        "                    if not resource:  # Skip unassigned tasks\n",
        "                        continue\n",
        "\n",
        "                    # Calculate task execution time\n",
        "                    transfer_time = ((task.input_size + task.output_size) * 1024) / resource.total_bandwidth\n",
        "                    process_time = task.total_cpu_required / resource.total_cpu_rating\n",
        "                    total_task_time = transfer_time + process_time\n",
        "\n",
        "                    # Task starts at either its arrival time or when resource becomes available\n",
        "                    start_time = max(task.arrival_time, resource_completion_times[resource.id])\n",
        "                    completion_time = start_time + total_task_time\n",
        "\n",
        "                    # Update resource completion time\n",
        "                    resource_completion_times[resource.id] = completion_time\n",
        "\n",
        "                # Makespan is the maximum completion time across all resources\n",
        "                makespan = max(resource_completion_times.values()) if resource_completion_times else float('inf')\n",
        "\n",
        "                # Calculate throughput (tasks/time)\n",
        "                # Higher throughput is better, so we use negative coefficient\n",
        "                throughput = len(solution) / makespan if makespan > 0 else 0\n",
        "\n",
        "                # Calculate average utilization across resources WITHOUT penalties\n",
        "                resource_utilizations = []\n",
        "                if makespan > 0:\n",
        "                    for resource in self.resources:\n",
        "                        busy_time = resource_completion_times.get(resource.id, 0)\n",
        "                        utilization = busy_time / makespan\n",
        "                        resource_utilizations.append(utilization)\n",
        "\n",
        "                    avg_utilization = sum(resource_utilizations) / len(resource_utilizations) if resource_utilizations else 0\n",
        "                else:\n",
        "                    avg_utilization = 0\n",
        "\n",
        "                # Final fitness value (lower is better)\n",
        "                # Negative coefficients for throughput and utilization since we want to maximize them\n",
        "                fitness = (w_makespan * makespan) - (w_throughput * throughput) - (w_utilization * avg_utilization)\n",
        "\n",
        "                return fitness\n",
        "            # === STEP 4: Create Fully Random initial solution ===\n",
        "            logger.info(\"Phase 2/5: Creating initial population...\")\n",
        "\n",
        "            # Create initial population\n",
        "            # Create fully random initial population\n",
        "            population = []\n",
        "            for i in range(population_size):\n",
        "                # Create completely random solution\n",
        "                solution = {}\n",
        "                for task in initial_tasks:\n",
        "                    solution[task] = random.choice(self.resources)\n",
        "\n",
        "                population.append(solution)\n",
        "\n",
        "            # === STEP 5: Run Optimized Genetic Algorithm ===\n",
        "            logger.info(\"Phase 3/5: Running genetic algorithm optimization...\")\n",
        "\n",
        "            # Evaluate initial population\n",
        "            fitnesses = [simplified_fitness(sol) for sol in population]\n",
        "\n",
        "            # Track best solution\n",
        "            best_idx = fitnesses.index(min(fitnesses))\n",
        "            best_solution = copy.deepcopy(population[best_idx])\n",
        "            best_fitness = fitnesses[best_idx]\n",
        "            best_cost = simplified_fitness(best_solution)\n",
        "\n",
        "            # Record initial state for the solution comparison CSV\n",
        "            initial_resource_distribution = json.dumps(_get_resource_distribution(best_solution))\n",
        "            initial_task_type_distribution = json.dumps(_get_task_type_distribution(best_solution))\n",
        "            initial_task_times = [fast_exec_time_estimate(task, resource.id) for task, resource in best_solution.items()]\n",
        "            avg_time = sum(initial_task_times) / len(initial_task_times) if initial_task_times else 0\n",
        "            worst_time = max(initial_task_times) if initial_task_times else 0\n",
        "            best_time = min(initial_task_times) if initial_task_times else 0\n",
        "\n",
        "            # Calculate initial load balance\n",
        "            load_balance = np.std([len([t for t, r in best_solution.items() if r.id == res.id]) for res in self.resources])\n",
        "\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Initial_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    best_fitness,  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    initial_resource_distribution,  # Resource distribution\n",
        "                    initial_task_type_distribution,  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "\n",
        "            # Run genetic algorithm with progress updates\n",
        "            stagnation_counter = 0\n",
        "\n",
        "            for gen in range(generations):\n",
        "                # Progress indicator\n",
        "                current_cost = simplified_fitness(best_solution)\n",
        "                logger.info(f\"Generation {gen+1}/{generations} - Current best cost: {current_cost}\")\n",
        "\n",
        "                # Update epsilon for this generation\n",
        "                current_ga_epsilon = max(epsilon_end, epsilon_start - gen * ga_epsilon_decay_rate)\n",
        "                logger.info(f\"Current GA epsilon: {current_ga_epsilon:.3f}\")\n",
        "\n",
        "                # Sort by fitness and retain elites\n",
        "                sorted_indices = sorted(range(len(fitnesses)), key=lambda i: fitnesses[i])\n",
        "                new_population = [copy.deepcopy(population[i]) for i in sorted_indices[:elite_count]]\n",
        "\n",
        "                # Fill remaining population with crossover and mutation\n",
        "                while len(new_population) < population_size:\n",
        "\n",
        "                    # Use epsilon-greedy selection for parent selection\n",
        "                    if random.random() < current_ga_epsilon:\n",
        "                        # Exploration: Random selection\n",
        "                        parent1_idx = random.randrange(len(population))\n",
        "                        parent2_idx = random.randrange(len(population))\n",
        "                    else:\n",
        "                        # Exploitation: Tournament selection\n",
        "                        tournament_size = min(3, len(population))\n",
        "                        parent1_idx = min(random.sample(range(len(population)), tournament_size),\n",
        "                                        key=lambda i: fitnesses[i])\n",
        "                        parent2_idx = min(random.sample(range(len(population)), tournament_size),\n",
        "                                        key=lambda i: fitnesses[i])\n",
        "\n",
        "                    parent1 = population[parent1_idx]\n",
        "                    parent2 = population[parent2_idx]\n",
        "                    # Adaptive crossover rate based on epsilon\n",
        "                    effective_crossover_rate = crossover_rate * (1 - current_ga_epsilon * 0.3)\n",
        "\n",
        "                    # Simplified crossover\n",
        "                    if random.random() < effective_crossover_rate:\n",
        "                        child = {}\n",
        "\n",
        "                        # Create a random crossover point\n",
        "                        tasks_ordered = sorted(parent1.keys(), key=lambda t: t.arrival_time)\n",
        "                        crossover_point = random.randint(1, len(tasks_ordered) - 1)\n",
        "\n",
        "                        # Take first part from parent1, second from parent2\n",
        "                        for i, task in enumerate(tasks_ordered):\n",
        "                            if i < crossover_point:\n",
        "                                child[task] = parent1[task]\n",
        "                            else:\n",
        "                                # Make sure task exists in parent2\n",
        "                                if task in parent2:\n",
        "                                    child[task] = parent2[task]\n",
        "                                else:\n",
        "                                    child[task] = parent1[task]\n",
        "                    else:\n",
        "                        # No crossover, just clone parent1\n",
        "                        child = copy.deepcopy(parent1)\n",
        "\n",
        "                    # Epsilon-influenced mutation\n",
        "                    # Higher mutation when epsilon is high (exploration), lower when epsilon is low (exploitation)\n",
        "                    epsilon_adjusted_mutation_rate = current_mutation_rate * current_ga_epsilon + min_mutation_rate\n",
        "                    # Simple mutation\n",
        "                    if random.random() < epsilon_adjusted_mutation_rate:\n",
        "                        # Adaptive mutation count based on epsilon\n",
        "                        # More aggressive mutation when epsilon is high\n",
        "                        mutation_ratio = 0.05 + current_ga_epsilon * 0.15  # 5-20% mutation\n",
        "                        mutation_count = max(1, int(len(child) * mutation_ratio))\n",
        "                        tasks_to_mutate = random.sample(list(child.keys()), k=mutation_count)\n",
        "\n",
        "                        for task in tasks_to_mutate:\n",
        "                            child[task] = random.choice(self.resources)\n",
        "\n",
        "                    new_population.append(child)\n",
        "\n",
        "                # Update population\n",
        "                population = new_population\n",
        "\n",
        "                # Calculate fitness - evaluate each solution\n",
        "                fitnesses = [simplified_fitness(sol) for sol in population]\n",
        "\n",
        "                # Check for improvement\n",
        "                current_best_idx = fitnesses.index(min(fitnesses))\n",
        "                current_best_fitness = fitnesses[current_best_idx]\n",
        "\n",
        "                if current_best_fitness < best_fitness:\n",
        "                    best_solution = copy.deepcopy(population[current_best_idx])\n",
        "                    best_fitness = current_best_fitness\n",
        "                    best_cost = simplified_fitness(best_solution)\n",
        "                    logger.info(f\"✓ New best solution found! Fitness: {best_fitness}, Cost: {best_cost}\")\n",
        "                    stagnation_counter = 0\n",
        "                else:\n",
        "                    stagnation_counter += 1\n",
        "\n",
        "                # Calculate diversity (percentage of different assignments between solutions)\n",
        "                diversity = _calculate_population_diversity(population)\n",
        "\n",
        "                if diversity < diversity_threshold_low:\n",
        "                    # Low diversity - increase mutation rate to encourage exploration\n",
        "                    current_mutation_rate = min(max_mutation_rate, current_mutation_rate * 1.5)\n",
        "                    logger.info(f\"Low diversity ({diversity:.1f}%) - increasing mutation rate to {current_mutation_rate:.3f}\")\n",
        "                elif diversity > diversity_threshold_high:\n",
        "                    # High diversity - decrease mutation rate to encourage exploitation\n",
        "                    current_mutation_rate = max(min_mutation_rate, current_mutation_rate * 0.8)\n",
        "                    logger.info(f\"High diversity ({diversity:.1f}%) - decreasing mutation rate to {current_mutation_rate:.3f}\")\n",
        "\n",
        "                # Calculate resource distribution\n",
        "                resource_distribution = _get_resource_distribution(best_solution)\n",
        "\n",
        "                # Calculate task type distribution\n",
        "                task_type_distribution = _get_task_type_distribution(best_solution)\n",
        "\n",
        "                # Calculate task execution times\n",
        "                task_times = [fast_exec_time_estimate(task, resource.id)\n",
        "                            for task, resource in best_solution.items()]\n",
        "                avg_time = sum(task_times) / len(task_times) if task_times else 0\n",
        "                worst_time = max(task_times) if task_times else 0\n",
        "                best_time = min(task_times) if task_times else 0\n",
        "\n",
        "                # Create a hash to track solution differences\n",
        "                solution_hash = hash(frozenset((task.id, resource.id)\n",
        "                                            for task, resource in best_solution.items()))\n",
        "\n",
        "                # Calculate load balance score\n",
        "                load_balance = np.std([len([t for t, r in best_solution.items() if r.id == res.id])\n",
        "                                    for res in self.resources])\n",
        "\n",
        "                # Write GA progress to CSV\n",
        "                with open(ga_csv_path, 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\n",
        "                        gen + 1,  # Generation number\n",
        "                        best_fitness,  # Best fitness\n",
        "                        sum(fitnesses) / len(fitnesses),  # Average fitness\n",
        "                        diversity,  # Population diversity\n",
        "                        current_mutation_rate,  # Mutation rate\n",
        "                        crossover_rate,  # Crossover rate\n",
        "                        json.dumps(resource_distribution),  # Tasks per resource\n",
        "                        load_balance,  # Load balance score\n",
        "                        solution_hash  # Solution hash\n",
        "                    ])\n",
        "\n",
        "                # Record overall solution at end of each generation\n",
        "                if gen % 5 == 0 or gen == generations - 1:  # Every 5 generations and the last one\n",
        "                    with open(solution_csv_path, 'a', newline='') as f:\n",
        "                        writer = csv.writer(f)\n",
        "                        writer.writerow([\n",
        "                            f\"GA_Gen_{gen+1}\",  # Algorithm phase\n",
        "                            datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                            best_fitness,  # Solution cost\n",
        "                            load_balance,  # Load balance score\n",
        "                            json.dumps(resource_distribution),  # Resource distribution\n",
        "                            json.dumps(task_type_distribution),  # Task type distribution\n",
        "                            avg_time,  # Average execution time\n",
        "                            worst_time,  # Worst task time\n",
        "                            best_time  # Best task time\n",
        "                        ])\n",
        "\n",
        "                # Early stopping\n",
        "                if stagnation_counter >= max_stagnation:\n",
        "                    logger.info(f\"Early stopping at generation {gen+1} (no improvement for {max_stagnation} generations)\")\n",
        "                    break\n",
        "\n",
        "            # Record final GA solution\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Final_GA_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    best_fitness,  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    json.dumps(_get_resource_distribution(best_solution)),  # Resource distribution\n",
        "                    json.dumps(_get_task_type_distribution(best_solution)),  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "\n",
        "            # === STEP 6: Apply focused Tabu Search to best solution ===\n",
        "            logger.info(\"Phase 4/5: Applying focused Tabu Search with Multiple Neighborhood Structures...\")\n",
        "\n",
        "            # Define neighborhood types\n",
        "            neighborhood_types = ['high_impact', 'swap_pairs', 'cluster_reassign']\n",
        "\n",
        "            # Create a copy of the best solution\n",
        "            current_solution = copy.deepcopy(best_solution)\n",
        "            current_fitness = simplified_fitness(current_solution)\n",
        "            tabu_best_solution = copy.deepcopy(current_solution)\n",
        "            tabu_best_fitness = current_fitness\n",
        "\n",
        "            # Function to identify high-impact tasks (focuses on tasks with largest execution times)\n",
        "            def identify_high_impact_tasks(solution, limit=20):\n",
        "                # Calculate execution times for each task\n",
        "                task_times = []\n",
        "                for task, resource in solution.items():\n",
        "                    if resource:\n",
        "                        exec_time = fast_exec_time_estimate(task, resource.id)\n",
        "                        task_times.append((task, exec_time))\n",
        "\n",
        "                # Sort by execution time and return top tasks\n",
        "                return [task for task, _ in sorted(task_times, key=lambda x: x[1], reverse=True)[:limit]]\n",
        "\n",
        "            # Run efficient Tabu Search\n",
        "            stagnation_counter = 0\n",
        "\n",
        "            for iteration in range(tabu_iterations):\n",
        "                # Update epsilon for this iteration\n",
        "                current_ts_epsilon = max(epsilon_end, epsilon_start - iteration * ts_epsilon_decay_rate)\n",
        "                # Progress indicator every few iterations\n",
        "                if iteration % 5 == 0 or iteration == tabu_iterations-1:\n",
        "                    current_cost = simplified_fitness(tabu_best_solution)\n",
        "                    logger.info(f\"Tabu iteration {iteration+1}/{tabu_iterations} - Current best cost: {current_cost}, Epsilon: {current_ts_epsilon:.3f}\")\n",
        "\n",
        "                # Select neighborhood type based on iteration\n",
        "                # Use epsilon to determine neighborhood size and move selection strategy\n",
        "                # When epsilon is high: broader neighborhood exploration\n",
        "                # When epsilon is low: more focused exploitation\n",
        "\n",
        "                # Select neighborhood type based on epsilon and iteration\n",
        "                if random.random() < current_ts_epsilon:\n",
        "                    # More exploration when epsilon is high - randomize neighborhood\n",
        "                    current_neighborhood = random.randint(0, len(neighborhood_types)-1)\n",
        "                else:\n",
        "                    # More exploitation when epsilon is low - use deterministic pattern\n",
        "                    current_neighborhood = iteration % len(neighborhood_types)\n",
        "                neighborhood_type = neighborhood_types[current_neighborhood]\n",
        "                logger.info(f\"Using neighborhood type: {neighborhood_type}\")\n",
        "\n",
        "                # Generate and evaluate a limited set of moves\n",
        "                best_move = None\n",
        "                best_move_fitness = float('inf')\n",
        "                best_move_details = {}\n",
        "\n",
        "                if neighborhood_type == 'high_impact':\n",
        "                    # Adjust task limit based on epsilon - higher epsilon means more tasks to consider\n",
        "                    task_limit_base = max(20, int(len(current_solution) * 0.2))\n",
        "                    task_limit = int(task_limit_base * (1 + current_ts_epsilon))\n",
        "                    high_impact_tasks = identify_high_impact_tasks(current_solution, limit=task_limit)\n",
        "\n",
        "                    # For each high-impact task, try a few resources\n",
        "                    for task in high_impact_tasks:\n",
        "                        current_resource = current_solution[task]\n",
        "\n",
        "                        # Epsilon-adjusted resource sampling\n",
        "                        resource_sample_size = min(int(3 + current_ts_epsilon * 7), len(self.resources))\n",
        "                        sampled_resources = random.sample(self.resources, resource_sample_size)\n",
        "\n",
        "                        for resource in sampled_resources:\n",
        "                            # Check if the move is tabu\n",
        "                            is_tabu = (task.id, resource.id) in tabu_list\n",
        "                            # Skip current assignment\n",
        "                            if resource == current_resource:\n",
        "                                continue\n",
        "\n",
        "                            # Evaluate the move\n",
        "                            current_solution[task] = resource\n",
        "                            move_fitness = simplified_fitness(current_solution)\n",
        "\n",
        "                            # *** ASPIRATION CRITERION ***\n",
        "                            # Allow tabu moves if they lead to the best solution found so far\n",
        "                            aspiration_satisfied = move_fitness < tabu_best_fitness\n",
        "                            # Update best move if better and either not tabu OR aspiration criterion is satisfied\n",
        "                            # Update best move if better and either not tabu OR aspiration criterion is satisfied OR epsilon exploration\n",
        "                            if (not is_tabu or aspiration_satisfied or random.random() < current_ts_epsilon * 0.3) and move_fitness < best_move_fitness:\n",
        "                                best_move = (task, resource)\n",
        "                                best_move_fitness = move_fitness\n",
        "                                best_move_details = {\n",
        "                                    'task_id': task.id,\n",
        "                                    'task_type': task.type,\n",
        "                                    'old_resource': current_resource.type,\n",
        "                                    'new_resource': resource.type,\n",
        "                                    'fitness_before': current_fitness,\n",
        "                                    'fitness_after': move_fitness,\n",
        "                                    'move_type': 'high_impact',\n",
        "                                    'aspiration_used': is_tabu and (aspiration_satisfied or random.random() < current_ts_epsilon * 0.3)\n",
        "                                }\n",
        "\n",
        "                                # Log if aspiration criterion was used\n",
        "                                if is_tabu and aspiration_satisfied:\n",
        "                                    logger.info(f\"Aspiration criterion applied: Tabu move allowed for task {task.id} as it improves best solution\")\n",
        "\n",
        "                            # Restore current solution\n",
        "                            current_solution[task] = current_resource\n",
        "\n",
        "                elif neighborhood_type == 'swap_pairs':\n",
        "                    # Swap pairs of tasks between resources\n",
        "                    # Adjust candidate count based on epsilon\n",
        "                    all_tasks = list(current_solution.keys())\n",
        "                    base_candidate_count = min(40, len(all_tasks))\n",
        "                    candidate_count = min(len(all_tasks), int(base_candidate_count * (1 + current_ts_epsilon)))\n",
        "                    candidate_tasks = random.sample(all_tasks, candidate_count)\n",
        "                    # Try swapping pairs\n",
        "                    for i in range(candidate_count // 2):\n",
        "                        if i * 2 + 1 >= candidate_count:\n",
        "                            break\n",
        "\n",
        "                        task1 = candidate_tasks[i * 2]\n",
        "                        task2 = candidate_tasks[i * 2 + 1]\n",
        "\n",
        "                        resource1 = current_solution[task1]\n",
        "                        resource2 = current_solution[task2]\n",
        "\n",
        "                        # Skip if tasks are on the same resource\n",
        "                        if resource1 == resource2:\n",
        "                            continue\n",
        "\n",
        "                        # Check if swap is tabu (both directions)\n",
        "                        swap_tabu = (task1.id, resource2.id) in tabu_list or (task2.id, resource1.id) in tabu_list\n",
        "\n",
        "                        # Try the swap\n",
        "                        current_solution[task1] = resource2\n",
        "                        current_solution[task2] = resource1\n",
        "\n",
        "                        # Evaluate\n",
        "                        swap_fitness = simplified_fitness(current_solution)\n",
        "\n",
        "                        # Aspiration criterion\n",
        "                        aspiration_satisfied = swap_fitness < tabu_best_fitness\n",
        "\n",
        "                        # Update best move if better and either not tabu OR aspiration criterion is satisfied OR epsilon exploration\n",
        "                        if (not swap_tabu or aspiration_satisfied or random.random() < current_ts_epsilon * 0.3) and swap_fitness < best_move_fitness:\n",
        "                            best_move = ('swap', task1, task2, resource1, resource2)\n",
        "                            best_move_fitness = swap_fitness\n",
        "                            best_move_details = {\n",
        "                                'task1_id': task1.id,\n",
        "                                'task2_id': task2.id,\n",
        "                                'resource1': resource1.type,\n",
        "                                'resource2': resource2.type,\n",
        "                                'fitness_before': current_fitness,\n",
        "                                'fitness_after': swap_fitness,\n",
        "                                'move_type': 'swap_pairs',\n",
        "                                'aspiration_used': swap_tabu and (aspiration_satisfied or random.random() < current_ts_epsilon * 0.3)\n",
        "                            }\n",
        "\n",
        "                            # Log if aspiration criterion was used\n",
        "                            if swap_tabu and aspiration_satisfied:\n",
        "                                logger.info(f\"Aspiration criterion applied: Tabu swap allowed as it improves best solution\")\n",
        "\n",
        "                        # Restore current solution\n",
        "                        current_solution[task1] = resource1\n",
        "                        current_solution[task2] = resource2\n",
        "\n",
        "                elif neighborhood_type == 'cluster_reassign':\n",
        "                    # Reassign clusters of related tasks\n",
        "                    # Group tasks by type\n",
        "                    task_clusters = {}\n",
        "                    for task in current_solution:\n",
        "                        if task.type not in task_clusters:\n",
        "                            task_clusters[task.type] = []\n",
        "                        task_clusters[task.type].append(task)\n",
        "\n",
        "                    # For each task type, try reassigning a subset of tasks\n",
        "                    for task_type, tasks in task_clusters.items():\n",
        "                        # Skip very large clusters\n",
        "                        if len(tasks) > 30:\n",
        "                            subset_base_size = min(10, len(tasks))\n",
        "                            subset_size = min(len(tasks), int(subset_base_size * (1 + current_ts_epsilon)))\n",
        "                            tasks_subset = random.sample(tasks, subset_size)\n",
        "                        else:\n",
        "                            tasks_subset = tasks\n",
        "\n",
        "                        # Try each target resource\n",
        "                        for target_resource in self.resources:\n",
        "                            # Store original assignments\n",
        "                            original_assignments = {task: current_solution[task] for task in tasks_subset}\n",
        "                            cluster_tabu = False\n",
        "\n",
        "                            # Check if any reassignment would be tabu\n",
        "                            for task in tasks_subset:\n",
        "                                if (task.id, target_resource.id) in tabu_list:\n",
        "                                    cluster_tabu = True\n",
        "                                    break\n",
        "\n",
        "                            # Reassign all tasks in subset to target resource\n",
        "                            for task in tasks_subset:\n",
        "                                current_solution[task] = target_resource\n",
        "\n",
        "                            # Evaluate the cluster reassignment\n",
        "                            cluster_fitness = simplified_fitness(current_solution)\n",
        "\n",
        "                            # Aspiration criterion\n",
        "                            aspiration_satisfied = cluster_fitness < tabu_best_fitness\n",
        "\n",
        "                            # Update best move if better and either not tabu OR aspiration criterion is satisfied\n",
        "                            # Update best move if better and either not tabu OR aspiration criterion is satisfied OR epsilon exploration\n",
        "                            if (not cluster_tabu or aspiration_satisfied or random.random() < current_ts_epsilon * 0.3) and cluster_fitness < best_move_fitness:\n",
        "                                # Deep copy the subset and assignments for later use\n",
        "                                best_tasks_subset = tasks_subset.copy()\n",
        "                                best_original_assignments = copy.deepcopy(original_assignments)\n",
        "\n",
        "                                best_move = ('cluster', best_tasks_subset, target_resource, best_original_assignments)\n",
        "                                best_move_fitness = cluster_fitness\n",
        "                                best_move_details = {\n",
        "                                    'task_type': task_type,\n",
        "                                    'cluster_size': len(tasks_subset),\n",
        "                                    'target_resource': target_resource.type,\n",
        "                                    'fitness_before': current_fitness,\n",
        "                                    'fitness_after': cluster_fitness,\n",
        "                                    'move_type': 'cluster_reassign',\n",
        "                                    'aspiration_used': cluster_tabu and (aspiration_satisfied or random.random() < current_ts_epsilon * 0.3)\n",
        "                                }\n",
        "\n",
        "                                # Log if aspiration criterion was used\n",
        "                                if cluster_tabu and aspiration_satisfied:\n",
        "                                    logger.info(f\"Aspiration criterion applied: Tabu cluster move allowed as it improves best solution\")\n",
        "\n",
        "                            # Restore original assignments\n",
        "                            for task, resource in original_assignments.items():\n",
        "                                current_solution[task] = resource\n",
        "\n",
        "                # Apply best move if found\n",
        "                if best_move:\n",
        "                    move_type = best_move_details.get('move_type', 'unknown')\n",
        "\n",
        "                    if move_type == 'high_impact':\n",
        "                        task, new_resource = best_move\n",
        "                        old_resource = current_solution[task]\n",
        "                        current_solution[task] = new_resource\n",
        "\n",
        "                        # Add to tabu list\n",
        "                        tabu_list.append((task.id, new_resource.id))\n",
        "\n",
        "                        logger.info(f\"Applied high-impact move: Task {task.id} moved from {old_resource.type} to {new_resource.type}\")\n",
        "\n",
        "                    elif move_type == 'swap_pairs':\n",
        "                        _, task1, task2, resource1, resource2 = best_move\n",
        "                        current_solution[task1] = resource2\n",
        "                        current_solution[task2] = resource1\n",
        "\n",
        "                        # Add both moves to tabu list\n",
        "                        tabu_list.append((task1.id, resource2.id))\n",
        "                        tabu_list.append((task2.id, resource1.id))\n",
        "\n",
        "                        logger.info(f\"Applied swap move: Tasks {task1.id} and {task2.id} swapped resources\")\n",
        "\n",
        "                    elif move_type == 'cluster_reassign':\n",
        "                        _, tasks_subset, target_resource, original_assignments = best_move\n",
        "\n",
        "                        # Apply cluster reassignment\n",
        "                        for task in tasks_subset:\n",
        "                            current_solution[task] = target_resource\n",
        "                            # Add to tabu list\n",
        "                            tabu_list.append((task.id, target_resource.id))\n",
        "\n",
        "                        logger.info(f\"Applied cluster move: {len(tasks_subset)} tasks of type {best_move_details['task_type']} moved to {target_resource.type}\")\n",
        "\n",
        "                    # Update current fitness\n",
        "                    old_fitness = current_fitness\n",
        "                    current_fitness = best_move_fitness\n",
        "\n",
        "                    # Calculate improvement percentage\n",
        "                    improvement_pct = (old_fitness - current_fitness) / old_fitness * 100 if old_fitness > 0 else 0\n",
        "\n",
        "                    # Write TS progress to CSV with move type info\n",
        "                    with open(ts_csv_path, 'a', newline='') as f:\n",
        "                        writer = csv.writer(f)\n",
        "                        writer.writerow([\n",
        "                            iteration + 1,  # Iteration\n",
        "                            current_fitness,  # Current fitness\n",
        "                            tabu_best_fitness,  # Best fitness\n",
        "                            move_type,  # Move type\n",
        "                            best_move_details.get('task_id', best_move_details.get('task1_id', '')),  # Task ID\n",
        "                            best_move_details.get('old_resource', best_move_details.get('resource1', '')),  # Old resource\n",
        "                            best_move_details.get('new_resource', best_move_details.get('resource2', '')),  # New resource\n",
        "                            len(tabu_list),  # Tabu list size\n",
        "                            improvement_pct,  # Improvement percentage\n",
        "                            hash(frozenset((t.id, r.id) for t, r in current_solution.items()))  # Solution hash\n",
        "                        ])\n",
        "\n",
        "                    # Update best solution if improved\n",
        "                    if current_fitness < tabu_best_fitness:\n",
        "                        tabu_best_solution = copy.deepcopy(current_solution)\n",
        "                        tabu_best_fitness = current_fitness\n",
        "                        stagnation_counter = 0\n",
        "                    else:\n",
        "                        stagnation_counter += 1\n",
        "                else:\n",
        "                    stagnation_counter += 1\n",
        "\n",
        "                    # Log iteration with no move\n",
        "                    with open(ts_csv_path, 'a', newline='') as f:\n",
        "                        writer = csv.writer(f)\n",
        "                        writer.writerow([\n",
        "                            iteration + 1,  # Iteration\n",
        "                            current_fitness,  # Current fitness\n",
        "                            tabu_best_fitness,  # Best fitness\n",
        "                            \"no_move\",  # Move type\n",
        "                            \"\",  # Task ID\n",
        "                            \"\",  # Old resource\n",
        "                            \"\",  # New resource\n",
        "                            len(tabu_list),  # Tabu list size\n",
        "                            0.0,  # Improvement percentage\n",
        "                            hash(frozenset((t.id, r.id) for t, r in current_solution.items()))  # Solution hash\n",
        "                        ])\n",
        "\n",
        "                # Record overall solution every few iterations\n",
        "                if iteration % 10 == 0 or iteration == tabu_iterations - 1:\n",
        "                    # Calculate metrics similar to GA section\n",
        "                    resource_distribution = _get_resource_distribution(tabu_best_solution)\n",
        "                    task_type_distribution = _get_task_type_distribution(tabu_best_solution)\n",
        "                    task_times = [fast_exec_time_estimate(task, resource.id)\n",
        "                                for task, resource in tabu_best_solution.items()]\n",
        "                    avg_time = sum(task_times) / len(task_times) if task_times else 0\n",
        "                    worst_time = max(task_times) if task_times else 0\n",
        "                    best_time = min(task_times) if task_times else 0\n",
        "                    load_balance = np.std([len([t for t, r in tabu_best_solution.items() if r.id == res.id])\n",
        "                                        for res in self.resources])\n",
        "\n",
        "                    with open(solution_csv_path, 'a', newline='') as f:\n",
        "                        writer = csv.writer(f)\n",
        "                        writer.writerow([\n",
        "                            f\"TS_Iter_{iteration+1}\",  # Algorithm phase\n",
        "                            datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                            tabu_best_fitness,  # Solution cost\n",
        "                            load_balance,  # Load balance score\n",
        "                            json.dumps(resource_distribution),  # Resource distribution\n",
        "                            json.dumps(task_type_distribution),  # Task type distribution\n",
        "                            avg_time,  # Average execution time\n",
        "                            worst_time,  # Worst task time\n",
        "                            best_time  # Best task time\n",
        "                        ])\n",
        "\n",
        "                # Early stopping\n",
        "                if stagnation_counter >= 5:\n",
        "                    logger.info(f\"Early stopping Tabu Search at iteration {iteration+1} (no improvement for 5 iterations)\")\n",
        "                    break\n",
        "\n",
        "            # Record final TS solution\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Final_TS_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    tabu_best_fitness,  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    json.dumps(_get_resource_distribution(tabu_best_solution)),  # Resource distribution\n",
        "                    json.dumps(_get_task_type_distribution(tabu_best_solution)),  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "            # Add edge-cloud distribution logging here\n",
        "            tabu_edge_cloud_dist = self._get_edge_cloud_distribution(tabu_best_solution)\n",
        "            logger.info(f\"Tabu Solution - Edge: {tabu_edge_cloud_dist['edge_percentage']:.2f}%, Cloud: {tabu_edge_cloud_dist['cloud_percentage']:.2f}%\")\n",
        "\n",
        "            # === STEP 7: Select final solution and apply to resources ===\n",
        "            logger.info(\"Phase 5/5: Finalizing solution and distributing tasks...\")\n",
        "\n",
        "            # Validate solution before proceeding\n",
        "            valid_assignments = 0\n",
        "            for task, resource in tabu_best_solution.items():\n",
        "                if resource is not None:\n",
        "                    valid_assignments += 1\n",
        "\n",
        "            logger.info(f\"Validating tabu solution: {valid_assignments} valid assignments out of {len(tabu_best_solution)} tasks\")\n",
        "\n",
        "            # Check GA solution as well\n",
        "            ga_valid_assignments = 0\n",
        "            for task, resource in best_solution.items():\n",
        "                if resource is not None:\n",
        "                    ga_valid_assignments += 1\n",
        "\n",
        "            logger.info(f\"Validating GA solution: {ga_valid_assignments} valid assignments out of {len(best_solution)} tasks\")\n",
        "            tabu_cost = simplified_fitness(tabu_best_solution)\n",
        "\n",
        "            final_solution = tabu_best_solution\n",
        "            logger.info(f\"Using Tabu-improved solution (cost: {tabu_cost}, valid assignments: {valid_assignments})\")\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            # Calculate and log final edge-cloud distribution\n",
        "            final_edge_cloud_dist = self._get_edge_cloud_distribution(final_solution)\n",
        "            logger.info(f\"Final Solution - Edge: {final_edge_cloud_dist['edge_percentage']:.2f}%, Cloud: {final_edge_cloud_dist['cloud_percentage']:.2f}%\")\n",
        "\n",
        "            # Print summary\n",
        "            logger.info(\"\\n=== Optimization Complete ===\")\n",
        "            logger.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
        "            # Include edge-cloud distribution in summary\n",
        "            logger.info(f\"Edge-Cloud Distribution: {final_edge_cloud_dist['edge_percentage']:.2f}% Edge, {final_edge_cloud_dist['cloud_percentage']:.2f}% Cloud\")\n",
        "            # Reset resources - explicitly clear queues\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = []\n",
        "                resource.failed_tasks = []\n",
        "                logger.info(f\"Reset {resource.type} queue to 0 tasks\")\n",
        "\n",
        "            # Apply solution to resources\n",
        "            distributed_tasks = []\n",
        "            assignment_data = []\n",
        "            base_time = datetime.now()\n",
        "\n",
        "            # Task distribution counts\n",
        "            resource_assignment_counts = {r.type: 0 for r in self.resources}\n",
        "            # Temporary task assignment tracking\n",
        "            temp_resource_task_queues = {resource.id: [] for resource in self.resources}\n",
        "\n",
        "            # Process and apply all assignments, logging each step\n",
        "            logger.info(f\"Processing {len(final_solution)} task assignments...\")\n",
        "            assignment_count = 0\n",
        "\n",
        "            # CRITICALLY IMPORTANT: Set reference to task's assigned resource\n",
        "            # This helps maintain the connection between tasks and resources\n",
        "            for task, resource in final_solution.items():\n",
        "                # Verify task is valid\n",
        "                if task is None:\n",
        "                    logger.error(\"Found None task in solution!\")\n",
        "                    continue\n",
        "\n",
        "                # Handle case where resource is None by assigning to least loaded resource\n",
        "                if resource is None:\n",
        "                    logger.warning(f\"Task {task.id} has no resource assignment, assigning to least loaded resource\")\n",
        "                    continue\n",
        "\n",
        "                # Set task status and add to queue\n",
        "                task.status = 'CREATED'  # Ensure correct initial status\n",
        "                task.assigned_resource = resource  # CRITICAL: Maintain reference to resource\n",
        "                temp_resource_task_queues[resource.id].append(task)\n",
        "                resource_assignment_counts[resource.type] += 1\n",
        "                assignment_count += 1\n",
        "\n",
        "                # Debug log for each assignment\n",
        "                logger.info(f\"Assigned Task {task.id} ({task.type}) to {resource.type}, queue now has {len(resource.task_queue)} tasks\")\n",
        "\n",
        "                # Record assignment\n",
        "                # Record assignment using direct timestamp conversion without offset\n",
        "                arrival_time = datetime.fromtimestamp(task.arrival_time)\n",
        "                record = {\n",
        "                    'Task ID': task.id,\n",
        "                    'Type': task.type,\n",
        "                    'Input Size (GB)': task.input_size,\n",
        "                    'Output Size (GB)': task.output_size,\n",
        "                    'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'Status': task.status,\n",
        "                    'Assigned Node': resource.type if resource else 'None',\n",
        "                    'Estimated Time': fast_exec_time_estimate(task, resource.id) if resource else 'N/A'\n",
        "                }\n",
        "                assignment_data.append(record)\n",
        "                distributed_tasks.append(task)\n",
        "\n",
        "            logger.info(f\"Completed {assignment_count} task assignments\")\n",
        "\n",
        "            # Write to CSV\n",
        "            csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "            os.makedirs(csv_folder, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_filepath = os.path.join(csv_folder, f'optimized_hybrid_{timestamp}.csv')\n",
        "\n",
        "            with open(csv_filepath, mode='w', newline='') as f:\n",
        "                fieldnames = [\n",
        "                    'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "                    'Time of Arrival', 'Status', 'Assigned Node',\n",
        "                    'Estimated Time'\n",
        "                ]\n",
        "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(assignment_data)\n",
        "\n",
        "            # Print summary\n",
        "            logger.info(\"\\n=== Optimization Complete ===\")\n",
        "            logger.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
        "            logger.info(f\"Final solution cost: {simplified_fitness(final_solution)}\")\n",
        "            logger.info(f\"Task distribution:\")\n",
        "            for resource_type, count in resource_assignment_counts.items():\n",
        "                if count > 0:\n",
        "                    logger.info(f\"  {resource_type}: {count} tasks\")\n",
        "            logger.info(f\"Results saved to: {csv_filepath}\")\n",
        "\n",
        "            # Record final comparison\n",
        "            with open(solution_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"Final_Applied_Solution\",  # Algorithm phase\n",
        "                    datetime.now().strftime(\"%H:%M:%S\"),  # Timestamp\n",
        "                    simplified_fitness(final_solution),  # Solution cost\n",
        "                    load_balance,  # Load balance score\n",
        "                    json.dumps(resource_assignment_counts),  # Resource distribution (actual applied counts)\n",
        "                    json.dumps(_get_task_type_distribution(final_solution)),  # Task type distribution\n",
        "                    avg_time,  # Average execution time\n",
        "                    worst_time,  # Worst task time\n",
        "                    best_time  # Best task time\n",
        "                ])\n",
        "\n",
        "            # Add detailed debug information to verify task distribution\n",
        "            total_queued = 0\n",
        "            for resource in self.resources:\n",
        "                resource.task_queue = temp_resource_task_queues[resource.id]\n",
        "                logger.info(f\"Copied {len(resource.task_queue)} tasks to {resource.type} queue\")\n",
        "                queued = len(resource.task_queue)\n",
        "                total_queued += queued\n",
        "                logger.info(f\"After distribution: {resource.type} has {queued} tasks in queue\")\n",
        "\n",
        "            logger.info(f\"Total tasks queued: {total_queued} of {total_tasks} requested\")\n",
        "\n",
        "            # Ensure we always have the right number of tasks\n",
        "            # If we don't have enough tasks, apply Tabu-based logic for additional tasks\n",
        "            if total_queued < total_tasks:\n",
        "                logger.warning(f\"Distribution incomplete: only {total_queued} of {total_tasks} tasks distributed\")\n",
        "\n",
        "                # Calculate how many more tasks we need\n",
        "                tasks_needed = total_tasks - total_queued\n",
        "                logger.info(f\"Adding {tasks_needed} additional tasks using Tabu optimization logic\")\n",
        "\n",
        "                # Create additional tasks\n",
        "                additional_tasks = []\n",
        "                for i in range(tasks_needed):\n",
        "                    # Get task type\n",
        "                    # task_type = list(TASK_TYPES.keys())[i % len(TASK_TYPES)]\n",
        "                    task_type = random.choice(list(TASK_TYPES.keys()))\n",
        "                    specs = TASK_TYPES[task_type]\n",
        "\n",
        "                    # Create task\n",
        "                    new_task_id = total_queued + i + 1\n",
        "                    task = Task(\n",
        "                        task_id=new_task_id,\n",
        "                        task_type=task_type,\n",
        "                        input_size=specs[\"input_size\"],\n",
        "                        output_size=specs[\"output_size\"],\n",
        "                        cpu_required=specs[\"cpu_required\"]\n",
        "                    )\n",
        "\n",
        "                    # Set arrival time\n",
        "                    task.arrival_time = datetime.now().timestamp()\n",
        "                    task.status = 'CREATED'\n",
        "                    additional_tasks.append(task)\n",
        "\n",
        "                # Apply Tabu-based assignment logic for the additional tasks\n",
        "                # First calculate current resource loads from existing assignments\n",
        "                current_loads = {r.id: len(r.task_queue) for r in self.resources}\n",
        "\n",
        "                # For each additional task, assign based on the same criteria used in Tabu Search\n",
        "                for task in additional_tasks:\n",
        "                    # Find best resource based on type compatibility and load\n",
        "                    best_resource = None\n",
        "                    best_cost = float('inf')\n",
        "\n",
        "                    for resource in self.resources:\n",
        "                        # Calculate cost based on the same logic used in Tabu Search\n",
        "                        exec_time = fast_exec_time_estimate(task, resource.id)\n",
        "\n",
        "                        # With this (remove the conditional completely):\n",
        "                        # No task-type specific penalties\n",
        "                        # Add load balancing factor\n",
        "                        load_factor = current_loads[resource.id] * 500\n",
        "                        total_cost = exec_time + load_factor\n",
        "\n",
        "                        if total_cost < best_cost:\n",
        "                            best_cost = total_cost\n",
        "                            best_resource = resource\n",
        "\n",
        "                    # Assign to the best resource\n",
        "                    if best_resource:\n",
        "                        task.assigned_resource = best_resource\n",
        "                        best_resource.task_queue.append(task)\n",
        "                        current_loads[best_resource.id] += 1\n",
        "                        distributed_tasks.append(task)\n",
        "                        logger.info(f\"Added task {task.id} ({task.type}) to {best_resource.type} using Tabu logic\")\n",
        "\n",
        "                # Verify final count\n",
        "                final_count = sum(len(r.task_queue) for r in self.resources)\n",
        "                logger.info(f\"Final distribution check: {final_count} of {total_tasks} tasks distributed\")\n",
        "\n",
        "            logger.info(f\"Returning {len(distributed_tasks)} distributed tasks\")\n",
        "\n",
        "            # Return CSV paths alongside the distributed tasks\n",
        "            return distributed_tasks, {\n",
        "                'ga_csv': ga_csv_path,\n",
        "                'ts_csv': ts_csv_path,\n",
        "                'solution_csv': solution_csv_path,\n",
        "                'final_assignments': csv_filepath\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Optimized Hybrid algorithm: {e}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "\n",
        "            # Just raise the exception instead of falling back to round-robin\n",
        "            logger.error(\"Hybrid algorithm failed without fallback mechanism enabled\")\n",
        "            raise\n",
        "\n",
        "    def _shortest_job_first_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Distribute tasks using Shortest Job First approach. Tasks are sorted by estimated\n",
        "        execution time before distribution, while maintaining original task validation logic.\n",
        "        \"\"\"\n",
        "        # Step 1: Generate tasks and arrival times\n",
        "        tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "        # Define task types with input/output sizes\n",
        "        task_types = [\n",
        "            # Read Tasks: input_size > 0, output_size = 0\n",
        "            {\"type\": \"RT1\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"RT2\", \"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "            {\"type\": \"RT3\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "            {\"type\": \"RT4\", \"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "            # Write Tasks: input_size = 0, output_size > 0\n",
        "            {\"type\": \"WT1\", \"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"WT2\", \"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "            {\"type\": \"WT3\", \"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "            {\"type\": \"WT4\", \"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "        ]\n",
        "\n",
        "        # Step 2: Create temporary tasks with execution time estimates\n",
        "        task_estimates = []\n",
        "        for task_record in tasks:\n",
        "            #task_type = task_types[task_record.id % len(task_types)]\n",
        "            task_type = random.choice(task_types)\n",
        "\n",
        "            # Create temporary task for estimation\n",
        "            temp_task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "\n",
        "            # Find minimum execution time across all compatible resources\n",
        "            min_execution_time = float('inf')\n",
        "            for resource in self.resources:\n",
        "                can_process, _ = resource.can_process_task(temp_task)\n",
        "                if can_process:\n",
        "                    estimated_time = temp_task.estimate_execution_time(resource)\n",
        "                    min_execution_time = min(min_execution_time, estimated_time)\n",
        "\n",
        "            task_estimates.append({\n",
        "                'task_record': task_record,\n",
        "                'task_type': task_type,\n",
        "                'estimated_time': min_execution_time\n",
        "            })\n",
        "\n",
        "        # Step 3: Sort tasks by estimated execution time\n",
        "        sorted_tasks = sorted(task_estimates, key=lambda x: x['estimated_time'])\n",
        "\n",
        "        # Step 4: Create resource task map\n",
        "        resource_task_map = {resource.id: {'can_process': [], 'cannot_process': []}\n",
        "                            for resource in self.resources}\n",
        "\n",
        "        # Step 5: Pre-validate tasks (keeping original validation logic)\n",
        "        for task_info in sorted_tasks:\n",
        "            task_type = task_info['task_type']\n",
        "            task_record = task_info['task_record']\n",
        "\n",
        "            temp_task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "\n",
        "            for resource in self.resources:\n",
        "                can_process, reason = resource.can_process_task(temp_task)\n",
        "                if can_process:\n",
        "                    resource_task_map[resource.id]['can_process'].append(task_record)\n",
        "                else:\n",
        "                    resource_task_map[resource.id]['cannot_process'].append(task_record)\n",
        "\n",
        "        # Step 6: Reset resources\n",
        "        for resource in self.resources:\n",
        "            resource.task_queue = []\n",
        "            resource.failed_tasks = []\n",
        "\n",
        "        # Step 7: Distribute sorted tasks\n",
        "        distributed_tasks = []\n",
        "        resource_index = 0\n",
        "        csv_data = []\n",
        "        base_time = datetime.now()\n",
        "\n",
        "        for task_info in sorted_tasks:\n",
        "            task_record = task_info['task_record']\n",
        "            task_type = task_info['task_type']\n",
        "            resource = self.resources[resource_index]\n",
        "\n",
        "            # Create the actual task\n",
        "            task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "            task.arrival_time = task_record.arrival_time\n",
        "\n",
        "            arrival_time = base_time + timedelta(seconds=task_record.arrival_time)\n",
        "            assigned_node = resource.type\n",
        "\n",
        "            # Check if resource can process this task\n",
        "            if task_record in resource_task_map[resource.id]['can_process']:\n",
        "                task.status = 'READY'\n",
        "                resource.task_queue.append(task)\n",
        "            else:\n",
        "                task.status = 'FAILED'\n",
        "                task.failure_reason = f\"Cannot process on {resource.type}\"\n",
        "                resource.failed_tasks.append(task)\n",
        "\n",
        "            # Record task assignment\n",
        "            csv_data.append({\n",
        "                'Task ID': task.id,\n",
        "                'Type': task.type,\n",
        "                'Input Size': task_type[\"input_size\"],\n",
        "                'Output Size': task_type[\"output_size\"],\n",
        "                'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'Status': task.status,\n",
        "                'Assigned Node': assigned_node,\n",
        "                'Estimated Time': task_info['estimated_time']\n",
        "            })\n",
        "\n",
        "            distributed_tasks.append(task)\n",
        "            resource_index = (resource_index + 1) % len(self.resources)\n",
        "\n",
        "\n",
        "        # Logging and verification steps\n",
        "        logger.info(\"\\n=== Task Distribution Summary (SJF) ===\")\n",
        "        logger.info(f\"Total tasks requested: {total_tasks}\")\n",
        "\n",
        "        # Resource breakdown\n",
        "        for resource in self.resources:\n",
        "            queued = len(resource.task_queue)\n",
        "            failed = len(resource.failed_tasks)\n",
        "            logger.info(\n",
        "                f\"{resource.type}: \"\n",
        "                f\"Queue={queued}, Failed={failed}, \"\n",
        "                f\"Total={queued + failed}\"\n",
        "            )\n",
        "\n",
        "        # Verify counts\n",
        "        distributed_count = len(distributed_tasks)\n",
        "        queued_count = sum(len(r.task_queue) for r in self.resources)\n",
        "        failed_count = sum(len(r.failed_tasks) for r in self.resources)\n",
        "        total_count = queued_count + failed_count\n",
        "\n",
        "        logger.info(f\"Tasks distributed: {distributed_count}\")\n",
        "        logger.info(f\"Tasks queued: {queued_count}\")\n",
        "        logger.info(f\"Tasks failed: {failed_count}\")\n",
        "        logger.info(f\"Total count: {total_count}\")\n",
        "\n",
        "        assert distributed_count == total_tasks, \\\n",
        "            f\"Distribution count mismatch: {distributed_count} != {total_tasks}\"\n",
        "        assert total_count == total_tasks, \\\n",
        "            f\"Total count mismatch: {total_count} != {total_tasks}\"\n",
        "        assert len(set(t.id for t in distributed_tasks)) == total_tasks, \\\n",
        "            f\"Task ID uniqueness violation\"\n",
        "\n",
        "        # Write data to CSV\n",
        "        self.write_tasks_to_sjf_csv(csv_data)\n",
        "\n",
        "        return distributed_tasks\n",
        "    def _round_robin_distribution(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Distribute tasks using Round Robin approach\n",
        "        \"\"\"\n",
        "        # Step 1: Generate tasks and arrival times\n",
        "        tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "        # Step 1: Define task types with input/output sizes\n",
        "        task_types = [\n",
        "            # Read Tasks: input_size > 0, output_size = 0\n",
        "            {\"type\": \"RT1\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"RT2\", \"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "            {\"type\": \"RT3\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "            {\"type\": \"RT4\", \"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "            # Write Tasks: input_size = 0, output_size > 0\n",
        "            {\"type\": \"WT1\", \"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"WT2\", \"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "            {\"type\": \"WT3\", \"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "            {\"type\": \"WT4\", \"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "        ]\n",
        "\n",
        "        # Step 2: Create resource task map\n",
        "        resource_task_map = {resource.id: {'can_process': [], 'cannot_process': []}\n",
        "                            for resource in self.resources}\n",
        "\n",
        "        # Step 3: Pre-validate tasks\n",
        "        for task_record in tasks:\n",
        "            # task_type = task_types[task_record.id % len(task_types)]\n",
        "            task_type = random.choice(task_types)\n",
        "            temp_task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "\n",
        "            # Create temporary task for estimation\n",
        "            temp_task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=random.choice(task_types)[\"type\"],  # Randomly select task type\n",
        "                input_size=random.choice(task_types)[\"input_size\"],\n",
        "                output_size=random.choice(task_types)[\"output_size\"],\n",
        "                cpu_required=random.choice(task_types)[\"cpu_required\"]\n",
        "            )\n",
        "\n",
        "            for resource in self.resources:\n",
        "                can_process, reason = resource.can_process_task(temp_task)\n",
        "                if can_process:\n",
        "                    resource_task_map[resource.id]['can_process'].append(task_record)\n",
        "                else:\n",
        "                    resource_task_map[resource.id]['cannot_process'].append(task_record)\n",
        "\n",
        "        # Step 4: Reset resources\n",
        "        for resource in self.resources:\n",
        "            resource.task_queue = []\n",
        "            resource.failed_tasks = []\n",
        "\n",
        "        # Step 5: Distribute tasks\n",
        "        distributed_tasks = []\n",
        "        resource_index = 0\n",
        "        csv_data = []\n",
        "        base_time = datetime.now()\n",
        "\n",
        "        for task_record in tasks:\n",
        "            resource = self.resources[resource_index]\n",
        "\n",
        "            # Determine task type\n",
        "            # task_type = task_types[task_record.id % len(task_types)]\n",
        "            task_type = random.choice(task_types)\n",
        "\n",
        "            task = Task(\n",
        "                task_id=task_record.id,\n",
        "                task_type=task_type[\"type\"],\n",
        "                input_size=task_type[\"input_size\"],\n",
        "                output_size=task_type[\"output_size\"],\n",
        "                cpu_required=task_type[\"cpu_required\"]\n",
        "            )\n",
        "            task.arrival_time = task_record.arrival_time\n",
        "\n",
        "            arrival_time = base_time + timedelta(seconds=task_record.arrival_time)\n",
        "            assigned_node = resource.type\n",
        "\n",
        "            if task_record in resource_task_map[resource.id]['can_process']:\n",
        "                task.status = 'READY'\n",
        "                resource.task_queue.append(task)\n",
        "            else:\n",
        "                task.status = 'FAILED'\n",
        "                task.failure_reason = f\"Cannot process on {resource.type}\"\n",
        "                resource.failed_tasks.append(task)\n",
        "\n",
        "            csv_data.append({\n",
        "                'Task ID': task.id,\n",
        "                'Type': task.type,\n",
        "                'Input Size': task_type[\"input_size\"],\n",
        "                'Output Size': task_type[\"output_size\"],\n",
        "                'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'Status': task.status,\n",
        "                'Assigned Node': assigned_node\n",
        "            })\n",
        "\n",
        "            distributed_tasks.append(task)\n",
        "            resource_index = (resource_index + 1) % len(self.resources)\n",
        "\n",
        "        # Logging and verification steps\n",
        "        logger.info(\"\\n=== Task Distribution Summary ===\")\n",
        "        logger.info(f\"Total tasks requested: {total_tasks}\")\n",
        "\n",
        "        # Resource breakdown\n",
        "        for resource in self.resources:\n",
        "            queued = len(resource.task_queue)\n",
        "            failed = len(resource.failed_tasks)\n",
        "            logger.info(\n",
        "                f\"{resource.type}: \"\n",
        "                f\"Queue={queued}, Failed={failed}, \"\n",
        "                f\"Total={queued + failed}\"\n",
        "            )\n",
        "\n",
        "        # Verify counts\n",
        "        distributed_count = len(distributed_tasks)\n",
        "        queued_count = sum(len(r.task_queue) for r in self.resources)\n",
        "        failed_count = sum(len(r.failed_tasks) for r in self.resources)\n",
        "        total_count = queued_count + failed_count\n",
        "\n",
        "        logger.info(f\"Tasks distributed: {distributed_count}\")\n",
        "        logger.info(f\"Tasks queued: {queued_count}\")\n",
        "        logger.info(f\"Tasks failed: {failed_count}\")\n",
        "        logger.info(f\"Total count: {total_count}\")\n",
        "\n",
        "        assert distributed_count == total_tasks, \\\n",
        "            f\"Distribution count mismatch: {distributed_count} != {total_tasks}\"\n",
        "        assert total_count == total_tasks, \\\n",
        "            f\"Total count mismatch: {total_count} != {total_tasks}\"\n",
        "        assert len(set(t.id for t in distributed_tasks)) == total_tasks, \\\n",
        "            f\"Task ID uniqueness violation\"\n",
        "\n",
        "        # Write data to CSV\n",
        "        self.write_tasks_to_csv(csv_data)\n",
        "\n",
        "        return distributed_tasks\n",
        "    def _Default_distribute_tasks(self, total_tasks: int) -> List[Task]:\n",
        "        \"\"\"\n",
        "        Guaranteed distribution of exactly total_tasks tasks.\n",
        "        Uses pre-validation, strict counting, and saves distribution data to CSV.\n",
        "        \"\"\"\n",
        "        tasks, cumulative_times, inter_arrival_times = self.generate_tasks(total_tasks)\n",
        "\n",
        "        # Step 1: Define task types with input/output sizes\n",
        "        task_types = [\n",
        "            # Read Tasks: input_size > 0, output_size = 0\n",
        "            {\"type\": \"RT1\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"RT2\", \"input_size\": 0.2, \"output_size\": 0, \"cpu_required\": 4_000_000},\n",
        "            {\"type\": \"RT3\", \"input_size\": 5.0, \"output_size\": 0, \"cpu_required\": 200_000},\n",
        "            {\"type\": \"RT4\", \"input_size\": 0.5, \"output_size\": 0, \"cpu_required\": 500_000},\n",
        "            # Write Tasks: input_size = 0, output_size > 0\n",
        "            {\"type\": \"WT1\", \"input_size\": 0, \"output_size\": 2.0, \"cpu_required\": 2_000_000},\n",
        "            {\"type\": \"WT2\", \"input_size\": 0, \"output_size\": 0.5, \"cpu_required\": 1_000_000},\n",
        "            {\"type\": \"WT3\", \"input_size\": 0, \"output_size\": 5.0, \"cpu_required\": 500_000},\n",
        "            {\"type\": \"WT4\", \"input_size\": 0, \"output_size\": 0.2, \"cpu_required\": 200_000}\n",
        "        ]\n",
        "\n",
        "        # Step 2: Generate task records\n",
        "        task_records = []\n",
        "        for i, arrival_time in enumerate(cumulative_times):\n",
        "            task_type = task_types[i % len(task_types)]\n",
        "            task_records.append({\n",
        "                'id': i + 1,\n",
        "                'type': task_type[\"type\"],\n",
        "                'input_size': task_type[\"input_size\"],\n",
        "                'output_size': task_type[\"output_size\"],\n",
        "                'cpu_required': task_type[\"cpu_required\"],\n",
        "                'arrival_time': arrival_time\n",
        "            })\n",
        "\n",
        "        # Step 3: Create resource task map\n",
        "        resource_task_map = {resource.id: {'can_process': [], 'cannot_process': []}\n",
        "                            for resource in self.resources}\n",
        "\n",
        "        # Step 4: Pre-validate tasks\n",
        "        for task_record in task_records:\n",
        "            temp_task = Task(\n",
        "                task_id=task_record['id'],\n",
        "                task_type=task_record['type'],\n",
        "                input_size=task_record['input_size'],\n",
        "                output_size=task_record['output_size'],\n",
        "                cpu_required=task_record['cpu_required']\n",
        "            )\n",
        "\n",
        "            for resource in self.resources:\n",
        "                can_process, reason = resource.can_process_task(temp_task)\n",
        "                if can_process:\n",
        "                    resource_task_map[resource.id]['can_process'].append(task_record)\n",
        "                else:\n",
        "                    resource_task_map[resource.id]['cannot_process'].append(task_record)\n",
        "\n",
        "        # Step 5: Reset resources\n",
        "        for resource in self.resources:\n",
        "            resource.task_queue = []\n",
        "            resource.failed_tasks = []\n",
        "\n",
        "        # Step 6: Distribute tasks\n",
        "        distributed_tasks = []\n",
        "        resource_index = 0\n",
        "        csv_data = []\n",
        "        base_time = datetime.now()\n",
        "\n",
        "        for task_record in task_records:\n",
        "            resource = self.resources[resource_index]\n",
        "\n",
        "            task = Task(\n",
        "                task_id=task_record['id'],\n",
        "                task_type=task_record['type'],\n",
        "                input_size=task_record['input_size'],\n",
        "                output_size=task_record['output_size'],\n",
        "                cpu_required=task_record['cpu_required']\n",
        "            )\n",
        "            task.arrival_time = task_record['arrival_time']\n",
        "\n",
        "            arrival_time = base_time + timedelta(seconds=task_record['arrival_time'])\n",
        "            assigned_node = resource.type\n",
        "\n",
        "            if task_record in resource_task_map[resource.id]['can_process']:\n",
        "                task.status = 'READY'  # Changed from 'queued' to match Task class states\n",
        "                resource.task_queue.append(task)\n",
        "            else:\n",
        "                task.status = 'FAILED'  # Changed from 'failed' to match Task class states\n",
        "                task.failure_reason = f\"Cannot process on {resource.type}\"\n",
        "                resource.failed_tasks.append(task)\n",
        "\n",
        "            csv_data.append({\n",
        "                'Task ID': task.id,\n",
        "                'Type': task.type,\n",
        "                'Input Size': task_record['input_size'],\n",
        "                'Output Size': task_record['output_size'],\n",
        "                'Time of Arrival': arrival_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'Status': task.status,\n",
        "                'Assigned Node': assigned_node\n",
        "            })\n",
        "\n",
        "            distributed_tasks.append(task)\n",
        "            resource_index = (resource_index + 1) % len(self.resources)\n",
        "\n",
        "        # Step 7: Verify counts\n",
        "        distributed_count = len(distributed_tasks)\n",
        "        queued_count = sum(len(r.task_queue) for r in self.resources)\n",
        "        failed_count = sum(len(r.failed_tasks) for r in self.resources)\n",
        "        total_count = queued_count + failed_count\n",
        "\n",
        "        # Log summary\n",
        "        logger.info(\"\\n=== Task Distribution Summary ===\")\n",
        "        logger.info(f\"Total tasks requested: {total_tasks}\")\n",
        "        logger.info(f\"Tasks distributed: {distributed_count}\")\n",
        "        logger.info(f\"Tasks queued: {queued_count}\")\n",
        "        logger.info(f\"Tasks failed: {failed_count}\")\n",
        "        logger.info(f\"Total count: {total_count}\")\n",
        "\n",
        "        # Resource breakdown\n",
        "        for resource in self.resources:\n",
        "            queued = len(resource.task_queue)\n",
        "            failed = len(resource.failed_tasks)\n",
        "            logger.info(\n",
        "                f\"{resource.type}: \"\n",
        "                f\"Queue={queued}, Failed={failed}, \"\n",
        "                f\"Total={queued + failed}\"\n",
        "            )\n",
        "\n",
        "        # Verify counts\n",
        "        assert distributed_count == total_tasks, \\\n",
        "            f\"Distribution count mismatch: {distributed_count} != {total_tasks}\"\n",
        "        assert total_count == total_tasks, \\\n",
        "            f\"Total count mismatch: {total_count} != {total_tasks}\"\n",
        "        assert len(set(t.id for t in distributed_tasks)) == total_tasks, \\\n",
        "            f\"Task ID uniqueness violation\"\n",
        "\n",
        "        # Write data to CSV\n",
        "        self.write_tasks_to_csv(csv_data)\n",
        "\n",
        "        return distributed_tasks\n",
        "\n",
        "    def write_tasks_to_sjf_csv(self, task_data):\n",
        "        \"\"\"\n",
        "        Write task distribution data to CSV with updated filename for SJF algorithm\n",
        "        \"\"\"\n",
        "        # Create CSV folder if it doesn't exist\n",
        "        csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "        if not os.path.exists(csv_folder):\n",
        "            os.makedirs(csv_folder)\n",
        "\n",
        "        # Generate filename with timestamp and 'sjf' identifier\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filepath = os.path.join(csv_folder, f'sjf_task_distribution_{timestamp}.csv')\n",
        "\n",
        "        # Define CSV fields\n",
        "        fieldnames = [\n",
        "            'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "            'Time of Arrival', 'Start Time', 'Input Transfer Time',\n",
        "            'Processing Time', 'Output Transfer Time', 'Total Time',\n",
        "            'Status', 'Assigned Node', 'Estimated Time'\n",
        "        ]\n",
        "\n",
        "        # Write data to CSV\n",
        "        with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for task in task_data:\n",
        "                writer.writerow({\n",
        "                    'Task ID': task['Task ID'],\n",
        "                    'Type': task['Type'],\n",
        "                    'Input Size (GB)': task.get('Input Size', 0),\n",
        "                    'Output Size (GB)': task.get('Output Size', 0),\n",
        "                    'Time of Arrival': task['Time of Arrival'],\n",
        "                    'Start Time': task.get('Start Time', ''),\n",
        "                    'Input Transfer Time': task.get('Input Transfer Time', ''),\n",
        "                    'Processing Time': task.get('Processing Time', ''),\n",
        "                    'Output Transfer Time': task.get('Output Transfer Time', ''),\n",
        "                    'Total Time': task.get('Total Time', ''),\n",
        "                    'Status': task['Status'],\n",
        "                    'Assigned Node': task['Assigned Node'],\n",
        "                    'Estimated Time': task.get('Estimated Time', 'N/A')\n",
        "                })\n",
        "\n",
        "        logger.info(f\"Task distribution data written to {csv_filepath}\")\n",
        "        return csv_filepath\n",
        "\n",
        "    def write_tasks_to_csv(self, task_data):\n",
        "        \"\"\"\n",
        "        Updated CSV output to include transfer phases and data sizes\n",
        "        \"\"\"\n",
        "        csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "        if not os.path.exists(csv_folder):\n",
        "            os.makedirs(csv_folder)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filepath = os.path.join(csv_folder, f'round_robin_distribution_{timestamp}.csv')\n",
        "\n",
        "        # Updated fieldnames to include transfer information\n",
        "        fieldnames = [\n",
        "            'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "            'Time of Arrival', 'Start Time', 'Input Transfer Time',\n",
        "            'Processing Time', 'Output Transfer Time', 'Total Time',\n",
        "            'Status', 'Assigned Node'\n",
        "        ]\n",
        "\n",
        "        with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for task in task_data:\n",
        "                writer.writerow({\n",
        "                    'Task ID': task['Task ID'],\n",
        "                    'Type': task['Type'],\n",
        "                    'Input Size (GB)': task.get('input_size', 0),\n",
        "                    'Output Size (GB)': task.get('output_size', 0),\n",
        "                    'Time of Arrival': task['Time of Arrival'],\n",
        "                    'Start Time': task.get('start_time', ''),\n",
        "                    'Input Transfer Time': task.get('input_transfer_time', ''),\n",
        "                    'Processing Time': task.get('processing_time', ''),\n",
        "                    'Output Transfer Time': task.get('output_transfer_time', ''),\n",
        "                    'Total Time': task.get('total_time', ''),\n",
        "                    'Status': task['Status'],\n",
        "                    'Assigned Node': task['Assigned Node']\n",
        "                })\n",
        "\n",
        "        logger.info(f\"Task distribution data written to {csv_filepath}\")\n",
        "\n",
        "        # Flush and sync to ensure all data is written to the drive\n",
        "        logger.info(\"Google Drive has been unmounted. You can now access the CSV file in your Google Drive.\")\n",
        "    def distribute_tasks(self, total_tasks: int, distribution_type: str = 'default'):\n",
        "        \"\"\"\n",
        "        Enhanced distribution method with verification and emergency fallback\n",
        "        \"\"\"\n",
        "        # Mapping of distribution strategies\n",
        "        distribution_strategies = {\n",
        "            'default': self._Default_distribute_tasks,\n",
        "            'round_robin': self._round_robin_distribution,\n",
        "            'shortest_job_first': self._shortest_job_first_distribution,\n",
        "            'hybrid': self._optimized_hybrid_algorithm,\n",
        "            'greywolf': self._standard_gwo_distribution,\n",
        "            'genetic_modified': self.genetic_modified,\n",
        "            'gwo_tabu': self._hybrid_gwo_tabu_distribution,\n",
        "            'tabu_gwo': self._tabu_standard_gwo_distribution,\n",
        "\n",
        "        }\n",
        "\n",
        "        # Select and execute the appropriate distribution strategy\n",
        "        if distribution_type in distribution_strategies:\n",
        "            try:\n",
        "                logger.info(f\"Using {distribution_type} distribution strategy for {total_tasks} tasks\")\n",
        "\n",
        "                # First attempt with selected algorithm\n",
        "                result = distribution_strategies[distribution_type](total_tasks)\n",
        "\n",
        "                # Handle different return types\n",
        "                if isinstance(result, tuple) and len(result) == 2:\n",
        "                    distributed_tasks, csv_paths = result\n",
        "                else:\n",
        "                    distributed_tasks = result\n",
        "\n",
        "                # Verify tasks were actually queued and fix if needed\n",
        "                total_queued = sum(len(resource.task_queue) for resource in self.resources)\n",
        "                if total_queued < total_tasks:\n",
        "                    logger.warning(f\"Verification failed: Only {total_queued} of {total_tasks} queued after {distribution_type}\")\n",
        "                    fixed_tasks = self.verify_and_fix_task_distribution(total_tasks)\n",
        "\n",
        "                    # If we got CSV paths, package them with the fixed tasks\n",
        "                    if 'csv_paths' in locals():\n",
        "                        return fixed_tasks, csv_paths\n",
        "                    else:\n",
        "                        return fixed_tasks\n",
        "\n",
        "                # Return the original result\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in {distribution_type} distribution: {e}\")\n",
        "                logger.info(\"Falling back to emergency distribution due to error\")\n",
        "                return self.verify_and_fix_task_distribution(total_tasks)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution type: {distribution_type}. \"\n",
        "                            f\"Supported types are: {list(distribution_strategies.keys())}\")\n",
        "    def _write_tabu_assignments_to_csv(self, assignment_data):\n",
        "        \"\"\"Helper method to write Tabu Search assignments to CSV with total cost.\"\"\"\n",
        "        csv_folder = \"/content/drive/My Drive/CSV_dump\"\n",
        "        os.makedirs(csv_folder, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filepath = os.path.join(csv_folder, f'tabu_task_distribution_{timestamp}.csv')\n",
        "\n",
        "        # Calculate total cost for each task when written\n",
        "        for task_record in assignment_data:\n",
        "            # Skip if no resource assigned\n",
        "            if task_record['Assigned Node'] == 'None':\n",
        "                task_record['Total Cost'] = 'N/A'\n",
        "                continue\n",
        "\n",
        "            # Base execution time\n",
        "            base_cost = float(task_record['Estimated Time'])\n",
        "            total_cost = base_cost\n",
        "\n",
        "            # Add WT3-Raspberry penalty if applicable\n",
        "            if task_record['Type'] == 'WT3' and task_record['Assigned Node'].startswith('Raspberry_'):\n",
        "                total_cost += base_cost * 3  # 5x penalty\n",
        "\n",
        "            # Add cloud penalty if applicable\n",
        "            elif task_record['Assigned Node'].startswith('Cloud_') and task_record['Type'] not in ['RT1', 'RT3']:\n",
        "                total_cost += base_cost * 0.5\n",
        "\n",
        "            task_record['Total Cost'] = f\"{total_cost:.2f}\"\n",
        "\n",
        "        fieldnames = [\n",
        "            'Task ID', 'Type', 'Input Size (GB)', 'Output Size (GB)',\n",
        "            'Time of Arrival', 'Status', 'Assigned Node', 'Estimated Time',\n",
        "            'Total Cost'  # Added Total Cost field\n",
        "        ]\n",
        "\n",
        "        with open(csv_filepath, mode='w', newline='') as csv_file:\n",
        "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(assignment_data)\n",
        "\n",
        "        logger.info(f\"Task assignments with costs written to {csv_filepath}\")\n",
        "        return csv_filepath\n",
        "\n",
        "    def generate_live_feed(self):\n",
        "        \"\"\"\n",
        "        Updated live visualization to show transfer phases\n",
        "        \"\"\"\n",
        "        table = Table(\n",
        "            title=\"Live Resource Processing Status\",\n",
        "            box=box.MINIMAL_DOUBLE_HEAD,\n",
        "            show_header=True,\n",
        "            header_style=\"bold magenta\",\n",
        "            border_style=\"bold green\",\n",
        "            show_lines=True,\n",
        "            padding=(0, 1)\n",
        "        )\n",
        "\n",
        "        # Updated columns to show transfer phases\n",
        "        table.add_column(\"Resource\", style=\"bold blue\", width=15)\n",
        "        table.add_column(\"Type\", style=\"bold cyan\", width=20)\n",
        "        table.add_column(\"Current Task\", style=\"bold yellow\", width=15)\n",
        "        table.add_column(\"Phase\", style=\"bold green\", width=26)\n",
        "        table.add_column(\"Progress\", style=\"bold red\", width=18)\n",
        "        table.add_column(\"Data Size\", style=\"bold yellow\", width=18)\n",
        "        table.add_column(\"Queue\", style=\"bold green\", width=15)\n",
        "        table.add_column(\"Completed\", style=\"bold magenta\", width=15)\n",
        "        table.add_column(\"Failed\", style=\"bold red\", width=15)\n",
        "\n",
        "        for resource in self.resources:\n",
        "            current_task = \"None\"\n",
        "            phase = \"Idle\"\n",
        "            progress = \"N/A\"\n",
        "            data_size = \"N/A\"\n",
        "\n",
        "            if resource.current_task:\n",
        "                task = resource.current_task\n",
        "                current_task = f\"{task.id} ({task.type})\"\n",
        "\n",
        "                # Phase and progress based on task status\n",
        "                phase_colors = {\n",
        "                    'TRANSFERRING_INPUT': \"yellow\",\n",
        "                    'PROCESSING': \"blue\",\n",
        "                    'TRANSFERRING_OUTPUT': \"cyan\",\n",
        "                    'COMPLETED': \"green\"\n",
        "                }\n",
        "\n",
        "                phase = task.status\n",
        "                progress = f\"{task.completion_percentage:.1f}%\"\n",
        "\n",
        "                # Show relevant data size based on phase\n",
        "                if task.status == 'TRANSFERRING_INPUT':\n",
        "                    data_size = f\"↑{task.input_size:.1f}GB\"\n",
        "                elif task.status == 'TRANSFERRING_OUTPUT':\n",
        "                    data_size = f\"↓{task.output_size:.1f}GB\"\n",
        "                else:\n",
        "                    data_size = f\"↕{max(task.input_size, task.output_size):.1f}GB\"\n",
        "\n",
        "            # Add row with updated information\n",
        "            table.add_row(\n",
        "                f\"{resource.id:<8}\",\n",
        "                f\"{resource.type:<11}\",\n",
        "                current_task,\n",
        "                phase,\n",
        "                progress,\n",
        "                data_size,\n",
        "                f\"{len(resource.task_queue):<5}\",\n",
        "                f\"{len(resource.completed_tasks):<9}\",\n",
        "                str(len(resource.failed_tasks))\n",
        "            )\n",
        "\n",
        "        return table\n",
        "    def write_simulation_results(self, metrics, start_time, distribution_type):\n",
        "        \"\"\"\n",
        "        Write simulation results to CSV file with enhanced edge-cloud distribution metrics\n",
        "\n",
        "        Args:\n",
        "            metrics (dict): Simulation metrics\n",
        "            start_time (float): Simulation start timestamp\n",
        "            distribution_type (str): Type of distribution algorithm used\n",
        "        \"\"\"\n",
        "        csv_folder = \"/content/drive/My Drive/EdgeSimPy/results\"\n",
        "        if not os.path.exists(csv_folder):\n",
        "            os.makedirs(csv_folder)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_filepath = os.path.join(csv_folder, f'run_simulation_results_{distribution_type}_{timestamp}.csv')\n",
        "\n",
        "        # Prepare simulation results\n",
        "        results = {\n",
        "            'Distribution_Algorithm': distribution_type,\n",
        "            'Simulation_Start_Time': datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Simulation_End_Time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Total_Tasks': metrics['total_tasks'],\n",
        "            'Completed_Tasks': metrics['completed_tasks'],\n",
        "            'Failed_Tasks': metrics['failed_tasks'],\n",
        "            'Makespan': f\"{metrics['makespan']:.2f}s\",\n",
        "            'Throughput': f\"{metrics['throughput']:.2f} tasks/s\",\n",
        "        }\n",
        "\n",
        "        # Add edge-cloud distribution metrics\n",
        "        if 'edge_cloud_distribution' in metrics:\n",
        "            edge_cloud = metrics['edge_cloud_distribution']\n",
        "            results.update({\n",
        "                'Edge_Task_Percentage': f\"{edge_cloud['edge_percentage']:.2f}%\",\n",
        "                'Cloud_Task_Percentage': f\"{edge_cloud['cloud_percentage']:.2f}%\",\n",
        "                'Edge_Task_Count': edge_cloud['edge_tasks'],\n",
        "                'Cloud_Task_Count': edge_cloud['cloud_tasks'],\n",
        "            })\n",
        "\n",
        "        # Add load balancing metrics if available\n",
        "        if 'load_balancing_metrics' in metrics:\n",
        "            lb_metrics = metrics['load_balancing_metrics']\n",
        "            results.update({\n",
        "                'Average_Resource_Utilization': f\"{lb_metrics.get('average_utilization', 0):.4f}\",\n",
        "                'Coefficient_of_Variation': f\"{lb_metrics.get('load_balance_score', 0):.4f}\",\n",
        "                'CoV_Standard_Deviation': f\"{lb_metrics.get('utilization_std_dev', 0):.4f}\"\n",
        "            })\n",
        "\n",
        "            # Add resource type specific metrics\n",
        "            if 'resource_type_metrics' in lb_metrics:\n",
        "                for resource_type, type_metrics in lb_metrics['resource_type_metrics'].items():\n",
        "                    if resource_type != 'inter_type_balance':\n",
        "                        results.update({\n",
        "                            f'{resource_type}_avg_utilization': f\"{type_metrics.get('average_utilization', 0):.4f}\",\n",
        "                            f'{resource_type}_CoV': f\"{type_metrics.get('load_balance_score', 0):.4f}\"\n",
        "                        })\n",
        "\n",
        "                # Add inter-type balance if available\n",
        "                if 'inter_type_balance' in lb_metrics['resource_type_metrics']:\n",
        "                    inter_balance = lb_metrics['resource_type_metrics']['inter_type_balance']\n",
        "                    results.update({\n",
        "                        'Inter_Type_CoV': f\"{inter_balance.get('balance_score', 0):.4f}\"\n",
        "                    })\n",
        "\n",
        "        # Add timing metrics if available\n",
        "        if 'average_turnaround_time' in metrics:\n",
        "            results['Average_Turnaround_Time'] = f\"{metrics['average_turnaround_time']:.2f}s\"\n",
        "        if 'average_waiting_time' in metrics:\n",
        "            results['Average_Waiting_Time'] = f\"{metrics['average_waiting_time']:.2f}s\"\n",
        "        if 'average_execution_time' in metrics:\n",
        "            results['Average_Execution_Time'] = f\"{metrics['average_execution_time']:.2f}s\"\n",
        "\n",
        "        # Add failed tasks breakdown\n",
        "        for task_type, count in metrics['failed_tasks_by_type'].items():\n",
        "            results[f'Failed_{task_type}'] = count\n",
        "\n",
        "        for resource_type, count in metrics['failed_tasks_by_resource'].items():\n",
        "            results[f'Failed_On_{resource_type}'] = count\n",
        "\n",
        "        # Write to CSV\n",
        "        with open(csv_filepath, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Metric', 'Value'])\n",
        "            for key, value in results.items():\n",
        "                writer.writerow([key, value])\n",
        "\n",
        "        logger.info(f\"Simulation results written to: {csv_filepath}\")\n",
        "        return csv_filepath\n",
        "    def run_simulation(self, total_tasks: int, distribution_type: str = 'default') -> Dict:\n",
        "        \"\"\"\n",
        "        Run the simulation with enhanced logging and timing metrics.\n",
        "\n",
        "        This method handles the simulation execution, including:\n",
        "        1. Task distribution via selected algorithm\n",
        "        2. Real-time processing and visualization\n",
        "        3. Timing metrics calculation\n",
        "        4. Resource utilization tracking\n",
        "        5. Comprehensive result gathering\n",
        "\n",
        "        Args:\n",
        "            total_tasks (int): Number of tasks to simulate\n",
        "            distribution_type (str): The distribution algorithm to use\n",
        "\n",
        "        Returns:\n",
        "            Dict: Comprehensive metrics about the simulation run\n",
        "        \"\"\"\n",
        "        # Distribute tasks using the chosen algorithm\n",
        "        distribution_result = self.distribute_tasks(total_tasks, distribution_type)\n",
        "\n",
        "        # Check if result contains CSV paths (from hybrid algorithm)\n",
        "        if isinstance(distribution_result, tuple) and len(distribution_result) == 2:\n",
        "            tasks, csv_paths = distribution_result\n",
        "            # Store CSV paths for later analysis\n",
        "            self.algorithm_tracking_paths = csv_paths\n",
        "            logger.info(f\"Algorithm tracking data saved to: {csv_paths}\")\n",
        "        else:\n",
        "            tasks = distribution_result\n",
        "            self.algorithm_tracking_paths = {}\n",
        "\n",
        "        simulation_start_time = datetime.now().timestamp()\n",
        "\n",
        "        logger.info(f\"Starting simulation with {total_tasks} total tasks\")\n",
        "        logger.info(f\"Distributed tasks count: {len(tasks)}\")\n",
        "\n",
        "        # Start with original task distribution logging\n",
        "        logger.info(\"\\n=== Initial Task Distribution ===\")\n",
        "        for resource in self.resources:\n",
        "            logger.debug(f\"\\nProcessing Resource: {resource.type}\")\n",
        "            logger.debug(f\"  Current Task: {resource.current_task.type if resource.current_task else 'None'}\")\n",
        "            logger.debug(f\"  Queue Length: {len(resource.task_queue)}\")\n",
        "            if resource.task_queue:\n",
        "                for task in resource.task_queue[:5]:\n",
        "                    logger.info(f\"\"\"\n",
        "                    Task {task.id}:\n",
        "                    - Type: {task.type}\n",
        "                    - CPU Required: {task.total_cpu_required}\n",
        "                    - Arrival Time: {task.arrival_time}\n",
        "                    \"\"\")\n",
        "\n",
        "        resource_utilization = {}\n",
        "        datacenter_utilization_snapshots = []\n",
        "\n",
        "        # Main simulation loop\n",
        "        with Live(self.generate_live_feed(), refresh_per_second=1) as live:\n",
        "            while True:\n",
        "                self.current_time = datetime.now().timestamp() - simulation_start_time\n",
        "                all_completed = True\n",
        "\n",
        "                for resource in self.resources:\n",
        "                    logger.debug(f\"\\nProcessing Resource: {resource.type}\")\n",
        "                    logger.debug(f\"  Current Task: {resource.current_task.type if resource.current_task else 'None'}\")\n",
        "                    logger.debug(f\"  Queue Length: {len(resource.task_queue)}\")\n",
        "                    # Add a debug log right here to check task states\n",
        "                    for i, task in enumerate(resource.task_queue[:3]):  # Just check first 3 tasks\n",
        "                        logger.info(f\"Queue {i}: Task {task.id} - Status: {task.status}\")\n",
        "\n",
        "                    # Process the current queue for this resource\n",
        "                    queue_status = resource.process_queue(self.current_time)\n",
        "                    utilization = resource.calculate_resource_utilization()\n",
        "                    resource_utilization[resource.type] = utilization\n",
        "\n",
        "                    # Check completion status\n",
        "                    if resource.current_task:\n",
        "                        if (resource.current_task.status != 'COMPLETED' or\n",
        "                            resource.current_task.completion_percentage < 100):\n",
        "                            all_completed = False\n",
        "                    if resource.task_queue:\n",
        "                        all_completed = False\n",
        "\n",
        "                # Update datacenter utilization\n",
        "                datacenter_utilization_snapshots.append({\n",
        "                    'timestamp': self.current_time,\n",
        "                    'utilization': self.calculate_datacenter_utilization(\n",
        "                        simulation_start_time,\n",
        "                        datetime.now().timestamp() - simulation_start_time\n",
        "                    )\n",
        "                })\n",
        "\n",
        "                # Update live visualization\n",
        "                live.update(self.generate_live_feed())\n",
        "\n",
        "                # Check if all tasks are complete\n",
        "                total_completed = sum(len(r.completed_tasks) for r in self.resources)\n",
        "                total_failed = sum(len(r.failed_tasks) for r in self.resources) + len(self.metrics['globally_failed_tasks'])\n",
        "\n",
        "                if all_completed and (total_completed + total_failed >= total_tasks):\n",
        "                    all_tasks_truly_complete = True\n",
        "\n",
        "                    # Double-check all tasks are truly complete\n",
        "                    for resource in self.resources:\n",
        "                        for task in resource.completed_tasks:\n",
        "                            if task.completion_percentage < 100 or task.status != 'COMPLETED':\n",
        "                                all_tasks_truly_complete = False\n",
        "                                break\n",
        "                        if not all_tasks_truly_complete:\n",
        "                            break\n",
        "\n",
        "                    if all_tasks_truly_complete:\n",
        "                        break\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            # Calculate final metrics\n",
        "            completed_tasks = []\n",
        "            for resource in self.resources:\n",
        "                completed_tasks.extend(resource.completed_tasks)\n",
        "                logger.info(f\"\"\"\n",
        "                Resource {resource.type} Final Status:\n",
        "                - Completed Tasks: {len(resource.completed_tasks)}\n",
        "                - Failed Tasks: {len(resource.failed_tasks)}\n",
        "                \"\"\")\n",
        "\n",
        "            # Calculate timing metrics with detailed debugging\n",
        "            total_turnaround_time = 0.0\n",
        "            total_waiting_time = 0.0\n",
        "            valid_tasks = 0\n",
        "\n",
        "            logger.info(f\"\\nCalculating timing metrics for {len(completed_tasks)} tasks:\")\n",
        "\n",
        "            for task in completed_tasks:\n",
        "                # Debug log all timing values\n",
        "                logger.info(f\"\"\"\n",
        "                Task {task.id} Raw Timing Values Debug:\n",
        "                - Status: {task.status}\n",
        "                - Arrival Time Present: {task.arrival_time is not None}\n",
        "                - Arrival Time: {task.arrival_time if task.arrival_time is not None else 'None'}\n",
        "                - Start Time Present: {task.start_time is not None}\n",
        "                - Start Time: {task.start_time if task.start_time is not None else 'None'}\n",
        "                - Completion Time Present: {task.completion_time is not None}\n",
        "                - Completion Time: {task.completion_time if task.completion_time is not None else 'None'}\n",
        "                - Actual Exec Time: {task.actual_exec_time}\n",
        "                \"\"\")\n",
        "\n",
        "                # Check each condition separately for debugging\n",
        "                has_completion = task.completion_time is not None\n",
        "                has_arrival = task.arrival_time is not None\n",
        "                has_exec_time = task.actual_exec_time > 0\n",
        "                completion_after_arrival = (task.completion_time >= task.arrival_time) if (has_completion and has_arrival) else False\n",
        "\n",
        "                logger.info(f\"\"\"\n",
        "                Task {task.id} Validation Checks:\n",
        "                - Has Completion Time: {has_completion}\n",
        "                - Has Arrival Time: {has_arrival}\n",
        "                - Has Exec Time > 0: {has_exec_time}\n",
        "                - Completion After Arrival: {completion_after_arrival}\n",
        "                \"\"\")\n",
        "\n",
        "                if has_completion and has_arrival and has_exec_time and completion_after_arrival:\n",
        "                    # Calculate turnaround time\n",
        "                    task.turnaround_time = self.calculate_turnaround_time(task, simulation_start_time)\n",
        "                    task.waiting_time = self.calculate_waiting_time(task)\n",
        "\n",
        "                    # Log task timing details\n",
        "                    logger.info(f\"\"\"\n",
        "                    Task {task.id} Timing Details:\n",
        "                    - Arrival Time: {task.arrival_time}\n",
        "                    - Completion Time: {task.completion_time}\n",
        "                    - Actual Execution Time: {task.actual_exec_time}\n",
        "                    - Turnaround Time: {task.turnaround_time}\n",
        "                    - Waiting Time: {task.waiting_time}\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Additional validation for calculated times\n",
        "                    if task.turnaround_time > 0:\n",
        "                        total_turnaround_time += task.turnaround_time\n",
        "                        total_waiting_time += task.waiting_time\n",
        "                        valid_tasks += 1\n",
        "\n",
        "                        logger.info(f\"Task {task.id} ACCEPTED for metrics calculation\")\n",
        "                    else:\n",
        "                        logger.warning(f\"\"\"\n",
        "                        Task {task.id} has invalid calculated times:\n",
        "                        - Turnaround Time: {task.turnaround_time:.2f}\n",
        "                        - Waiting Time: {task.waiting_time:.2f}\n",
        "                        \"\"\")\n",
        "                else:\n",
        "                    logger.warning(f\"\"\"\n",
        "                    Task {task.id} failed validation:\n",
        "                    - Missing completion time: {not has_completion}\n",
        "                    - Missing arrival time: {not has_arrival}\n",
        "                    - No execution time: {not has_exec_time}\n",
        "                    - Completion before arrival: {not completion_after_arrival}\n",
        "                    \"\"\")\n",
        "\n",
        "            # After processing all tasks, log summary\n",
        "            logger.info(f\"\"\"\n",
        "            Timing Metrics Summary:\n",
        "            Total Completed Tasks: {len(completed_tasks)}\n",
        "            Valid Tasks for Timing: {valid_tasks}\n",
        "            Total Turnaround Time: {total_turnaround_time:.2f}\n",
        "            Total Waiting Time: {total_waiting_time:.2f}\n",
        "            \"\"\")\n",
        "\n",
        "            # Calculate averages\n",
        "            if valid_tasks > 0:\n",
        "                avg_turnaround = total_turnaround_time / valid_tasks\n",
        "                avg_waiting = total_waiting_time / valid_tasks\n",
        "                logger.info(f\"\"\"\n",
        "                Final Timing Metrics:\n",
        "                - Valid Tasks: {valid_tasks}\n",
        "                - Average Turnaround Time: {avg_turnaround:.2f}s\n",
        "                - Average Waiting Time: {avg_waiting:.2f}s\n",
        "                \"\"\")\n",
        "            else:\n",
        "                logger.warning(\"No valid tasks found for timing calculation!\")\n",
        "                avg_turnaround = 0\n",
        "                avg_waiting = 0\n",
        "\n",
        "            # Calculate edge-cloud distribution based on final task assignments\n",
        "            edge_tasks = 0\n",
        "            cloud_tasks = 0\n",
        "\n",
        "            for resource in self.resources:\n",
        "                if resource.type.startswith(\"Cloud_\"):\n",
        "                    cloud_tasks += len(resource.completed_tasks) + len(resource.task_queue) + len(resource.failed_tasks)\n",
        "                else:  # Edge resources (Raspberry Pi, Smartphone)\n",
        "                    edge_tasks += len(resource.completed_tasks) + len(resource.task_queue) + len(resource.failed_tasks)\n",
        "\n",
        "            total_assigned = edge_tasks + cloud_tasks\n",
        "\n",
        "            edge_cloud_distribution = {\n",
        "                \"edge_tasks\": edge_tasks,\n",
        "                \"cloud_tasks\": cloud_tasks,\n",
        "                \"edge_percentage\": (edge_tasks / total_assigned * 100) if total_assigned > 0 else 0,\n",
        "                \"cloud_percentage\": (cloud_tasks / total_assigned * 100) if total_assigned > 0 else 0,\n",
        "                \"total_tasks\": total_assigned\n",
        "            }\n",
        "\n",
        "            logger.info(f\"\\nEdge-Cloud Distribution for {distribution_type}:\")\n",
        "            logger.info(f\"Edge: {edge_tasks} tasks ({edge_cloud_distribution['edge_percentage']:.2f}%)\")\n",
        "            logger.info(f\"Cloud: {cloud_tasks} tasks ({edge_cloud_distribution['cloud_percentage']:.2f}%)\")\n",
        "\n",
        "            # Calculate load balancing metrics\n",
        "            load_balancing_metrics = self.calculate_load_balancing_metrics()\n",
        "\n",
        "            # Update metrics dictionary\n",
        "            self.metrics.update({\n",
        "                'total_tasks': total_tasks,\n",
        "                'completed_tasks': total_completed,\n",
        "                'failed_tasks': total_failed,\n",
        "                'makespan': self.current_time,\n",
        "                'throughput': total_completed / self.current_time if self.current_time > 0 else 0,\n",
        "                'load_balancing_metrics': load_balancing_metrics,  # Add load balancing metrics here\n",
        "                'average_turnaround_time': avg_turnaround,\n",
        "                'average_waiting_time': avg_waiting,\n",
        "                'valid_tasks': valid_tasks,\n",
        "                'edge_cloud_distribution': edge_cloud_distribution  # Add edge-cloud distribution metrics\n",
        "            })\n",
        "\n",
        "            # If we have algorithm tracking data, add it to metrics\n",
        "            if hasattr(self, 'algorithm_tracking_paths') and self.algorithm_tracking_paths:\n",
        "                self.metrics['algorithm_tracking_paths'] = self.algorithm_tracking_paths\n",
        "\n",
        "            # Process failures\n",
        "            detailed_failed_tasks = []\n",
        "            failed_tasks_by_type = {}\n",
        "            failed_tasks_by_resource = {}\n",
        "\n",
        "            for resource in self.resources:\n",
        "                for task in resource.failed_tasks:\n",
        "                    failure_info = {\n",
        "                        'task_id': task.id,\n",
        "                        'task_type': task.type,\n",
        "                        'resource_type': resource.type,\n",
        "                        'reason': getattr(task, 'failure_reason', 'Unknown reason')\n",
        "                    }\n",
        "                    detailed_failed_tasks.append(failure_info)\n",
        "                    failed_tasks_by_type[task.type] = failed_tasks_by_type.get(task.type, 0) + 1\n",
        "                    failed_tasks_by_resource[resource.type] = failed_tasks_by_resource.get(resource.type, 0) + 1\n",
        "\n",
        "            # Add globally failed tasks\n",
        "            for task in self.metrics['globally_failed_tasks']:\n",
        "                detailed_failed_tasks.append({\n",
        "                    'task_id': task.id,\n",
        "                    'task_type': task.type,\n",
        "                    'resource_type': \"None\",\n",
        "                    'reason': getattr(task, 'failure_reason', 'Unknown reason')\n",
        "                })\n",
        "\n",
        "            # Update failure metrics\n",
        "            self.metrics.update({\n",
        "                'detailed_failed_tasks': detailed_failed_tasks,\n",
        "                'failed_tasks_by_type': failed_tasks_by_type,\n",
        "                'failed_tasks_by_resource': failed_tasks_by_resource\n",
        "            })\n",
        "\n",
        "            # Store final datacenter utilization\n",
        "            self.metrics['final_datacenter_utilization'] = datacenter_utilization_snapshots[-1]['utilization'] if datacenter_utilization_snapshots else {}\n",
        "\n",
        "            # Write results and finish\n",
        "            results_file = self.write_simulation_results(self.metrics, simulation_start_time, distribution_type)\n",
        "            logger.info(f\"Simulation results saved to: {results_file}\")\n",
        "\n",
        "            # If we have algorithm tracking data, generate the visualizations\n",
        "            if 'algorithm_tracking_paths' in self.metrics:\n",
        "                try:\n",
        "                    visualization_files = self.analyze_algorithm_progress(self.metrics['algorithm_tracking_paths'])\n",
        "                    logger.info(f\"Algorithm progress visualizations saved to: {visualization_files}\")\n",
        "                    self.metrics['algorithm_visualizations'] = visualization_files\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error generating algorithm visualizations: {e}\")\n",
        "\n",
        "            return self.metrics\n",
        "def create_resources():\n",
        "    \"\"\"\n",
        "    Create resources with 10 Smartphones, 5 Raspberry Pis, and 5 Cloud Hosts\n",
        "    \"\"\"\n",
        "    resources = []\n",
        "\n",
        "    # Create 10 Smartphone Nodes\n",
        "    for i in range(1, 11):\n",
        "        resources.append(\n",
        "            Resource(\n",
        "                resource_id=i,\n",
        "                resource_type=f\"Smartphone_{i}\",  # Changed from Edge_ to Smartphone_\n",
        "                cpu_rating=400000,   # 400,000 MI/s\n",
        "                memory=4,            # 4 GB\n",
        "                bandwidth=400         # 400 MB/s\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Create 5 Raspberry Pi Nodes\n",
        "    for i in range(1, 6):\n",
        "        resources.append(\n",
        "            Resource(\n",
        "                resource_id=i+10,  # IDs 11-15\n",
        "                resource_type=f\"Raspberry_{i}\",\n",
        "                cpu_rating=80000,    # 80,000 MI/s\n",
        "                memory=1,            # 1 GB\n",
        "                bandwidth=100          # 100 MB/s\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Create 5 Cloud Hosts\n",
        "    for i in range(1, 6):\n",
        "        resources.append(\n",
        "            Resource(\n",
        "                resource_id=i+15,  # IDs 16-20\n",
        "                resource_type=f\"Cloud_{i}\",\n",
        "                cpu_rating=1000000,  # 1,000,000 MI/s\n",
        "                memory=32,           # 32 GB\n",
        "                bandwidth=1200         # 1200 MB/s\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Log created resources\n",
        "    logger.info(f\"Created {len(resources)} resources: {[r.type for r in resources]}\")\n",
        "\n",
        "    return resources\n",
        "def main():\n",
        "\n",
        "    # Disable all logging at the beginning\n",
        "    #import logging\n",
        "    logging.disable(logging.CRITICAL)\n",
        "    # Create resources\n",
        "    resources = create_resources()\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = ResourceFocusedScheduler(resources)\n",
        "\n",
        "    try:\n",
        "        # Define the total number of tasks - use a very small number for debugging\n",
        "        total_tasks = 2000  # Just 5 tasks for easier debugging\n",
        "\n",
        "        # Increase logging detail\n",
        "        #logger.setLevel(logging.DEBUG)\n",
        "\n",
        "        logger.info(\"=== STARTING SIMULATION WITH DETAILED DEBUGGING ===\")\n",
        "\n",
        "        # Run simulation\n",
        "        metrics = scheduler.run_simulation(total_tasks, distribution_type='hybrid')\n",
        "\n",
        "        logger.info(\"=== SIMULATION COMPLETED ===\")\n",
        "\n",
        "        # Print final metrics\n",
        "        print(\"\\nFinal Simulation Metrics:\")\n",
        "        for key, value in metrics.items():\n",
        "            if key not in ['resource_utilization', 'datacenter_utilization_snapshots', 'detailed_failed_tasks']:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled error in main: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}